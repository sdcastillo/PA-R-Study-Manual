<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title> 16 Tree-based models | Exam PA Study Guide, Fall 2020</title>
  <meta name="description" content=" 16 Tree-based models | Exam PA Study Guide, Fall 2020" />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content=" 16 Tree-based models | Exam PA Study Guide, Fall 2020" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content=" 16 Tree-based models | Exam PA Study Guide, Fall 2020" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="images/artificial_actuary_logo_favicon.png" type="image/x-icon" />
<link rel="prev" href="bias-variance-trade-off.html"/>
<link rel="next" href="unsupervised-learning.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  { background-color: #f8f8f8; }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Exam PA Study Manual</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a><ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#faq-frequently-asked-questions"><i class="fa fa-check"></i><b>0.1</b> FAQ: Frequently Asked Questions</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="actuarial-outpost.html"><a href="actuarial-outpost.html"><i class="fa fa-check"></i><b>1</b> Actuarial Outpost</a></li>
<li class="chapter" data-level="2" data-path="the-exam.html"><a href="the-exam.html"><i class="fa fa-check"></i><b>2</b> The exam</a></li>
<li class="chapter" data-level="3" data-path="prometric-demo.html"><a href="prometric-demo.html"><i class="fa fa-check"></i><b>3</b> Prometric Demo</a></li>
<li class="chapter" data-level="4" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>4</b> Introduction</a></li>
<li class="chapter" data-level="5" data-path="getting-started.html"><a href="getting-started.html"><i class="fa fa-check"></i><b>5</b> Getting started</a><ul>
<li class="chapter" data-level="5.1" data-path="getting-started.html"><a href="getting-started.html#download-the-data"><i class="fa fa-check"></i><b>5.1</b> Download the data</a></li>
<li class="chapter" data-level="5.2" data-path="getting-started.html"><a href="getting-started.html#download-islr"><i class="fa fa-check"></i><b>5.2</b> Download ISLR</a></li>
<li class="chapter" data-level="5.3" data-path="getting-started.html"><a href="getting-started.html#new-users"><i class="fa fa-check"></i><b>5.3</b> New users</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="how-much-r-do-i-need-to-know-to-pass.html"><a href="how-much-r-do-i-need-to-know-to-pass.html"><i class="fa fa-check"></i><b>6</b> How much R do I need to know to pass?</a><ul>
<li class="chapter" data-level="6.1" data-path="how-much-r-do-i-need-to-know-to-pass.html"><a href="how-much-r-do-i-need-to-know-to-pass.html#example-soa-pa-61620-task-8"><i class="fa fa-check"></i><b>6.1</b> Example: SOA PA 6/16/20, Task 8</a></li>
<li class="chapter" data-level="6.2" data-path="how-much-r-do-i-need-to-know-to-pass.html"><a href="how-much-r-do-i-need-to-know-to-pass.html#example-2---data-exploration"><i class="fa fa-check"></i><b>6.2</b> Example 2 - Data exploration</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="r-programming.html"><a href="r-programming.html"><i class="fa fa-check"></i><b>7</b> R programming</a><ul>
<li class="chapter" data-level="7.1" data-path="r-programming.html"><a href="r-programming.html#notebook-chunks"><i class="fa fa-check"></i><b>7.1</b> Notebook chunks</a></li>
<li class="chapter" data-level="7.2" data-path="r-programming.html"><a href="r-programming.html#basic-operations"><i class="fa fa-check"></i><b>7.2</b> Basic operations</a></li>
<li class="chapter" data-level="7.3" data-path="r-programming.html"><a href="r-programming.html#lists"><i class="fa fa-check"></i><b>7.3</b> Lists</a></li>
<li class="chapter" data-level="7.4" data-path="r-programming.html"><a href="r-programming.html#functions"><i class="fa fa-check"></i><b>7.4</b> Functions</a></li>
<li class="chapter" data-level="7.5" data-path="r-programming.html"><a href="r-programming.html#data-frames"><i class="fa fa-check"></i><b>7.5</b> Data frames</a></li>
<li class="chapter" data-level="7.6" data-path="r-programming.html"><a href="r-programming.html#pipes"><i class="fa fa-check"></i><b>7.6</b> Pipes</a></li>
<li class="chapter" data-level="7.7" data-path="r-programming.html"><a href="r-programming.html#the-soas-code-doesnt-use-pipes-or-dplyr-so-can-i-skip-learning-this"><i class="fa fa-check"></i><b>7.7</b> The SOA‚Äôs code doesn‚Äôt use pipes or dplyr, so can I skip learning this?</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="data-exploration.html"><a href="data-exploration.html"><i class="fa fa-check"></i><b>8</b> Data exploration</a><ul>
<li class="chapter" data-level="8.1" data-path="data-exploration.html"><a href="data-exploration.html#how-to-make-graphs-in-r"><i class="fa fa-check"></i><b>8.1</b> How to make graphs in R</a><ul>
<li class="chapter" data-level="8.1.1" data-path="data-exploration.html"><a href="data-exploration.html#add-a-plot"><i class="fa fa-check"></i><b>8.1.1</b> Add a plot</a></li>
<li class="chapter" data-level="8.1.2" data-path="data-exploration.html"><a href="data-exploration.html#data-manipulation-chaining"><i class="fa fa-check"></i><b>8.1.2</b> Data manipulation chaining</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="data-exploration.html"><a href="data-exploration.html#the-different-graph-types"><i class="fa fa-check"></i><b>8.2</b> The different graph types</a><ul>
<li class="chapter" data-level="8.2.1" data-path="data-exploration.html"><a href="data-exploration.html#histogram"><i class="fa fa-check"></i><b>8.2.1</b> Histogram</a></li>
<li class="chapter" data-level="8.2.2" data-path="data-exploration.html"><a href="data-exploration.html#box-plot"><i class="fa fa-check"></i><b>8.2.2</b> Box plot</a></li>
<li class="chapter" data-level="8.2.3" data-path="data-exploration.html"><a href="data-exploration.html#scatterplot"><i class="fa fa-check"></i><b>8.2.3</b> Scatterplot</a></li>
<li class="chapter" data-level="8.2.4" data-path="data-exploration.html"><a href="data-exploration.html#bar-charts"><i class="fa fa-check"></i><b>8.2.4</b> Bar charts</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="data-exploration.html"><a href="data-exploration.html#how-to-save-time-with-dplyr"><i class="fa fa-check"></i><b>8.3</b> How to save time with dplyr</a></li>
<li class="chapter" data-level="8.4" data-path="data-exploration.html"><a href="data-exploration.html#how-to-explore-the-data"><i class="fa fa-check"></i><b>8.4</b> How to explore the data</a></li>
<li class="chapter" data-level="8.5" data-path="data-exploration.html"><a href="data-exploration.html#how-to-transform-the-data"><i class="fa fa-check"></i><b>8.5</b> How to transform the data</a></li>
<li class="chapter" data-level="8.6" data-path="data-exploration.html"><a href="data-exploration.html#example-soa-pa-121219-task-1"><i class="fa fa-check"></i><b>8.6</b> Example: SOA PA 12/12/19, Task 1</a><ul>
<li class="chapter" data-level="8.6.1" data-path="data-exploration.html"><a href="data-exploration.html#garbage-in-garbage-out"><i class="fa fa-check"></i><b>8.6.1</b> Garbage in; garbage out üóë</a></li>
<li class="chapter" data-level="8.6.2" data-path="data-exploration.html"><a href="data-exploration.html#be-a-detective"><i class="fa fa-check"></i><b>8.6.2</b> Be a detective üîç</a></li>
<li class="chapter" data-level="8.6.3" data-path="data-exploration.html"><a href="data-exploration.html#a-picture-is-worth-a-thousand-words"><i class="fa fa-check"></i><b>8.6.3</b> A picture is worth a thousand words üì∑</a></li>
<li class="chapter" data-level="8.6.4" data-path="data-exploration.html"><a href="data-exploration.html#factor-or-numeric"><i class="fa fa-check"></i><b>8.6.4</b> Factor or numeric ‚ùì</a></li>
<li class="chapter" data-level="8.6.5" data-path="data-exploration.html"><a href="data-exploration.html#of-statistics-are-false"><i class="fa fa-check"></i><b>8.6.5</b> 73.6% of statistics are false üò≤</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="data-exploration.html"><a href="data-exploration.html#exercises"><i class="fa fa-check"></i><b>8.7</b> Exercises</a></li>
<li class="chapter" data-level="8.8" data-path="data-exploration.html"><a href="data-exploration.html#answers-to-exercises"><i class="fa fa-check"></i><b>8.8</b> Answers to exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html"><i class="fa fa-check"></i><b>9</b> Introduction to modeling</a><ul>
<li class="chapter" data-level="9.1" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#modeling-vocabulary"><i class="fa fa-check"></i><b>9.1</b> Modeling vocabulary</a></li>
<li class="chapter" data-level="9.2" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#modeling-notation"><i class="fa fa-check"></i><b>9.2</b> Modeling notation</a></li>
<li class="chapter" data-level="9.3" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#ordinary-least-squares-ols"><i class="fa fa-check"></i><b>9.3</b> Ordinary Least Squares (OLS)</a></li>
<li class="chapter" data-level="9.4" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#r2-statistic"><i class="fa fa-check"></i><b>9.4</b> R^2 Statistic</a></li>
<li class="chapter" data-level="9.5" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#correlation"><i class="fa fa-check"></i><b>9.5</b> Correlation</a><ul>
<li class="chapter" data-level="9.5.1" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#pearsons-correlation"><i class="fa fa-check"></i><b>9.5.1</b> Pearson‚Äôs correlation</a></li>
<li class="chapter" data-level="9.5.2" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#spearman-rank-correlation"><i class="fa fa-check"></i><b>9.5.2</b> Spearman (rank) correlation</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#regression-vs.-classification"><i class="fa fa-check"></i><b>9.6</b> Regression vs.¬†classification</a></li>
<li class="chapter" data-level="9.7" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#regression-metrics"><i class="fa fa-check"></i><b>9.7</b> Regression metrics</a><ul>
<li class="chapter" data-level="9.7.1" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#example-soa-pa-61820-task-4"><i class="fa fa-check"></i><b>9.7.1</b> Example: SOA PA 6/18/20, Task 4</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#example-health-costs"><i class="fa fa-check"></i><b>9.8</b> Example: Health Costs</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html"><i class="fa fa-check"></i><b>10</b> Generalized linear Models (GLMs)</a><ul>
<li class="chapter" data-level="10.0.1" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#assumptions-of-ols"><i class="fa fa-check"></i><b>10.0.1</b> Assumptions of OLS</a></li>
<li class="chapter" data-level="10.0.2" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#assumptions-of-glms"><i class="fa fa-check"></i><b>10.0.2</b> Assumptions of GLMs</a></li>
<li class="chapter" data-level="10.1" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#advantages-and-disadvantages"><i class="fa fa-check"></i><b>10.1</b> Advantages and disadvantages</a></li>
<li class="chapter" data-level="10.2" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#glms-for-regression"><i class="fa fa-check"></i><b>10.2</b> GLMs for regression</a></li>
<li class="chapter" data-level="10.3" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#interpretation-of-coefficients"><i class="fa fa-check"></i><b>10.3</b> Interpretation of coefficients</a><ul>
<li class="chapter" data-level="10.3.1" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#identity-link"><i class="fa fa-check"></i><b>10.3.1</b> Identity link</a></li>
<li class="chapter" data-level="10.3.2" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#log-link"><i class="fa fa-check"></i><b>10.3.2</b> Log link</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#other-links"><i class="fa fa-check"></i><b>10.4</b> Other links</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="glms-for-classification.html"><a href="glms-for-classification.html"><i class="fa fa-check"></i><b>11</b> GLMs for classification</a><ul>
<li class="chapter" data-level="11.1" data-path="glms-for-classification.html"><a href="glms-for-classification.html#binary-target"><i class="fa fa-check"></i><b>11.1</b> Binary target</a></li>
<li class="chapter" data-level="11.2" data-path="glms-for-classification.html"><a href="glms-for-classification.html#count-target"><i class="fa fa-check"></i><b>11.2</b> Count target</a></li>
<li class="chapter" data-level="11.3" data-path="glms-for-classification.html"><a href="glms-for-classification.html#link-functions"><i class="fa fa-check"></i><b>11.3</b> Link functions</a></li>
<li class="chapter" data-level="11.4" data-path="glms-for-classification.html"><a href="glms-for-classification.html#interpretation-of-coefficients-1"><i class="fa fa-check"></i><b>11.4</b> Interpretation of coefficients</a><ul>
<li class="chapter" data-level="11.4.1" data-path="glms-for-classification.html"><a href="glms-for-classification.html#logit"><i class="fa fa-check"></i><b>11.4.1</b> Logit</a></li>
<li class="chapter" data-level="11.4.2" data-path="glms-for-classification.html"><a href="glms-for-classification.html#probit-cauchit-cloglog"><i class="fa fa-check"></i><b>11.4.2</b> Probit, Cauchit, Cloglog</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="glms-for-classification.html"><a href="glms-for-classification.html#demo-the-model-for-interpretation"><i class="fa fa-check"></i><b>11.5</b> Demo the model for interpretation</a></li>
<li class="chapter" data-level="11.6" data-path="glms-for-classification.html"><a href="glms-for-classification.html#example---auto-claims"><i class="fa fa-check"></i><b>11.6</b> Example - Auto Claims</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="classification-metrics.html"><a href="classification-metrics.html"><i class="fa fa-check"></i><b>12</b> Classification metrics</a><ul>
<li class="chapter" data-level="12.1" data-path="classification-metrics.html"><a href="classification-metrics.html#area-under-the-roc-curve-auc"><i class="fa fa-check"></i><b>12.1</b> Area Under the ROC Curve (AUC)</a></li>
<li class="chapter" data-level="12.2" data-path="classification-metrics.html"><a href="classification-metrics.html#example---auto-claims-1"><i class="fa fa-check"></i><b>12.2</b> Example - Auto Claims</a></li>
<li class="chapter" data-level="12.3" data-path="classification-metrics.html"><a href="classification-metrics.html#example-soa-hr-task-5"><i class="fa fa-check"></i><b>12.3</b> Example: SOA HR, Task 5</a></li>
<li class="chapter" data-level="12.4" data-path="classification-metrics.html"><a href="classification-metrics.html#example-soa-pa-121219-task-11"><i class="fa fa-check"></i><b>12.4</b> Example: SOA PA 12/12/19, Task 11</a></li>
<li class="chapter" data-level="12.5" data-path="classification-metrics.html"><a href="classification-metrics.html#additional-reading"><i class="fa fa-check"></i><b>12.5</b> Additional reading</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html"><i class="fa fa-check"></i><b>13</b> Additional GLM topics</a><ul>
<li class="chapter" data-level="13.1" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#residuals"><i class="fa fa-check"></i><b>13.1</b> Residuals</a><ul>
<li class="chapter" data-level="13.1.1" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#raw-residuals"><i class="fa fa-check"></i><b>13.1.1</b> Raw residuals</a></li>
<li class="chapter" data-level="13.1.2" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#deviance-residuals"><i class="fa fa-check"></i><b>13.1.2</b> Deviance residuals</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#example"><i class="fa fa-check"></i><b>13.2</b> Example</a></li>
<li class="chapter" data-level="13.3" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#log-transforms-of-predictors"><i class="fa fa-check"></i><b>13.3</b> Log transforms of predictors</a></li>
<li class="chapter" data-level="13.4" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#reference-levels"><i class="fa fa-check"></i><b>13.4</b> Reference levels</a></li>
<li class="chapter" data-level="13.5" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#interactions"><i class="fa fa-check"></i><b>13.5</b> Interactions</a></li>
<li class="chapter" data-level="13.6" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#offsets"><i class="fa fa-check"></i><b>13.6</b> Offsets</a></li>
<li class="chapter" data-level="13.7" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#tweedie-regression"><i class="fa fa-check"></i><b>13.7</b> Tweedie regression</a></li>
<li class="chapter" data-level="13.8" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#combinations-of-link-functions-and-target-distributions"><i class="fa fa-check"></i><b>13.8</b> Combinations of Link Functions and Target Distributions</a><ul>
<li class="chapter" data-level="13.8.1" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#gaussian-response-with-log-link"><i class="fa fa-check"></i><b>13.8.1</b> Gaussian Response with Log Link</a></li>
<li class="chapter" data-level="13.8.2" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#gaussian-response-with-inverse-link"><i class="fa fa-check"></i><b>13.8.2</b> Gaussian Response with Inverse Link</a></li>
<li class="chapter" data-level="13.8.3" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#gaussian-response-with-identity-link"><i class="fa fa-check"></i><b>13.8.3</b> Gaussian Response with Identity Link</a></li>
<li class="chapter" data-level="13.8.4" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#gaussian-response-with-log-link-and-negative-values"><i class="fa fa-check"></i><b>13.8.4</b> Gaussian Response with Log Link and Negative Values</a></li>
<li class="chapter" data-level="13.8.5" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#gamma-response-with-log-link"><i class="fa fa-check"></i><b>13.8.5</b> Gamma Response with Log Link</a></li>
<li class="chapter" data-level="13.8.6" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#gamma-with-inverse-link"><i class="fa fa-check"></i><b>13.8.6</b> Gamma with Inverse Link</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="glm-variable-selection.html"><a href="glm-variable-selection.html"><i class="fa fa-check"></i><b>14</b> GLM variable selection</a><ul>
<li class="chapter" data-level="14.1" data-path="glm-variable-selection.html"><a href="glm-variable-selection.html#stepwise-subset-selection"><i class="fa fa-check"></i><b>14.1</b> Stepwise subset selection</a></li>
<li class="chapter" data-level="14.2" data-path="glm-variable-selection.html"><a href="glm-variable-selection.html#example-soa-pa-61219-task-6"><i class="fa fa-check"></i><b>14.2</b> Example: SOA PA 6/12/19, Task 6</a></li>
<li class="chapter" data-level="14.3" data-path="glm-variable-selection.html"><a href="glm-variable-selection.html#penalized-linear-models"><i class="fa fa-check"></i><b>14.3</b> Penalized Linear Models</a></li>
<li class="chapter" data-level="14.4" data-path="glm-variable-selection.html"><a href="glm-variable-selection.html#ridge-regression"><i class="fa fa-check"></i><b>14.4</b> Ridge Regression</a></li>
<li class="chapter" data-level="14.5" data-path="glm-variable-selection.html"><a href="glm-variable-selection.html#lasso"><i class="fa fa-check"></i><b>14.5</b> Lasso</a></li>
<li class="chapter" data-level="14.6" data-path="glm-variable-selection.html"><a href="glm-variable-selection.html#elastic-net"><i class="fa fa-check"></i><b>14.6</b> Elastic Net</a></li>
<li class="chapter" data-level="14.7" data-path="glm-variable-selection.html"><a href="glm-variable-selection.html#advantages-and-disadvantages-1"><i class="fa fa-check"></i><b>14.7</b> Advantages and disadvantages</a></li>
<li class="chapter" data-level="14.8" data-path="glm-variable-selection.html"><a href="glm-variable-selection.html#example-ridge-regression"><i class="fa fa-check"></i><b>14.8</b> Example: Ridge Regression</a></li>
<li class="chapter" data-level="14.9" data-path="glm-variable-selection.html"><a href="glm-variable-selection.html#example-the-lasso"><i class="fa fa-check"></i><b>14.9</b> Example: The Lasso</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="bias-variance-trade-off.html"><a href="bias-variance-trade-off.html"><i class="fa fa-check"></i><b>15</b> Bias-variance trade-off</a></li>
<li class="chapter" data-level="16" data-path="tree-based-models.html"><a href="tree-based-models.html"><i class="fa fa-check"></i><b>16</b> Tree-based models</a><ul>
<li class="chapter" data-level="16.1" data-path="tree-based-models.html"><a href="tree-based-models.html#the-basics-of-decision-trees"><i class="fa fa-check"></i><b>16.1</b> The basics of decision trees</a></li>
<li class="chapter" data-level="16.2" data-path="tree-based-models.html"><a href="tree-based-models.html#choosing-where-to-split"><i class="fa fa-check"></i><b>16.2</b> Choosing where to split</a><ul>
<li class="chapter" data-level="16.2.1" data-path="tree-based-models.html"><a href="tree-based-models.html#entropy"><i class="fa fa-check"></i><b>16.2.1</b> Entropy</a></li>
<li class="chapter" data-level="16.2.2" data-path="tree-based-models.html"><a href="tree-based-models.html#gini"><i class="fa fa-check"></i><b>16.2.2</b> Gini</a></li>
<li class="chapter" data-level="16.2.3" data-path="tree-based-models.html"><a href="tree-based-models.html#example---srm-practice-question-33"><i class="fa fa-check"></i><b>16.2.3</b> Example - SRM Practice Question 33</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="tree-based-models.html"><a href="tree-based-models.html#tree-complexity"><i class="fa fa-check"></i><b>16.3</b> Tree complexity</a></li>
<li class="chapter" data-level="16.4" data-path="tree-based-models.html"><a href="tree-based-models.html#cost-complexity-pruning"><i class="fa fa-check"></i><b>16.4</b> Cost-complexity pruning</a></li>
<li class="chapter" data-level="16.5" data-path="tree-based-models.html"><a href="tree-based-models.html#example-soa-pa-6162020-task-6"><i class="fa fa-check"></i><b>16.5</b> Example: SOA PA 6/16/2020, Task 6</a></li>
<li class="chapter" data-level="16.6" data-path="tree-based-models.html"><a href="tree-based-models.html#advantages-and-disadvantages-2"><i class="fa fa-check"></i><b>16.6</b> Advantages and disadvantages</a></li>
<li class="chapter" data-level="16.7" data-path="tree-based-models.html"><a href="tree-based-models.html#ensemble-learning"><i class="fa fa-check"></i><b>16.7</b> Ensemble learning</a><ul>
<li class="chapter" data-level="16.7.1" data-path="tree-based-models.html"><a href="tree-based-models.html#bagging"><i class="fa fa-check"></i><b>16.7.1</b> Bagging</a></li>
<li class="chapter" data-level="16.7.2" data-path="tree-based-models.html"><a href="tree-based-models.html#boosting"><i class="fa fa-check"></i><b>16.7.2</b> Boosting</a></li>
</ul></li>
<li class="chapter" data-level="16.8" data-path="tree-based-models.html"><a href="tree-based-models.html#random-forests"><i class="fa fa-check"></i><b>16.8</b> Random Forests</a><ul>
<li class="chapter" data-level="16.8.1" data-path="tree-based-models.html"><a href="tree-based-models.html#example-1"><i class="fa fa-check"></i><b>16.8.1</b> Example</a></li>
<li class="chapter" data-level="16.8.2" data-path="tree-based-models.html"><a href="tree-based-models.html#variable-importance"><i class="fa fa-check"></i><b>16.8.2</b> Variable Importance</a></li>
<li class="chapter" data-level="16.8.3" data-path="tree-based-models.html"><a href="tree-based-models.html#partial-dependence"><i class="fa fa-check"></i><b>16.8.3</b> Partial dependence</a></li>
<li class="chapter" data-level="16.8.4" data-path="tree-based-models.html"><a href="tree-based-models.html#advantages-and-disadvantages-3"><i class="fa fa-check"></i><b>16.8.4</b> Advantages and disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="16.9" data-path="tree-based-models.html"><a href="tree-based-models.html#gradient-boosted-trees"><i class="fa fa-check"></i><b>16.9</b> Gradient Boosted Trees</a><ul>
<li class="chapter" data-level="16.9.1" data-path="tree-based-models.html"><a href="tree-based-models.html#gradient-boosting"><i class="fa fa-check"></i><b>16.9.1</b> Gradient Boosting</a></li>
<li class="chapter" data-level="16.9.2" data-path="tree-based-models.html"><a href="tree-based-models.html#notation"><i class="fa fa-check"></i><b>16.9.2</b> Notation</a></li>
<li class="chapter" data-level="16.9.3" data-path="tree-based-models.html"><a href="tree-based-models.html#parameters"><i class="fa fa-check"></i><b>16.9.3</b> Parameters</a></li>
<li class="chapter" data-level="16.9.4" data-path="tree-based-models.html"><a href="tree-based-models.html#example-2"><i class="fa fa-check"></i><b>16.9.4</b> Example</a></li>
<li class="chapter" data-level="16.9.5" data-path="tree-based-models.html"><a href="tree-based-models.html#advantages-and-disadvantages-4"><i class="fa fa-check"></i><b>16.9.5</b> Advantages and disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="16.10" data-path="tree-based-models.html"><a href="tree-based-models.html#exercises-1"><i class="fa fa-check"></i><b>16.10</b> Exercises</a><ul>
<li class="chapter" data-level="16.10.1" data-path="tree-based-models.html"><a href="tree-based-models.html#rf-with-randomforest"><i class="fa fa-check"></i><b>16.10.1</b> 1. RF with <code>randomForest</code></a></li>
<li class="chapter" data-level="16.10.2" data-path="tree-based-models.html"><a href="tree-based-models.html#rf-tuning-with-caret"><i class="fa fa-check"></i><b>16.10.2</b> 2. RF tuning with <code>caret</code></a></li>
<li class="chapter" data-level="16.10.3" data-path="tree-based-models.html"><a href="tree-based-models.html#tuning-a-gbm-with-caret"><i class="fa fa-check"></i><b>16.10.3</b> 3. Tuning a GBM with <code>caret</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="17" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html"><i class="fa fa-check"></i><b>17</b> Unsupervised Learning</a><ul>
<li class="chapter" data-level="17.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#types-of-learning"><i class="fa fa-check"></i><b>17.1</b> Types of Learning</a></li>
<li class="chapter" data-level="17.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#correlation-analysis"><i class="fa fa-check"></i><b>17.2</b> Correlation Analysis</a><ul>
<li class="chapter" data-level="17.2.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#correlation-does-not-equal-causation"><i class="fa fa-check"></i><b>17.2.1</b> Correlation does not equal causation</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#principal-component-analysis-pca"><i class="fa fa-check"></i><b>17.3</b> Principal Component Analysis (PCA)</a><ul>
<li class="chapter" data-level="17.3.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#example-us-arrests"><i class="fa fa-check"></i><b>17.3.1</b> Example: US Arrests</a></li>
<li class="chapter" data-level="17.3.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#soa-pa-61319-task-3"><i class="fa fa-check"></i><b>17.3.2</b> SOA PA 6/13/19, Task 3</a></li>
<li class="chapter" data-level="17.3.3" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#example-pca-on-cancel-cells"><i class="fa fa-check"></i><b>17.3.3</b> Example: PCA on Cancel Cells</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#clustering"><i class="fa fa-check"></i><b>17.4</b> Clustering</a></li>
<li class="chapter" data-level="17.5" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#k-means-clustering"><i class="fa fa-check"></i><b>17.5</b> K-Means Clustering</a></li>
<li class="chapter" data-level="17.6" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#hierarchical-clustering"><i class="fa fa-check"></i><b>17.6</b> Hierarchical Clustering</a><ul>
<li class="chapter" data-level="17.6.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#example-clustering-cancel-cells"><i class="fa fa-check"></i><b>17.6.1</b> Example: Clustering Cancel Cells</a></li>
<li class="chapter" data-level="17.6.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#references"><i class="fa fa-check"></i><b>17.6.2</b> References</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="references-1.html"><a href="references-1.html"><i class="fa fa-check"></i><b>18</b> References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Exam PA Study Guide, Fall 2020</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="tree-based-models" class="section level1">
<h1><span class="header-section-number"> 16</span> Tree-based models</h1>
<p>The models that you have used up to this point have been linear. On the PA Syllabus, these all fall under <strong>Topic 6: Generalized Linear Models.</strong> This if for good reason: every sitting of exam PA has included a GLM. While GLMs get a lot of attention, the syllabus also contains <strong>Topic 7: Decision Trees.</strong> Fortunately, much of what you already know about modeling will apply here. We will use the same set up for training and testing, performance metrics, concepts of bias and variance, and weights that you already know about. This chapter is called ‚Äútree-<strong>based</strong> models‚Äù because the algorithms all rely on the atomic unit of a decision tree.</p>
<p>As of the fall of 2020, there have been eight different PA exams. The below table shows which types of models were used on each. You will definitely see a question about GLMs or decision trees because every exam has included these. There are variations as to the type of GLM - whether it is using stepwise selection or elastic net. The final two models are popular with data scientists but do not get much attention on PA - the random forest and the GBM.</p>
<p><img src="images/model_q_history.png" width="100%" style="display: block; margin: auto;" /></p>
<p>The syllabus describes these topics.</p>
<p><img src="images/learning_obj7.png" width="400%" style="display: block; margin: auto;" /></p>
<div id="the-basics-of-decision-trees" class="section level2">
<h2><span class="header-section-number">16.1</span> The basics of decision trees</h2>
<iframe width="560" height="315" src="https://www.youtube.com/embed/7VeUPuFGJHk" frameborder="0" allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
<iframe width="560" height="315" src="https://www.youtube.com/embed/7VeUPuFGJHk&amp;?rel=0&amp;showinfo=1&amp;playlist=wpNl-JwwplA,g9c66TUylZ4,D0efHEJsfHo" frameborder="0" allowfullscreen>
</iframe>
<p><br></p>
<p>Let‚Äôs take an example of predicting whether or not a patient has a large claim of over $ 10,000 or not. First, we look at all of the patients on a graph. The shape and color reflects whether the claim is large or not. On the x-axis is the patient‚Äôs age and on the y-axis is their body-mass index (bmi).</p>
<p><img src="images/treeslabeled.png" width="100%" style="display: block; margin: auto;" /></p>
<p>Trees make predictions by asking a series of yes/no questions to each patient. The questions are called <em>nodes</em> or <em>splits</em>. The first node is called the <em>root</em>. You can remember this by imagining that the tree is upside down and growing out of the ground. The <em>depth</em> is the highest number of nodes. Every patient begins at the top and moves either left or right at each node. At the end is the predicted probability that that patient will have high claims.</p>
<ul>
<li>For patients over the age of 52, 96% had large claims and so the predicted probability is 0.96. This represents 24% of the data</li>
<li>For patients under the age of 52, who were smokers, 100% had large claims. This accounts for 16%.</li>
</ul>
<p>The other branches are interpreted in the same way.</p>
</div>
<div id="choosing-where-to-split" class="section level2">
<h2><span class="header-section-number">16.2</span> Choosing where to split</h2>
<p>We divide the predictor space, that is, the set of possible values for <code>age</code>, <code>children</code>, <code>charges</code>, <code>sex</code>, <code>smoker</code>, <code>age_bucket</code>, <code>bmi</code>, and <code>region</code>, into distinct non-overlaping regions. In theory, the trree could be composed by any combination of rules based on these variables. In reality, though, this is not computationally practical. Consider a single variable, say, <code>age</code>. You would need to consider all rules where <code>age&lt;1</code>, <code>age&lt;2</code>, <code>age&lt;3</code>, <code>age&lt;4</code>, and so on. Instead, an approximation method is used known as <em>recursive binary splitting</em>.</p>
<p>This is called a <em>top-down</em> approach because it begins at the top of the tree, which is the point where all observations belong to a single tree and then successively splits the predictor space. It‚Äôs also called a <em>greedy</em> approach because at each step, the best split is made.</p>
<p>How is the ‚Äúbest‚Äù split determined? There are two ways. Neither method works best on all data sets so it‚Äôs best to try out both and compare. On PA, this topic has never appeared on the exam and so you don‚Äôt need to memorize these details but should be familiar with the concepts.</p>
<div id="entropy" class="section level3">
<h3><span class="header-section-number">16.2.1</span> Entropy</h3>
<p>Also known as information gain, entropy is a measure of disorder or randomness. For a given split <span class="math inline">\(S\)</span> with binary outcomes, where the probability of being in the positive class is <span class="math inline">\(p\)</span> and <span class="math inline">\(\bar{p}\)</span> of being in the negative class (the compelementary probability),the entropy is</p>
<p><span class="math display">\[E(S) = -plog_2(p) - \bar{p}log_2(\bar{p})\]</span></p>
</div>
<div id="gini" class="section level3">
<h3><span class="header-section-number">16.2.2</span> Gini</h3>
<p>This is the method which the StatQuest videos show.</p>
<p><span class="math display">\[G(S) = 1 - p^2 - \bar{p}^2\]</span></p>
<p>Where <span class="math inline">\(G(S) = 0\)</span> or <span class="math inline">\(E(S) = 0\)</span> <span class="math inline">\(\rightarrow\)</span> Best split <span class="math inline">\(\rightarrow\)</span> Highest purity <span class="math inline">\(\rightarrow\)</span> all elements belong to a single class.</p>
<p>Where <span class="math inline">\(G(S) = 1\)</span> or <span class="math inline">\(E(S) = 1\)</span> <span class="math inline">\(\rightarrow\)</span> Worst split <span class="math inline">\(\rightarrow\)</span> Lowest purity <span class="math inline">\(\rightarrow\)</span> Elements are randomly distributed across the classes.</p>
<p>Consider the below split at <code>age = 20</code>. The gini and entropy are first calculated for each branch of the tree and then the overall average is taken.</p>
<p><img src="images/splitrule2.png" width="50%" style="display: block; margin: auto;" /></p>
<p><strong>Left Branch: Age &lt; 20</strong></p>
<p>There are four large claims and only one small claim and so <span class="math inline">\(p = 4/5 = 0.8\)</span>.</p>
<p><span class="math display">\[G(S) = 1 - 0.8^2 - 0.2^2 = 0.32\]</span></p>
<p><span class="math display">\[E(S) = -0.8log_2(0.8) - 0.2log_2(0.2) = 0.72\]</span></p>
<p><strong>Right branch: Age &gt;= 20</strong></p>
<p>There are four small claims and two large claims and so <span class="math inline">\(p = 2/6 = 0.33\)</span></p>
<p><span class="math display">\[G(S) = 1 - 0.33^2 - 0.67^2 = 0.44\]</span></p>
<p><span class="math display">\[E(s) = 0.33log2(0.33) - 0.67log2(0.67) = 0.91\]</span></p>
<p>To get the overall score, take the weighted average with the number of patients as the weights.</p>
<p><span class="math display">\[G(S) = (5(0.32) + 6(0.44))/11 = 0.39\]</span></p>
<p><span class="math display">\[E(S) = (5(0.72) + 6(0.91))/11 = 0.82\]</span></p>
<p>Is this better or worse than a split where <code>age = 30</code>?</p>
<p><img src="images/splitrule3.png" width="50%" style="display: block; margin: auto;" /></p>
<p><strong>Left Branch: Age &lt; 30</strong></p>
<p><span class="math display">\[p = 4/7 = 0.571\]</span></p>
<p><span class="math display">\[G(S) = 1 - 0.571^2 - 0.429^2 = 0.49\]</span>
<span class="math display">\[E(S) = 0.99\]</span></p>
<p><strong>Right Branch: Age &gt;= 30</strong></p>
<p><span class="math display">\[p = 2/4 = 0.5\]</span></p>
<p><span class="math display">\[G(S) = 1 - 0.5^2 - 0.5^2 = 0.5\]</span>
<span class="math display">\[E(S) = 1\]</span></p>
<p>The overall gini is <span class="math inline">\((0.49(7) + 0.5(4))/11 = 0.49\)</span> and the overall entropy is <span class="math inline">\((0.99(7) + 1.0(4))/11 = 0.99\)</span>. In both cases, this is higher (worse) than the split at <span class="math inline">\(age = 20\)</span> because the groups are less homogeneous.</p>
<p>Once the first split has been chosen based on the gini or entropy, the next step is to further subdivide each of the regions. For all patients over the age of 20, the algorithm will look at the variable-split-point combinations of all other variables and again choose the one with the best criteria. For example, this might be at <span class="math inline">\(\text{BMI} = 15\)</span>. The lower right and upper right regions have only divided the large claims from the small claims, which is the objective. The entropy and gini of these regions is 0.</p>
<p>This tree can be writtten as a series of simple if statements:</p>
<ul>
<li>If <code>age</code> &lt; 20, predict <code>yes</code> with probability 4/5</li>
<li>If <code>age</code> &gt;= 20 and <code>bmi</code> &gt; 15, predict <code>no</code> with probability 4/4</li>
<li>If <code>age</code> &gt;= 20 and <code>bmi</code> &lt; 15, predict <code>yes</code> with probability 2/2</li>
</ul>
<p>The first group on the left could be partiontioned down again. We could take into consideration other variables such as <code>smoker</code>, <code>children</code>, and so forth.</p>
<p><img src="images/splitrule4.png" width="50%" style="display: block; margin: auto;" /></p>
</div>
<div id="example---srm-practice-question-33" class="section level3">
<h3><span class="header-section-number">16.2.3</span> Example - SRM Practice Question 33</h3>
<p>This question from the SOA‚Äôs Statistics for Risk Modeling (SRM) exam is an example of how to interpret a tree. Simply begin at the top and proceed downwards, asking each question. The first auto has <code>agecat = 1</code> and so there‚Äôs only one possible value, which results in a prediction of 8.146. The second has <code>agecat &gt;=1.5</code>, <code>veh_age &gt;= 2.5</code>, <code>agecat &gt;= 4.5</code>, and <code>veh_age &gt;= 3.5</code>, which results in a predicted value of 8.028. The third auto is interpreted similarly.</p>
<p><img src="images/srm33.png" width="100%" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="tree-complexity" class="section level2">
<h2><span class="header-section-number">16.3</span> Tree complexity</h2>
<p>If this pattern of recursive partitioning continues for too long then the model will overfit. This results in high accuracy on the training data but poor accuracy on the test data. This type of model is useless in real life because predictions are always made on new, never-before-seen data. In addition, the tree structure is often too complicated to explain in business terms.</p>
<p>Consider this tree</p>
<div class="sourceCode" id="cb352"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb352-1" title="1"><span class="kw">library</span>(rpart)</a>
<a class="sourceLine" id="cb352-2" title="2"><span class="kw">library</span>(rpart.plot)</a>
<a class="sourceLine" id="cb352-3" title="3">df &lt;-<span class="st"> </span>health_insurance <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">large_claim =</span> <span class="kw">ifelse</span>(charges <span class="op">&gt;</span><span class="st"> </span><span class="dv">10000</span>, <span class="st">&quot;yes&quot;</span>, <span class="st">&quot;no&quot;</span>))</a>
<a class="sourceLine" id="cb352-4" title="4">tree &lt;-<span class="st"> </span><span class="kw">rpart</span>(<span class="dt">formula =</span> large_claim <span class="op">~</span><span class="st">  </span>age <span class="op">+</span><span class="st"> </span>bmi <span class="op">+</span><span class="st"> </span>smoker <span class="op">+</span><span class="st"> </span>sex <span class="op">+</span><span class="st"> </span>children, <span class="dt">data =</span> df,</a>
<a class="sourceLine" id="cb352-5" title="5">              <span class="dt">control =</span> <span class="kw">rpart.control</span>(<span class="dt">cp =</span> <span class="dv">0</span>))</a>
<a class="sourceLine" id="cb352-6" title="6"></a>
<a class="sourceLine" id="cb352-7" title="7"><span class="kw">rpart.plot</span>(tree, <span class="dt">type =</span> <span class="dv">3</span>)</a></code></pre></div>
<p><img src="Exam-PA-Study-Manual_files/figure-html/unnamed-chunk-213-1.png" width="576" /></p>
<p><em>Pruning</em> is the process of making a tree simpler by trimming off branches. This is very similar to how with linear models we reduce complexity by reducing the number of coefficients.</p>
<p>A measure of the depth of the tree is the <em>complexity</em>. A simple way of measuring this from the number of terminal nodes, called <span class="math inline">\(|T|\)</span>. In the above example, <span class="math inline">\(|T| = 8\)</span>. The amount of penalization is controlled by <span class="math inline">\(\alpha\)</span>. This is very similar to <span class="math inline">\(\lambda\)</span> in the Lasso.</p>
<p>Intuitively, only looking at the number of nodes by itself is too simple because not all data sets will have the same characteristics such as <span class="math inline">\(n\)</span>, <span class="math inline">\(p\)</span>, the number of categorical variables, correlations between variables, and so fourth. In addition, if we just looked at the error then we would overfit very easily. To address this issue, we use a cost function which takes into account the error as well as <span class="math inline">\(|T|\)</span>. If this were a regression tree then squared error would be used, and if a classification tree then misclassification error would be used.</p>
<p>To calculate the cost of a tree, number the terminal nodes from <span class="math inline">\(1\)</span> to <span class="math inline">\(|T|\)</span>, and let the set of observations that fall into the <span class="math inline">\(mth\)</span> bucket be <span class="math inline">\(R_m\)</span>. Then add up the squared error over all terminal nodes to the penalty term.</p>
<p><span class="math display">\[
\text{Cost}_\alpha(T) = \sum_{m=1}^{|T|} \sum_{R_m}(y_i - \hat{y}_{Rm})^2 + \alpha |T|
\]</span></p>
<p>The argument <code>cp</code> is the complexity parameter. Essentially, the user informs the program that any split which does not improve the fit by <code>cp</code> will likely be pruned off by cross-validation, and that hence the program need not pursue it. The above tree sets <code>cp = 0</code>which means that no pruning takes place. The tree continues to create more splits until there are 5 or fewer patients in the leaf nodes.</p>
<div class="studytip">
<p>
<strong>You don‚Äôt need to memorize parameter definitions!</strong> Simply type <code>?rpart.control</code> into the console to bring up the documentation. You can do this on the Prometric computer as well.
</p>
</div>
</div>
<div id="cost-complexity-pruning" class="section level2">
<h2><span class="header-section-number">16.4</span> Cost-complexity pruning</h2>
<p>Now that you have a way of comparing trees together based on their complexity, how do we know which amount of complexity is right? If complexity is too low, the model will be underfitting and will not capture all of the signal that is in the data. On the other hand, if complexity is too high, then the tree will be too sensitive to random noise and will overfit.</p>
<p>This table shows the <code>cp</code> along with the number of splits at each tree. When <code>nsplit = 0</code>, all patients are in the same group. When <code>cp = 0</code>, then <code>nsplit = 18</code> which is the above tree. The <code>xerror</code> column is the missclassification error, and <code>rel error</code> is the normalized version so that it is between 0 and 1. <code>xstd</code> is the standard deviation of the predictions. <strong>For PA, you want to choose a tree that strikes a balance between having a low error and having few splits so that it can be interpreted.</strong></p>
<div class="sourceCode" id="cb353"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb353-1" title="1">cost &lt;-<span class="st"> </span>tree<span class="op">$</span>cptable <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb353-2" title="2"><span class="st">  </span><span class="kw">as_tibble</span>() </a>
<a class="sourceLine" id="cb353-3" title="3">cost <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">head</span>()</a></code></pre></div>
<pre><code>## # A tibble: 6 x 5
##         CP nsplit `rel error` xerror   xstd
##      &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;
## 1 0.481         0       1      1     0.0292
## 2 0.345         1       0.519  0.519 0.0251
## 3 0.0152        2       0.174  0.174 0.0160
## 4 0.00160       4       0.144  0.147 0.0148
## 5 0.000145      7       0.139  0.169 0.0158
## 6 0            18       0.137  0.177 0.0161</code></pre>
<p>As more splits are added, the cost continues to decrease, reaches a minimum, and then begins to increase.</p>
<p><img src="Exam-PA-Study-Manual_files/figure-html/unnamed-chunk-216-1.png" width="576" /></p>
<p>The SOA may give you code to find the lowest CP value such as below. You could always find this value yourself by inspecting the CP table and choosing the value of <code>CP</code> which has the lowest <code>xerror</code>.</p>
<div class="sourceCode" id="cb355"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb355-1" title="1">pruned_tree &lt;-<span class="st"> </span><span class="kw">prune</span>(tree,</a>
<a class="sourceLine" id="cb355-2" title="2">                     <span class="dt">cp =</span> tree<span class="op">$</span>cptable[<span class="kw">which.min</span>(tree<span class="op">$</span>cptable[, <span class="st">&quot;xerror&quot;</span>]), <span class="st">&quot;CP&quot;</span>])</a></code></pre></div>
<p>Many questions on PA have given candidates a big tree and then asked that they use pruning to make it simpler. There are ways of doing this besides just adjusting <code>cp</code>. To make a simpler tree, you can</p>
<ul>
<li>Set <code>cp</code> to be higher</li>
<li>Set the maximum depth of a tree with <code>maxdepth</code></li>
<li>Use fewer input variables and avoid categories with many levels</li>
<li>Force a high number of minimum observations per terminal node with <code>minbucket</code></li>
</ul>
<p>For instance, using these suggestions allows for a simpler tree to be fit.</p>
<div class="sourceCode" id="cb356"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb356-1" title="1"><span class="kw">library</span>(caret)</a>
<a class="sourceLine" id="cb356-2" title="2"><span class="kw">set.seed</span>(<span class="dv">42</span>)</a>
<a class="sourceLine" id="cb356-3" title="3">index &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(<span class="dt">y =</span> health_insurance<span class="op">$</span>charges, </a>
<a class="sourceLine" id="cb356-4" title="4">                             <span class="dt">p =</span> <span class="fl">0.8</span>, <span class="dt">list =</span> F)</a>
<a class="sourceLine" id="cb356-5" title="5">train &lt;-<span class="st"> </span>health_insurance <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(index)</a>
<a class="sourceLine" id="cb356-6" title="6">test &lt;-<span class="st"> </span>health_insurance <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(<span class="op">-</span>index)</a>
<a class="sourceLine" id="cb356-7" title="7"></a>
<a class="sourceLine" id="cb356-8" title="8">simple_tree &lt;-<span class="st"> </span><span class="kw">rpart</span>(<span class="dt">formula =</span> charges <span class="op">~</span><span class="st">  </span>., </a>
<a class="sourceLine" id="cb356-9" title="9">              <span class="dt">data =</span> train,</a>
<a class="sourceLine" id="cb356-10" title="10">              <span class="dt">control =</span> <span class="kw">rpart.control</span>(<span class="dt">cp =</span> <span class="fl">0.0001</span>, </a>
<a class="sourceLine" id="cb356-11" title="11">                                      <span class="dt">minbucket =</span> <span class="dv">100</span>,</a>
<a class="sourceLine" id="cb356-12" title="12">                                      <span class="dt">maxdepth =</span> <span class="dv">4</span>))</a>
<a class="sourceLine" id="cb356-13" title="13"><span class="kw">rpart.plot</span>(simple_tree, <span class="dt">type =</span> <span class="dv">3</span>)</a></code></pre></div>
<p><img src="Exam-PA-Study-Manual_files/figure-html/unnamed-chunk-218-1.png" width="576" /></p>
</div>
<div id="example-soa-pa-6162020-task-6" class="section level2">
<h2><span class="header-section-number">16.5</span> Example: SOA PA 6/16/2020, Task 6</h2>
<iframe src="https://player.vimeo.com/video/467846520" width="640" height="360" frameborder="0" allow="autoplay; fullscreen" allowfullscreen>
</iframe>
<p>Already enrolled? Watch the <a target="_parent" href="https://course.exampa.net/mod/page/view.php?id=85">full video.</a></p>
<p>(10 Points)</p>
<blockquote>
<p>Describe what pruning does and why it might be considered it for this business problem.</p>
</blockquote>
<blockquote>
<p>Construct an unpruned regression tree using the code provided.</p>
</blockquote>
<blockquote>
<p>Review the complexity parameter table and plot for this tree. State the optimal
complexity parameter and the number of leaves that will result if the tree is pruned
using that value.</p>
</blockquote>
<blockquote>
<p>Prune the tree using a complexity parameter that will result in eight leaves. If eight is
not a possible option, select the largest number less than eight that is possible.</p>
</blockquote>
<blockquote>
<p>Calculate and compare the Pearson goodness-of-fit statistic on the test set for both
trees (original and pruned).</p>
</blockquote>
<blockquote>
<p>Interpret the entire pruned tree (all leaves) in the context of the business problem.</p>
</blockquote>
</div>
<div id="advantages-and-disadvantages-2" class="section level2">
<h2><span class="header-section-number">16.6</span> Advantages and disadvantages</h2>
<p><strong>Advantages</strong></p>
<ul>
<li>Easy to interpret</li>
<li>Performs variable selection</li>
<li>Categorical variables do not require binarization in order for each level to be used as a separate predictor</li>
<li>Captures non-linearities and interaction effects</li>
<li>Handles missing values</li>
</ul>
<p><strong>Disadvantages</strong></p>
<ul>
<li>Predictive accuracy tends to be lower than other models on PA</li>
<li>Is often a simplification of the underlying process because all observations at terminal nodes have equal predicted values</li>
<li>Can be very non-robust because small changes in the data can cause a large change in the final estimated tree</li>
</ul>
<table>
<thead>
<tr class="header">
<th>Readings</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ISLR 8.1.1 Basics of Decision Trees</td>
<td></td>
</tr>
<tr class="even">
<td>ISLR 8.1.2 Classification Trees</td>
<td></td>
</tr>
<tr class="odd">
<td><a href="https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf">rpart Documentation (Optional)</a></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<div id="ensemble-learning" class="section level2">
<h2><span class="header-section-number">16.7</span> Ensemble learning</h2>
<p>The ‚Äúwisdom of crowds‚Äù says that often many are smarter than the few. In the context of modeling, the models which we have looked at so far have been single guesses; however, often the underlying process is more complex than any single model can explain. If we build separate models and then combine them, known as <em>ensembling</em>, performance can be improved. Instead of trying to create a single perfect model, many simple models, known as <em>weak learners</em> are combined into a <em>meta-model</em>.</p>
<p>The two main ways that models are combined are through <em>bagging</em> and <em>boosting</em>.</p>
<div id="bagging" class="section level3">
<h3><span class="header-section-number">16.7.1</span> Bagging</h3>
<p>To start, we create many ‚Äúcopies‚Äù of the training data by sampling with replacement. Then we fit a simple model, typically a decision tree or linear model, to each of the data sets. Because each model is looking at different areas of the data, the predictions are different. The final model is a weighted average of each of the individual models.</p>
</div>
<div id="boosting" class="section level3">
<h3><span class="header-section-number">16.7.2</span> Boosting</h3>
<p>Boosting always uses the original training data and iteratively fits models to the error of the prior models. These weak learners are ineffective by themselves but powerful when added together. Unlike with bagging, the computer must train these weak learners <em>sequentially</em> instead of in parallel.</p>
</div>
</div>
<div id="random-forests" class="section level2">
<h2><span class="header-section-number">16.8</span> Random Forests</h2>
<p>A random forest is the most common example of bagging. As the name implies, a forest is made up of <em>trees</em>. Seperate trees are fit to sampled data sets. For random forests, there is one minor modification: in order to make each model even more different, each tree selects a <em>random subset of variables</em>.</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/J4Wdy0Wc_xQ?rel=0&amp;showinfo=1&amp;playlist=nyxTdL_4Q-Q,6EXPYzbfLCE" frameborder="0" allowfullscreen>
</iframe>
<p>If we had to explain why a random forest works in three steps, they would be:</p>
<ol style="list-style-type: decimal">
<li>Assume that the underlying process, <span class="math inline">\(Y\)</span>, has some signal within the data <span class="math inline">\(\mathbf{X}\)</span>.</li>
<li>Introduce randomness (variance) to capture the signal.</li>
<li>Remove the variance by taking an average.</li>
</ol>
<p>When using only a single tree, there can only be as many predictions as there are terminal nodes. In a random forest, predictions can be more granular due to the contribution of each of the trees.</p>
<p>The below graph illustrates this. A single tree (left) has stair-like, step-wise predictions whereas a random forest is free to predict any value. The color represents the predicted value (light blue = highest, black = lowest).</p>
<p><img src="Exam-PA-Study-Manual_files/figure-html/unnamed-chunk-219-1.png" width="576" /></p>
<p>Unlike decision trees, random forest trees do not need to be pruned. This is because overfitting is less of a problem: if one tree overfits, there are other trees which overfit in other areas to compensate.</p>
<p>In most applications, only the <code>mtry</code> parameter, which controls how many variables to consider at each split, needs to be tuned. Tuning the <code>ntrees</code> parameter is not required; however, the SOA may still ask you to.</p>
<div id="example-1" class="section level3">
<h3><span class="header-section-number">16.8.1</span> Example</h3>
<p>Using the basic <code>randomForest</code> package we fit a model with 500 trees.</p>
<p>This expects only numeric values. We create dummy (indicator) columns.</p>
<div class="sourceCode" id="cb357"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb357-1" title="1">rf_data &lt;-<span class="st"> </span>health_insurance <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb357-2" title="2"><span class="st">  </span><span class="kw">sample_frac</span>(<span class="fl">0.2</span>) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb357-3" title="3"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">sex =</span> <span class="kw">ifelse</span>(sex <span class="op">==</span><span class="st"> &quot;male&quot;</span>, <span class="dv">1</span>, <span class="dv">0</span>),</a>
<a class="sourceLine" id="cb357-4" title="4">         <span class="dt">smoker =</span> <span class="kw">ifelse</span>(smoker <span class="op">==</span><span class="st"> &quot;yes&quot;</span>, <span class="dv">1</span>, <span class="dv">0</span>),</a>
<a class="sourceLine" id="cb357-5" title="5">         <span class="dt">region_ne =</span> <span class="kw">ifelse</span>(region <span class="op">==</span><span class="st"> &quot;northeast&quot;</span>, <span class="dv">1</span>,<span class="dv">0</span>),</a>
<a class="sourceLine" id="cb357-6" title="6">         <span class="dt">region_nw =</span> <span class="kw">ifelse</span>(region <span class="op">==</span><span class="st"> &quot;northwest&quot;</span>, <span class="dv">1</span>,<span class="dv">0</span>),</a>
<a class="sourceLine" id="cb357-7" title="7">         <span class="dt">region_se =</span> <span class="kw">ifelse</span>(region <span class="op">==</span><span class="st"> &quot;southeast&quot;</span>, <span class="dv">1</span>,<span class="dv">0</span>),</a>
<a class="sourceLine" id="cb357-8" title="8">         <span class="dt">region_sw =</span> <span class="kw">ifelse</span>(region <span class="op">==</span><span class="st"> &quot;southwest&quot;</span>, <span class="dv">1</span>,<span class="dv">0</span>)) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb357-9" title="9"><span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>region)</a>
<a class="sourceLine" id="cb357-10" title="10">rf_data <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">glimpse</span>(<span class="dv">50</span>)</a></code></pre></div>
<pre><code>## Observations: 268
## Variables: 10
## $ age       &lt;dbl&gt; 42, 44, 31, 36, 64, 28, 45,...
## $ sex       &lt;dbl&gt; 1, 1, 1, 1, 1, 0, 1, 1, 0, ...
## $ bmi       &lt;dbl&gt; 26.125, 39.520, 27.645, 34....
## $ children  &lt;dbl&gt; 2, 0, 2, 2, 0, 3, 0, 0, 0, ...
## $ smoker    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, ...
## $ charges   &lt;dbl&gt; 7729.646, 6948.701, 5031.27...
## $ region_ne &lt;dbl&gt; 1, 0, 1, 0, 1, 0, 0, 0, 0, ...
## $ region_nw &lt;dbl&gt; 0, 1, 0, 0, 0, 1, 1, 0, 1, ...
## $ region_se &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 1, 0, ...
## $ region_sw &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</code></pre>
<div class="sourceCode" id="cb359"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb359-1" title="1"><span class="kw">library</span>(caret)</a>
<a class="sourceLine" id="cb359-2" title="2"><span class="kw">set.seed</span>(<span class="dv">42</span>)</a>
<a class="sourceLine" id="cb359-3" title="3">index &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(<span class="dt">y =</span> rf_data<span class="op">$</span>charges, </a>
<a class="sourceLine" id="cb359-4" title="4">                             <span class="dt">p =</span> <span class="fl">0.8</span>, <span class="dt">list =</span> F)</a>
<a class="sourceLine" id="cb359-5" title="5">train &lt;-<span class="st"> </span>rf_data <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(index)</a>
<a class="sourceLine" id="cb359-6" title="6">test &lt;-<span class="st"> </span>rf_data <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(<span class="op">-</span>index)</a></code></pre></div>
<div class="sourceCode" id="cb360"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb360-1" title="1">rf &lt;-<span class="st"> </span><span class="kw">randomForest</span>(charges <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> train, <span class="dt">ntree =</span> <span class="dv">400</span>)</a>
<a class="sourceLine" id="cb360-2" title="2"><span class="kw">plot</span>(rf)</a></code></pre></div>
<p><img src="Exam-PA-Study-Manual_files/figure-html/unnamed-chunk-222-1.png" width="576" /></p>
<p>We again use RMSLE. This is lower (better) than a model that uses the average as a baseline.</p>
<div class="sourceCode" id="cb361"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb361-1" title="1">pred &lt;-<span class="st"> </span><span class="kw">predict</span>(rf, test)</a>
<a class="sourceLine" id="cb361-2" title="2">get_rmsle &lt;-<span class="st"> </span><span class="cf">function</span>(y, y_hat){</a>
<a class="sourceLine" id="cb361-3" title="3">  <span class="kw">sqrt</span>(<span class="kw">mean</span>((<span class="kw">log</span>(y) <span class="op">-</span><span class="st"> </span><span class="kw">log</span>(y_hat))<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb361-4" title="4">}</a>
<a class="sourceLine" id="cb361-5" title="5"></a>
<a class="sourceLine" id="cb361-6" title="6"><span class="kw">get_rmsle</span>(test<span class="op">$</span>charges, pred)</a></code></pre></div>
<pre><code>## [1] 0.5252518</code></pre>
<div class="sourceCode" id="cb363"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb363-1" title="1"><span class="kw">get_rmsle</span>(test<span class="op">$</span>charges, <span class="kw">mean</span>(train<span class="op">$</span>charges))</a></code></pre></div>
<pre><code>## [1] 1.118947</code></pre>
</div>
<div id="variable-importance" class="section level3">
<h3><span class="header-section-number">16.8.2</span> Variable Importance</h3>
<p><em>Variable importance</em> is a way of measuring how each variable contributes to the overall model‚Äôs performance. For single decision trees, variable ‚Äúhigher up‚Äù in the tree have greater influence. Statistically, there are two ways of measuring this:</p>
<ol style="list-style-type: decimal">
<li><p>Look at the mean reduction in accuracy when the variable is randomly permuted verses using the actual values from the data. This is done with <code>type = 1</code> (default).</p></li>
<li><p>Use the total decrease in node impurities from splitting on the variable, averaged over all trees. For classification, the node impurity is measured by the Gini index; for regression, it is measured by the residual sum of squares <span class="math inline">\(\text{RSS}\)</span>. This is <code>type = 2</code>.</p></li>
</ol>
<p><code>smoker</code>, <code>bmi</code>, and <code>age</code> are the most importance predictors of charges. As you can imagine, variable importance is a highly useful tool for building models. We could use this to test out newly engineered features, or perform feature selection by taking the top-n features and use them in a different model. Random forests can handle very high dimensional data which allows for many tests to be run at once.</p>
<div class="sourceCode" id="cb365"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb365-1" title="1"><span class="kw">varImpPlot</span>(<span class="dt">x =</span> rf)</a></code></pre></div>
<p><img src="Exam-PA-Study-Manual_files/figure-html/unnamed-chunk-224-1.png" width="576" /></p>
</div>
<div id="partial-dependence" class="section level3">
<h3><span class="header-section-number">16.8.3</span> Partial dependence</h3>
<p>We know which variables are important, but what about the direction of the change? In a linear model we would be able to just look at the sign of the coefficient. In tree-based models, we have a tool called <em>partial dependence</em>. This attempts to measure the change in the predicted value by taking the average <span class="math inline">\(\hat{\mathbf{y}}\)</span> after removing the effects of all other predictors.</p>
<p>Although this is commonly used for trees, this approach is model-agnostic in that any model could be used.</p>
<p>Take a model of two predictors, <span class="math inline">\(\hat{\mathbf{y}} = f(\mathbf{X}_1, \mathbf{X_2})\)</span>. For simplicity, say that <span class="math inline">\(f(x_1, x_2) = 2x_1 + 3x_2\)</span>.</p>
<p>The data looks like this</p>
<div class="sourceCode" id="cb366"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb366-1" title="1">df &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">x1 =</span> <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">2</span>), <span class="dt">x2 =</span> <span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>)) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb366-2" title="2"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">f =</span> <span class="dv">2</span><span class="op">*</span>x1 <span class="op">+</span><span class="st"> </span><span class="dv">3</span><span class="op">*</span>x2)</a>
<a class="sourceLine" id="cb366-3" title="3">df</a></code></pre></div>
<pre><code>## # A tibble: 4 x 3
##      x1    x2     f
##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1     1     3    11
## 2     1     4    14
## 3     2     5    19
## 4     2     6    22</code></pre>
<p>Here is the partial dependence of <code>x1</code> on to <code>f</code>.</p>
<div class="sourceCode" id="cb368"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb368-1" title="1">df <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">group_by</span>(x1) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">summarise</span>(<span class="dt">f =</span> <span class="kw">mean</span>(f))</a></code></pre></div>
<pre><code>## # A tibble: 2 x 2
##      x1     f
##   &lt;dbl&gt; &lt;dbl&gt;
## 1     1  12.5
## 2     2  20.5</code></pre>
<p>This method of using the mean is know as the <em>Monte Carlo</em> method. There are other methods for partial dependence that are not on the syllabus.</p>
<p>For the Random Forest, this is done with <code>pdp::partial()</code>.</p>
<div class="sourceCode" id="cb370"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb370-1" title="1"><span class="kw">library</span>(pdp)</a>
<a class="sourceLine" id="cb370-2" title="2">bmi &lt;-<span class="st"> </span>pdp<span class="op">::</span><span class="kw">partial</span>(rf, <span class="dt">pred.var =</span> <span class="st">&quot;bmi&quot;</span>, </a>
<a class="sourceLine" id="cb370-3" title="3">                    <span class="dt">grid.resolution =</span> <span class="dv">15</span>) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb370-4" title="4"><span class="st">  </span><span class="kw">autoplot</span>() <span class="op">+</span><span class="st"> </span><span class="kw">theme_bw</span>()</a>
<a class="sourceLine" id="cb370-5" title="5">age &lt;-<span class="st"> </span>pdp<span class="op">::</span><span class="kw">partial</span>(rf, <span class="dt">pred.var =</span> <span class="st">&quot;age&quot;</span>, </a>
<a class="sourceLine" id="cb370-6" title="6">                    <span class="dt">grid.resolution =</span> <span class="dv">15</span>) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb370-7" title="7"><span class="st">  </span><span class="kw">autoplot</span>() <span class="op">+</span><span class="st"> </span><span class="kw">theme_bw</span>()</a>
<a class="sourceLine" id="cb370-8" title="8"></a>
<a class="sourceLine" id="cb370-9" title="9"><span class="kw">ggarrange</span>(bmi, age)</a></code></pre></div>
<div class="figure"><span id="fig:unnamed-chunk-227"></span>
<img src="Exam-PA-Study-Manual_files/figure-html/unnamed-chunk-227-1.png" alt="Partial Dependence" width="576" />
<p class="caption">
Figure 16.1: Partial Dependence
</p>
</div>
</div>
<div id="advantages-and-disadvantages-3" class="section level3">
<h3><span class="header-section-number">16.8.4</span> Advantages and disadvantages</h3>
<p><strong>Advantages</strong></p>
<ul>
<li>Resilient to overfitting due to bagging</li>
<li>Measures variable importance</li>
<li>Captures non-linearities</li>
<li>Captures interaction effects</li>
<li>Handles missing values</li>
<li>Only one parameter to tune (mtry, the number of features considered at each split)</li>
</ul>
<p><strong>Disadvantages</strong></p>
<ul>
<li>More difficult to interpret than a single tree because the result depends on many different trees</li>
<li>Often requires over or undersampling when the target is an imbalanced class</li>
<li>Tends to have lower predictive power than boosted trees</li>
<li>Unable to predict beyond training data for regression</li>
</ul>
<table>
<thead>
<tr class="header">
<th>Readings</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ISLR 8.2.1 Bagging</td>
<td></td>
</tr>
<tr class="even">
<td>ISLR 8.1.2 Random Forests</td>
<td></td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="gradient-boosted-trees" class="section level2">
<h2><span class="header-section-number">16.9</span> Gradient Boosted Trees</h2>
<p>Another ensemble learning method is <em>gradient boosting</em>, also known as the Gradient Boosted Machine (GBM). This is one of the most widely-used and powerful machine learning algorithms that is in use today.</p>
<p>Before diving in to the gradient boosting, understanding the AdaBoost algorithm is helpful.</p>
<div id="gradient-boosting" class="section level3">
<h3><span class="header-section-number">16.9.1</span> Gradient Boosting</h3>
<iframe width="560" height="315" src="https://www.youtube.com/embed/3CC4N4z3GJc?rel=0&amp;showinfo=1&amp;playlist=2xudPOBz-vs,jxuNLH5dXCs,StWY5QWMXCw" frameborder="0" allowfullscreen>
</iframe>
<p></br></p>
</div>
<div id="notation" class="section level3">
<h3><span class="header-section-number">16.9.2</span> Notation</h3>
<p>Start with an initial model, which is just a constant prediction of the mean.</p>
<p><span class="math display">\[f = f_0(\mathbf{x_i}) = \frac{1}{n}\sum_{i=1}^ny_i\]</span></p>
<p>Then we update the target (what the model is predicting) by subtracting off the previously predicted value.</p>
<p><span class="math display">\[ \hat{y_i} \leftarrow y_i - f_0(\mathbf{x_i})\]</span></p>
<p>This <span class="math inline">\(\hat{y_i}\)</span> is called the <em>residual</em>. In our example, instead of predicting <code>charges</code>, this would be predicting the residual of <span class="math inline">\(\text{charges}_i - \text{Mean}(\text{charges})\)</span>. We now use this model for the residuals to update the prediction.</p>
<p>If we updated each prediction with the prior residual directly, the algorithm would be unstable. To make this process more gradual, we use a <em>learning rate</em> parameter.</p>
<p>At step 2, we have</p>
<p><span class="math display">\[f = f_0 + \alpha f_1\]</span></p>
<p>Then we go back and fit another weak learner to this residual and repeat.</p>
<p><span class="math display">\[f = f_0 + \alpha f_1 + \alpha f_2\]</span></p>
<p>We then iterate through this process hundreds or thousands of times, slowly improving the prediction.</p>
<p>Because each new tree is fit to <em>residuals</em> instead of the response itself, the process continuously improves the prediction. As the prediction improves, the residuals get smaller and smaller. In random forests, or other bagging algorithms, the model performance is more limited by the individual trees because each only contributes to the overall average. The name is <em>gradient boosting</em> because the residuals are an approximation of the gradient, and gradient descent is how the loss functions are optimized.</p>
<p>Similarly to how GLMs can be used for classification problems through a logit transform (aka logistic regression), GBMs can also be used for classification.</p>
</div>
<div id="parameters" class="section level3">
<h3><span class="header-section-number">16.9.3</span> Parameters</h3>
<p>For random forests, the individual tree parameters do not get tuned. For GBMs, however, these parameters can make a significant difference in model performance.</p>
<p><strong>Boosting parameters:</strong></p>
<ul>
<li><p><code>n.trees</code>: Integer specifying the total number of trees to fit. This is equivalent to the number of iterations and the number of basis functions in the additive expansion. Default is 100.</p></li>
<li><p><code>shrinkage</code>: a shrinkage parameter applied to each tree in the expansion. Also known as the learning rate or step-size reduction; 0.001 to 0.1 usually work, but a smaller learning rate typically requires more trees. Default is 0.1.</p></li>
</ul>
<p><strong>Tree parameters:</strong></p>
<ul>
<li><p><code>interaction.depth</code>: Integer specifying the maximum depth of each tree (i.e., the highest level of variable interactions allowed). A value of 1 implies an additive model, a value of 2 implies a model with up to 2-way interactions, etc. Default is 1.</p></li>
<li><p><code>n.minobsinnode</code>: Integer specifying the minimum number of observations in the terminal nodes of the trees. Note that this is the actual number of observations, not the total weight.</p></li>
</ul>
<p>GBMs are easy to overfit, and the parameters need to be carefully tuned using cross-validation. In the Examples section we go through how to do this.</p>
<p></br></p>
<div class="studytip">
<p>
<strong>Tip:</strong> Whenever fitting a model, use <code>?model_name</code> to get the documentation. The parameters below are from <code>?gbm</code>.
</p>
</div>
</div>
<div id="example-2" class="section level3">
<h3><span class="header-section-number">16.9.4</span> Example</h3>
<p>We fit a gbm below without tuning the parameters for the sake of example.</p>
<div class="sourceCode" id="cb371"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb371-1" title="1"><span class="kw">library</span>(gbm)</a>
<a class="sourceLine" id="cb371-2" title="2">gbm &lt;-<span class="st"> </span><span class="kw">gbm</span>(charges <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> train,</a>
<a class="sourceLine" id="cb371-3" title="3">           <span class="dt">n.trees =</span> <span class="dv">100</span>,</a>
<a class="sourceLine" id="cb371-4" title="4">           <span class="dt">interaction.depth =</span> <span class="dv">2</span>,</a>
<a class="sourceLine" id="cb371-5" title="5">           <span class="dt">n.minobsinnode =</span> <span class="dv">50</span>,</a>
<a class="sourceLine" id="cb371-6" title="6">           <span class="dt">shrinkage =</span> <span class="fl">0.1</span>)</a></code></pre></div>
<pre><code>## Distribution not specified, assuming gaussian ...</code></pre>
<div class="sourceCode" id="cb373"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb373-1" title="1">pred &lt;-<span class="st"> </span><span class="kw">predict</span>(gbm, test, <span class="dt">n.trees =</span> <span class="dv">100</span>)</a>
<a class="sourceLine" id="cb373-2" title="2"></a>
<a class="sourceLine" id="cb373-3" title="3"><span class="kw">get_rmsle</span>(test<span class="op">$</span>charges, pred)</a></code></pre></div>
<pre><code>## [1] 1.052716</code></pre>
<div class="sourceCode" id="cb375"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb375-1" title="1"><span class="kw">get_rmsle</span>(test<span class="op">$</span>charges, <span class="kw">mean</span>(train<span class="op">$</span>charges))</a></code></pre></div>
<pre><code>## [1] 1.118947</code></pre>
</div>
<div id="advantages-and-disadvantages-4" class="section level3">
<h3><span class="header-section-number">16.9.5</span> Advantages and disadvantages</h3>
<p>This exam covers the basics of GBMs. There are many variations of GBMs not covered in detail such as <code>xgboost</code>.</p>
<p><strong>Advantages</strong></p>
<ul>
<li>High prediction accuracy</li>
<li>Shown to work empirically well on many types of problems</li>
<li>Nonlinearities, interaction effects, resilient to outliers, corrects for missing values</li>
<li>Deals with class imbalance directly by weighting observations</li>
</ul>
<p><strong>Disadvantages</strong></p>
<ul>
<li>Requires large sample size</li>
<li>Longer training time</li>
<li>Does not detect linear combinations of features. These must be engineered
Can overfit if not tuned correctly</li>
</ul>
<table>
<thead>
<tr class="header">
<th>Readings</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ISLR 8.2.3 Boosting</td>
<td></td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="exercises-1" class="section level2">
<h2><span class="header-section-number">16.10</span> Exercises</h2>
<div class="sourceCode" id="cb377"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb377-1" title="1"><span class="kw">library</span>(ExamPAData)</a>
<a class="sourceLine" id="cb377-2" title="2"><span class="kw">library</span>(tidyverse)</a></code></pre></div>
<p>Run this code on your computer to answer these exercises.</p>
<div id="rf-with-randomforest" class="section level3">
<h3><span class="header-section-number">16.10.1</span> 1. RF with <code>randomForest</code></h3>
<p>(Part 1 of 2)</p>
<p>The below code is set up to fit a random forest to the <code>soa_mortality</code> data set to predict <code>actual_cnt</code>.</p>
<p>There is a problem: all of the predictions are coming out to be 1. Find out why this is happening and fix it.</p>
<div class="sourceCode" id="cb378"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb378-1" title="1"><span class="kw">set.seed</span>(<span class="dv">42</span>)</a>
<a class="sourceLine" id="cb378-2" title="2"><span class="co">#When writing this book, only 5% of the records were used so that the code runs faster.</span></a>
<a class="sourceLine" id="cb378-3" title="3"><span class="co">#Increase this sampling if running this on your machine.</span></a>
<a class="sourceLine" id="cb378-4" title="4">df &lt;-<span class="st"> </span>soa_mortality <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb378-5" title="5"><span class="st">  </span><span class="kw">sample_frac</span>(<span class="fl">0.05</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="co">#20% sample</span></a>
<a class="sourceLine" id="cb378-6" title="6"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">target =</span> <span class="kw">as.factor</span>(<span class="kw">ifelse</span>(actual_cnt <span class="op">==</span><span class="st"> </span><span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>))) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb378-7" title="7"><span class="st">  </span><span class="kw">select</span>(target, prodcat, distchan, smoker, sex, issage, uwkey) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb378-8" title="8"><span class="st">  </span><span class="kw">mutate_if</span>(is.character, <span class="op">~</span><span class="kw">as.factor</span>(.x))</a>
<a class="sourceLine" id="cb378-9" title="9"></a>
<a class="sourceLine" id="cb378-10" title="10"><span class="co">#check that the target has 0&#39;s and 1&#39;s</span></a>
<a class="sourceLine" id="cb378-11" title="11">df <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">count</span>(target)</a></code></pre></div>
<div class="sourceCode" id="cb379"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb379-1" title="1"><span class="kw">library</span>(caret)</a>
<a class="sourceLine" id="cb379-2" title="2"><span class="kw">library</span>(randomForest)</a>
<a class="sourceLine" id="cb379-3" title="3">index &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(<span class="dt">y =</span> df<span class="op">$</span>target, <span class="dt">p =</span> <span class="fl">0.8</span>, <span class="dt">list =</span> F)</a>
<a class="sourceLine" id="cb379-4" title="4"></a>
<a class="sourceLine" id="cb379-5" title="5">train &lt;-<span class="st"> </span>df <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(index)</a>
<a class="sourceLine" id="cb379-6" title="6">test &lt;-<span class="st"> </span>df <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(<span class="op">-</span>index)</a>
<a class="sourceLine" id="cb379-7" title="7"></a>
<a class="sourceLine" id="cb379-8" title="8">k =<span class="st"> </span><span class="fl">0.5</span></a>
<a class="sourceLine" id="cb379-9" title="9">cutoff=<span class="kw">c</span>(k,<span class="dv">1</span><span class="op">-</span>k) </a>
<a class="sourceLine" id="cb379-10" title="10"></a>
<a class="sourceLine" id="cb379-11" title="11">model &lt;-<span class="st"> </span><span class="kw">randomForest</span>(</a>
<a class="sourceLine" id="cb379-12" title="12">  <span class="dt">formula =</span> target <span class="op">~</span><span class="st"> </span>., </a>
<a class="sourceLine" id="cb379-13" title="13">  <span class="dt">data =</span> train,</a>
<a class="sourceLine" id="cb379-14" title="14">  <span class="dt">ntree =</span> <span class="dv">100</span>,</a>
<a class="sourceLine" id="cb379-15" title="15">  <span class="dt">cutoff =</span> cutoff</a>
<a class="sourceLine" id="cb379-16" title="16">  )</a>
<a class="sourceLine" id="cb379-17" title="17"></a>
<a class="sourceLine" id="cb379-18" title="18">pred &lt;-<span class="st"> </span><span class="kw">predict</span>(model, test)</a>
<a class="sourceLine" id="cb379-19" title="19"><span class="kw">confusionMatrix</span>(pred, test<span class="op">$</span>target)</a></code></pre></div>
<p>(Part 2 of 2)</p>
<p>Downsample the majority class and refit the model, and then choose between the original data and the downsampled data based on the model performance. Use your own judgement when choosing how to evaluate the model based on accuracy, sensitivity, specificity, and Kappa.</p>
<div class="sourceCode" id="cb380"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb380-1" title="1">down_train &lt;-<span class="st"> </span><span class="kw">downSample</span>(<span class="dt">x =</span> train <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(<span class="op">-</span>target),</a>
<a class="sourceLine" id="cb380-2" title="2">                         <span class="dt">y =</span> train<span class="op">$</span>target)</a>
<a class="sourceLine" id="cb380-3" title="3"></a>
<a class="sourceLine" id="cb380-4" title="4">down_test &lt;-<span class="st"> </span><span class="kw">downSample</span>(<span class="dt">x =</span> test <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(<span class="op">-</span>target),</a>
<a class="sourceLine" id="cb380-5" title="5">                         <span class="dt">y =</span> test<span class="op">$</span>target)</a>
<a class="sourceLine" id="cb380-6" title="6"></a>
<a class="sourceLine" id="cb380-7" title="7">down_train <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">count</span>(Class)</a></code></pre></div>
<div class="sourceCode" id="cb381"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb381-1" title="1">model &lt;-<span class="st"> </span><span class="kw">randomForest</span>(</a>
<a class="sourceLine" id="cb381-2" title="2">  <span class="dt">formula =</span> Class <span class="op">~</span><span class="st"> </span>., </a>
<a class="sourceLine" id="cb381-3" title="3">  <span class="dt">data =</span> down_train,</a>
<a class="sourceLine" id="cb381-4" title="4">  <span class="dt">ntree =</span> <span class="dv">100</span>,</a>
<a class="sourceLine" id="cb381-5" title="5">  <span class="dt">cutoff =</span> cutoff</a>
<a class="sourceLine" id="cb381-6" title="6">  )</a>
<a class="sourceLine" id="cb381-7" title="7"></a>
<a class="sourceLine" id="cb381-8" title="8">down_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(model, down_test)</a>
<a class="sourceLine" id="cb381-9" title="9"><span class="kw">confusionMatrix</span>(down_pred, down_test<span class="op">$</span>Class)</a></code></pre></div>
<p>Now up-sample the minority class and repeat the same procedure.</p>
<div class="sourceCode" id="cb382"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb382-1" title="1">up_train &lt;-<span class="st"> </span><span class="kw">upSample</span>(<span class="dt">x =</span> train <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(<span class="op">-</span>target),</a>
<a class="sourceLine" id="cb382-2" title="2">                         <span class="dt">y =</span> train<span class="op">$</span>target)</a>
<a class="sourceLine" id="cb382-3" title="3"></a>
<a class="sourceLine" id="cb382-4" title="4">up_test &lt;-<span class="st"> </span><span class="kw">upSample</span>(<span class="dt">x =</span> test <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(<span class="op">-</span>target),</a>
<a class="sourceLine" id="cb382-5" title="5">                         <span class="dt">y =</span> test<span class="op">$</span>target)</a>
<a class="sourceLine" id="cb382-6" title="6"></a>
<a class="sourceLine" id="cb382-7" title="7">up_train <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">count</span>(Class)</a></code></pre></div>
<div class="sourceCode" id="cb383"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb383-1" title="1">model &lt;-<span class="st"> </span><span class="kw">randomForest</span>(</a>
<a class="sourceLine" id="cb383-2" title="2">  <span class="dt">formula =</span> Class <span class="op">~</span><span class="st"> </span>., </a>
<a class="sourceLine" id="cb383-3" title="3">  <span class="dt">data =</span> up_train,</a>
<a class="sourceLine" id="cb383-4" title="4">  <span class="dt">ntree =</span> <span class="dv">100</span>,</a>
<a class="sourceLine" id="cb383-5" title="5">  <span class="dt">cutoff =</span> cutoff</a>
<a class="sourceLine" id="cb383-6" title="6">  )</a>
<a class="sourceLine" id="cb383-7" title="7"></a>
<a class="sourceLine" id="cb383-8" title="8">up_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(model, up_test)</a>
<a class="sourceLine" id="cb383-9" title="9"><span class="kw">confusionMatrix</span>(up_pred, up_test<span class="op">$</span>Class)</a></code></pre></div>
</div>
<div id="rf-tuning-with-caret" class="section level3">
<h3><span class="header-section-number">16.10.2</span> 2. RF tuning with <code>caret</code></h3>
<p>The best practice of tuning a model is with cross-validation. This can only be done in the <code>caret</code> library. If the SOA asks you to use <code>caret</code>, they will likely ask you a question related to cross validation as below.</p>
<p>An actuary has trained a predictive model and chosen the best hyperparameters, cleaned the data, and performed feature engineering. They have one problem, however: the error on the training data is far lower than on new, unseen test data. Read the code below and determine their problem. Find a way to lower the error on the test data <em>without changing the model or the data.</em> Explain the rational behind your method.</p>
<div class="sourceCode" id="cb384"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb384-1" title="1"><span class="kw">set.seed</span>(<span class="dv">42</span>)</a>
<a class="sourceLine" id="cb384-2" title="2"><span class="co">#Take only 250 records </span></a>
<a class="sourceLine" id="cb384-3" title="3"><span class="co">#Uncomment this when completing this exercise</span></a>
<a class="sourceLine" id="cb384-4" title="4">data &lt;-<span class="st"> </span>health_insurance <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">sample_n</span>(<span class="dv">250</span>) </a>
<a class="sourceLine" id="cb384-5" title="5"></a>
<a class="sourceLine" id="cb384-6" title="6">index &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(</a>
<a class="sourceLine" id="cb384-7" title="7">  <span class="dt">y =</span> data<span class="op">$</span>charges, <span class="dt">p =</span> <span class="fl">0.8</span>, <span class="dt">list =</span> F) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb384-8" title="8"><span class="st">  </span><span class="kw">as.numeric</span>()</a>
<a class="sourceLine" id="cb384-9" title="9">train &lt;-<span class="st">  </span>health_insurance <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(index)</a>
<a class="sourceLine" id="cb384-10" title="10">test &lt;-<span class="st"> </span>health_insurance <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(<span class="op">-</span>index)</a>
<a class="sourceLine" id="cb384-11" title="11"></a>
<a class="sourceLine" id="cb384-12" title="12">control &lt;-<span class="st"> </span><span class="kw">trainControl</span>(</a>
<a class="sourceLine" id="cb384-13" title="13">  <span class="dt">method=</span><span class="st">&#39;boot&#39;</span>, </a>
<a class="sourceLine" id="cb384-14" title="14">  <span class="dt">number=</span><span class="dv">2</span>, </a>
<a class="sourceLine" id="cb384-15" title="15">  <span class="dt">p =</span> <span class="fl">0.2</span>)</a>
<a class="sourceLine" id="cb384-16" title="16"></a>
<a class="sourceLine" id="cb384-17" title="17">tunegrid &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">.mtry=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">5</span>))</a>
<a class="sourceLine" id="cb384-18" title="18">rf &lt;-<span class="st"> </span><span class="kw">train</span>(charges <span class="op">~</span><span class="st"> </span>.,</a>
<a class="sourceLine" id="cb384-19" title="19">            <span class="dt">data =</span> train,</a>
<a class="sourceLine" id="cb384-20" title="20">            <span class="dt">method=</span><span class="st">&#39;rf&#39;</span>, </a>
<a class="sourceLine" id="cb384-21" title="21">            <span class="dt">tuneGrid=</span>tunegrid, </a>
<a class="sourceLine" id="cb384-22" title="22">            <span class="dt">trControl=</span>control)</a>
<a class="sourceLine" id="cb384-23" title="23"></a>
<a class="sourceLine" id="cb384-24" title="24">pred_train &lt;-<span class="st"> </span><span class="kw">predict</span>(rf, train)</a>
<a class="sourceLine" id="cb384-25" title="25">pred_test &lt;-<span class="st"> </span><span class="kw">predict</span>(rf, test)</a>
<a class="sourceLine" id="cb384-26" title="26"></a>
<a class="sourceLine" id="cb384-27" title="27">get_rmse &lt;-<span class="st"> </span><span class="cf">function</span>(y, y_hat){</a>
<a class="sourceLine" id="cb384-28" title="28">  <span class="kw">sqrt</span>(<span class="kw">mean</span>((y <span class="op">-</span><span class="st"> </span>y_hat)<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb384-29" title="29">}</a>
<a class="sourceLine" id="cb384-30" title="30"></a>
<a class="sourceLine" id="cb384-31" title="31"><span class="kw">get_rmse</span>(pred_train, train<span class="op">$</span>charges)</a>
<a class="sourceLine" id="cb384-32" title="32"><span class="kw">get_rmse</span>(pred_test, test<span class="op">$</span>charges)</a></code></pre></div>
</div>
<div id="tuning-a-gbm-with-caret" class="section level3">
<h3><span class="header-section-number">16.10.3</span> 3. Tuning a GBM with <code>caret</code></h3>
<p>If the SOA asks you to tune a GBM, they will need to give you starting hyperparameters which are close to the ‚Äúbest‚Äù values due to how slow the Prometric computers are. Another possibility is that they pre-train a GBM model object and ask that you use it.</p>
<p>This example looks at 135 combinations of hyper parameters.</p>
<div class="sourceCode" id="cb385"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb385-1" title="1"><span class="kw">set.seed</span>(<span class="dv">42</span>)</a>
<a class="sourceLine" id="cb385-2" title="2">index &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(<span class="dt">y =</span> health_insurance<span class="op">$</span>charges, </a>
<a class="sourceLine" id="cb385-3" title="3">                             <span class="dt">p =</span> <span class="fl">0.8</span>, <span class="dt">list =</span> F)</a>
<a class="sourceLine" id="cb385-4" title="4"><span class="co">#To make this run faster, only take 50% sample</span></a>
<a class="sourceLine" id="cb385-5" title="5">df &lt;-<span class="st"> </span>health_insurance <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">sample_frac</span>(<span class="fl">0.50</span>) </a>
<a class="sourceLine" id="cb385-6" title="6">train &lt;-<span class="st"> </span>df <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(index) </a>
<a class="sourceLine" id="cb385-7" title="7">test &lt;-<span class="st"> </span>df <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">sample_frac</span>(<span class="fl">0.05</span>)<span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(<span class="op">-</span>index)</a>
<a class="sourceLine" id="cb385-8" title="8"></a>
<a class="sourceLine" id="cb385-9" title="9">tunegrid &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(</a>
<a class="sourceLine" id="cb385-10" title="10">    <span class="dt">interaction.depth =</span> <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">5</span>, <span class="dv">10</span>),</a>
<a class="sourceLine" id="cb385-11" title="11">    <span class="dt">n.trees =</span> <span class="kw">c</span>(<span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">200</span>, <span class="dv">300</span>, <span class="dv">400</span>), </a>
<a class="sourceLine" id="cb385-12" title="12">    <span class="dt">shrinkage =</span> <span class="kw">c</span>(<span class="fl">0.5</span>, <span class="fl">0.1</span>, <span class="fl">0.0001</span>),</a>
<a class="sourceLine" id="cb385-13" title="13">    <span class="dt">n.minobsinnode =</span> <span class="kw">c</span>(<span class="dv">5</span>, <span class="dv">30</span>, <span class="dv">100</span>)</a>
<a class="sourceLine" id="cb385-14" title="14">    )</a>
<a class="sourceLine" id="cb385-15" title="15"><span class="kw">nrow</span>(tunegrid)</a>
<a class="sourceLine" id="cb385-16" title="16"></a>
<a class="sourceLine" id="cb385-17" title="17">control &lt;-<span class="st"> </span><span class="kw">trainControl</span>(</a>
<a class="sourceLine" id="cb385-18" title="18">  <span class="dt">method=</span><span class="st">&#39;repeatedcv&#39;</span>, </a>
<a class="sourceLine" id="cb385-19" title="19">  <span class="dt">number=</span><span class="dv">5</span>, </a>
<a class="sourceLine" id="cb385-20" title="20">  <span class="dt">p =</span> <span class="fl">0.8</span>)</a></code></pre></div>
<div class="sourceCode" id="cb386"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb386-1" title="1">gbm &lt;-<span class="st"> </span><span class="kw">train</span>(charges <span class="op">~</span><span class="st"> </span>.,</a>
<a class="sourceLine" id="cb386-2" title="2">            <span class="dt">data =</span> train,</a>
<a class="sourceLine" id="cb386-3" title="3">            <span class="dt">method=</span><span class="st">&#39;gbm&#39;</span>, </a>
<a class="sourceLine" id="cb386-4" title="4">            <span class="dt">tuneGrid=</span>tunegrid, </a>
<a class="sourceLine" id="cb386-5" title="5">            <span class="dt">trControl=</span>control,</a>
<a class="sourceLine" id="cb386-6" title="6">            <span class="co">#Show detailed output</span></a>
<a class="sourceLine" id="cb386-7" title="7">            <span class="dt">verbose =</span> <span class="ot">FALSE</span></a>
<a class="sourceLine" id="cb386-8" title="8">            )</a></code></pre></div>
<p>The output shows the RMSE for each of the 135 models tested.</p>
<p>(Part 1 of 3)</p>
<p>Identify the hyperpameter combination that has the lowest training error.</p>
<p>(Part 2 of 3)</p>
<ol start="2" style="list-style-type: decimal">
<li>Suppose that the optimization measure was RMSE. The below table shows the results from three models. Explain why some sets of parameters have better RMSE than the others.</li>
</ol>
<div class="sourceCode" id="cb387"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb387-1" title="1">results &lt;-<span class="st"> </span>gbm<span class="op">$</span>results <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">arrange</span>(RMSE)</a>
<a class="sourceLine" id="cb387-2" title="2">top_result &lt;-<span class="st"> </span>results <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(<span class="dv">1</span>)<span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">param_rank =</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb387-3" title="3">tenth_result &lt;-<span class="st"> </span>results <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(<span class="dv">10</span>)<span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">param_rank =</span> <span class="dv">10</span>)</a>
<a class="sourceLine" id="cb387-4" title="4">twenty_seventh_result &lt;-<span class="st"> </span>results <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(<span class="dv">135</span>)<span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">param_rank =</span> <span class="dv">135</span>)</a>
<a class="sourceLine" id="cb387-5" title="5"></a>
<a class="sourceLine" id="cb387-6" title="6"><span class="kw">rbind</span>(top_result, tenth_result, twenty_seventh_result) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb387-7" title="7"><span class="st">  </span><span class="kw">select</span>(param_rank, <span class="dv">1</span><span class="op">:</span><span class="dv">5</span>)</a></code></pre></div>
<ol start="3" style="list-style-type: decimal">
<li>The partial dependence of <code>bmi</code> onto <code>charges</code> makes it appear as if <code>charges</code> increases monotonically as <code>bmi</code> increases.</li>
</ol>
<div class="sourceCode" id="cb388"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb388-1" title="1">pdp<span class="op">::</span><span class="kw">partial</span>(gbm, <span class="dt">pred.var =</span> <span class="st">&quot;bmi&quot;</span>, <span class="dt">grid.resolution =</span> <span class="dv">15</span>, <span class="dt">plot =</span> T)</a></code></pre></div>
<p>However, when we add in the <code>ice</code> curves, we see that there is something else going on. Explain this graph. Why are there two groups of lines?</p>
<div class="sourceCode" id="cb389"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb389-1" title="1">pdp<span class="op">::</span><span class="kw">partial</span>(gbm, <span class="dt">pred.var =</span> <span class="st">&quot;bmi&quot;</span>, <span class="dt">grid.resolution =</span> <span class="dv">20</span>, <span class="dt">plot =</span> T, <span class="dt">ice =</span> T, <span class="dt">alpha =</span> <span class="fl">0.1</span>, <span class="dt">palette =</span> <span class="st">&quot;viridis&quot;</span>)</a></code></pre></div>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="bias-variance-trade-off.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="unsupervised-learning.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/sdcastillo/PA-R-Study-Manual/edit/master/05-tree-based-models.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Exam-PA-Study-Manual.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
