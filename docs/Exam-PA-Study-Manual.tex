\documentclass[openany]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Exam PA Study Manual},
            pdfauthor={Sam Castillo},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{natbib}
\bibliographystyle{apalike}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\providecommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{Exam PA Study Manual}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{Sam Castillo}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
      \predate{\centering\large\emph}
  \postdate{\par}
    \date{2020-01-03}

\usepackage{booktabs}
\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{whats-in-this-book}{%
\chapter{What's in this book}\label{whats-in-this-book}}

\begin{itemize}
\tightlist
\item
  Explanations of the statistical concepts
\item
  All data sets needed packaged in an R library
\item
  \texttt{R\ code} examples
\end{itemize}

\hypertarget{about-the-author}{%
\section{About the author}\label{about-the-author}}

Sam Castillo is a predictive modeler at Milliman in Chicago, maintains a \href{http://artificialactuary.com/}{blog} about the future of risk, and won the 2019 SOA Predictive Analytics and Fururism's \href{https://nbviewer.jupyter.org/github/SOASections/SOA-Predictive-Modeling-Innovation-and-Industry-Contest-2019-First-Place/blob/master/Predicting\%20Uncertainty\%20Prediction\%20Intervals\%20from\%20Gradient\%20Boosted\%20Quantile\%20Regression.ipynb}{Jupyter contest}.

\textbf{Contact:}

Support: \href{mailto:sam@futuroinsight.com}{\nolinkurl{sam@futuroinsight.com}}

\hypertarget{the-exam}{%
\chapter{The exam}\label{the-exam}}

The main challenge of this exam is in communication: both understanding what they want you to do as well as telling the grader what it is that you did.

You will have 5 hours and 15 minutes to use RStudio and Excel to fill out a report in Word on a Prometric computer. The syllabus uses fancy language to describe the topics covered on the exam, making it sound more difficult than it should be. A good analogy is a job description that has many complex-sounding tasks, when in reality the day-to-day operations of the employee are far simpler.

A non-technical translation is as follows:

\textbf{Writing in Microsoft Word (30-40\%)}

\begin{itemize}
\tightlist
\item
  Write in professional language
\item
  Type more than 50 words-per-minute
\end{itemize}

\textbf{Manipulating Data in R (15-25\%)}

\begin{itemize}
\tightlist
\item
  Quickly clean data sets
\item
  Find data errors planted by the SOA
\item
  Perform queries (aggregations, summaries, transformations)
\end{itemize}

\textbf{Machine learning and statistics (40-50\%)}

\begin{itemize}
\tightlist
\item
  Interpret results within a business context
\item
  Change model parameters
\end{itemize}

\hypertarget{prometric-demo}{%
\chapter{Prometric Demo}\label{prometric-demo}}

The following video from Prometric shows what the computer set up will look like. In addition to the files shown in the video, they will give you a printed out project statement (If they don't give this to you right away, ask for it.)

SOAFinalCut from Prometric on Vimeo.

\hypertarget{you-already-know-what-learning-is}{%
\chapter{You already know what learning is}\label{you-already-know-what-learning-is}}

All of use are already familiar with how to learn - by improving from our mistakes. By repeating what is successful and avoiding what results in failure, we learn by doing, by experience, or trial-and-error. Machines learn in a similar way.

Take for example the process of studying for an exam. Some study methods work well, but other methods do not. The ``data'' are the practice problems, and the ``label'' is the answer (A,B,C,D,E). We want to build a mental ``model'' that reads the question and predicts the answer.

We all know that memorizing answers without understanding concepts is ineffective, and statistics calls this ``overfitting''. Conversely, not learning enough of the details and only learning the high-level concepts is ``underfitting''.

The more practice problems that we do, the larger the training data set, and the better the prediction. When we see new problems, ones which have not appeared in the practice exams, we often have a difficult time. Quizing ourselves on realistic questions estimates our preparedness, and this is identical to a process known as ``holdout testing'' or ``cross-validation''.

We can clearly state our objective: get as many correct answers as possible! We want to correctly predict the solution to every problem. Said another way, we are trying to minimize the error, known as the ``loss function''.

Different study methods work well for different people. Some cover material quickly and others slowly absorb every detail. A model has many ``parameters'' such as the ``learning rate''. The only way to know which parameters are best is to test them on real data, known as ``training''.

\hypertarget{getting-started}{%
\chapter{Getting started}\label{getting-started}}

\hypertarget{download-the-data}{%
\section{Download the data}\label{download-the-data}}

For your convenience, all data in this book, including data from prior exams and sample solutions, has been put into a library called \texttt{ExamPAData} by the author. To access, simplly run the below lines of code to download this data.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Install remotes if it's not yet installed}
\CommentTok{# install.packages("remotes")}
\NormalTok{remotes}\OperatorTok{::}\KeywordTok{install_github}\NormalTok{(}\StringTok{"sdcastillo/ExamPAData"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Once this has run, you can access the data using \texttt{library(ExamPAData)}. To check that this is installed correctly see if the \texttt{insurance} data set has loaded. If this returns ``object not found'', then the library was not installed.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ExamPAData)}
\KeywordTok{summary}\NormalTok{(insurance)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     district       group               age               holders       
##  Min.   :1.00   Length:64          Length:64          Min.   :   3.00  
##  1st Qu.:1.75   Class :character   Class :character   1st Qu.:  46.75  
##  Median :2.50   Mode  :character   Mode  :character   Median : 136.00  
##  Mean   :2.50                                         Mean   : 364.98  
##  3rd Qu.:3.25                                         3rd Qu.: 327.50  
##  Max.   :4.00                                         Max.   :3582.00  
##      claims      
##  Min.   :  0.00  
##  1st Qu.:  9.50  
##  Median : 22.00  
##  Mean   : 49.23  
##  3rd Qu.: 55.50  
##  Max.   :400.00
\end{verbatim}

\hypertarget{download-islr}{%
\section{Download ISLR}\label{download-islr}}

This book references the publically-avialable textbook ``An Introduction to Statistical Learning'', which can be downloaded for free

\url{http://faculty.marshall.usc.edu/gareth-james/ISL/}

If you already have R and RStudio installed then skip to ``Download the data''.

\hypertarget{new-users}{%
\section{New users}\label{new-users}}

Install R:

This is the engine that \emph{runs} the code. \url{https://cran.r-project.org/mirrors.html}

Install RStudio

This is the tool that helps you to \emph{write} the code. Just as MS Word creates documents, RStudio creates R scripts and other documents. Download RStudio Desktop (the free edition) and choose a place on your computer to install it.

\url{https://rstudio.com/products/rstudio/download/}

Set the R library

R code is organized into libraries. You want to use the exact same code that will be on the Prometric Computers. This requires installing older versions of libraries. Change your R library to the one which was included within the SOA's modules.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{.libPaths}\NormalTok{(}\StringTok{"PATH_TO_SOAS_LIBRARY/PAlibrary"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{r-programming}{%
\chapter{R programming}\label{r-programming}}

This chapter covers the bare minimum of R programming needed for Exam PA. The book
``R for Data Science'' provides more detail.

\url{https://r4ds.had.co.nz/}

\hypertarget{notebook-chunks}{%
\section{Notebook chunks}\label{notebook-chunks}}

On the Exam, you will start with an .Rmd (R Markdown) template, which organize
code into \href{https://bookdown.org/yihui/rmarkdown/notebook.html}{R Notebooks}.
Within each notebook, code is organized into chunks.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# This is a chunk}
\end{Highlighting}
\end{Shaded}

Your time is valuable. Throughout this book, I will include useful keyboard shortcuts.

\begin{quote}
\textbf{Shortcut:} To run everything in a chunk quickly, press \texttt{CTRL\ +\ SHIFT\ +\ ENTER}.
To create a new chunk, use \texttt{CTRL\ +\ ALT\ +\ I}.
\end{quote}

\hypertarget{basic-operations}{%
\section{Basic operations}\label{basic-operations}}

The usual math operations apply.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Addition}
\DecValTok{1} \OperatorTok{+}\StringTok{ }\DecValTok{2} 
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{3} \OperatorTok{-}\StringTok{ }\DecValTok{2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Multiplication}
\DecValTok{2} \OperatorTok{*}\StringTok{ }\DecValTok{2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Division}
\DecValTok{4} \OperatorTok{/}\StringTok{ }\DecValTok{2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Exponentiation}
\DecValTok{2}\OperatorTok{^}\DecValTok{3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 8
\end{verbatim}

There are two assignment operators: \texttt{=} and \texttt{\textless{}-}. The latter is preferred because
it is specific to assigning a variable to a value. The \texttt{=} operator is also used
for specifying arguments in functions (see the functions section).

\begin{quote}
\textbf{Shortcut:} \texttt{ALT\ +\ -} creates a \texttt{\textless{}-}..
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Variable assignment}
\NormalTok{y <-}\StringTok{ }\DecValTok{2}

\CommentTok{# Equality}
\DecValTok{4} \OperatorTok{==}\StringTok{ }\DecValTok{2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{5} \OperatorTok{==}\StringTok{ }\DecValTok{5}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{3.14} \OperatorTok{>}\StringTok{ }\DecValTok{3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{3.14} \OperatorTok{>=}\StringTok{ }\DecValTok{3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

Vectors can be added just like numbers. The \texttt{c} stands for ``concatenate'', which
creates vectors.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\NormalTok{y <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{)}
\NormalTok{x }\OperatorTok{+}\StringTok{ }\NormalTok{y}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4 6
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OperatorTok{*}\StringTok{ }\NormalTok{y}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3 8
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{z <-}\StringTok{ }\NormalTok{x }\OperatorTok{+}\StringTok{ }\NormalTok{y}
\NormalTok{z}\OperatorTok{^}\DecValTok{2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 16 36
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{z }\OperatorTok{/}\StringTok{ }\DecValTok{2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2 3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{z }\OperatorTok{+}\StringTok{ }\DecValTok{3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 7 9
\end{verbatim}

I already mentioned \texttt{numeric} types. There are also \texttt{character} (string) types,
\texttt{factor} types, and \texttt{boolean} types.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{character <-}\StringTok{ "The"}
\NormalTok{character_vector <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"The"}\NormalTok{, }\StringTok{"Quick"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Character vectors can be combined with the \texttt{paste()} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a <-}\StringTok{ "The"}
\NormalTok{b <-}\StringTok{ "Quick"}
\NormalTok{c <-}\StringTok{ "Brown"}
\NormalTok{d <-}\StringTok{ "Fox"}
\KeywordTok{paste}\NormalTok{(a, b, c, d)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "The Quick Brown Fox"
\end{verbatim}

Factors look like character vectors but can only contain a finite number of predefined
values.

The below factor has only one ``level'', which is the list of assigned values.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{factor <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(character)}
\KeywordTok{levels}\NormalTok{(factor)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "The"
\end{verbatim}

The levels of a factor are by default in R in alphabetical order (Q comes alphabetically
before T).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{factor_vector <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(character_vector)}
\KeywordTok{levels}\NormalTok{(factor_vector)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Quick" "The"
\end{verbatim}

\textbf{In building linear models, the order of the factors matters.} In GLMs, the
``reference level'' or ``base level'' should always be the level which has the most
observations. This will be covered in the section on linear models.

Booleans are just \texttt{TRUE} and \texttt{FALSE} values. R understands \texttt{T} or \texttt{TRUE} in the
same way, but the latter is preferred. When doing math, bools are converted to
0/1 values where 1 is equivalent to TRUE and 0 FALSE.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bool_true <-}\StringTok{ }\OtherTok{TRUE}
\NormalTok{bool_false <-}\StringTok{ }\OtherTok{FALSE}
\NormalTok{bool_true }\OperatorTok{*}\StringTok{ }\NormalTok{bool_false}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0
\end{verbatim}

Booleans are automatically converted into 0/1 values when there is a math operation.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bool_true }\OperatorTok{+}\StringTok{ }\DecValTok{1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2
\end{verbatim}

Vectors work in the same way.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bool_vect <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\OtherTok{TRUE}\NormalTok{, }\OtherTok{TRUE}\NormalTok{, }\OtherTok{FALSE}\NormalTok{)}
\KeywordTok{sum}\NormalTok{(bool_vect)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2
\end{verbatim}

Vectors are indexed using \texttt{{[}}. If you are only extracting a single element, you
should use \texttt{{[}{[}} for clarity.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{abc <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"a"}\NormalTok{, }\StringTok{"b"}\NormalTok{, }\StringTok{"c"}\NormalTok{)}
\NormalTok{abc[[}\DecValTok{1}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "a"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{abc[[}\DecValTok{2}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "b"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{abc[}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "a" "c"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{abc[}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "a" "b"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{abc[}\OperatorTok{-}\DecValTok{2}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "a" "c"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{abc[}\OperatorTok{-}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "a"
\end{verbatim}

\hypertarget{lists}{%
\section{Lists}\label{lists}}

Lists are vectors that can hold mixed object types.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my_list <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\OtherTok{TRUE}\NormalTok{, }\StringTok{"Character"}\NormalTok{, }\FloatTok{3.14}\NormalTok{)}
\NormalTok{my_list}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [[1]]
## [1] TRUE
## 
## [[2]]
## [1] "Character"
## 
## [[3]]
## [1] 3.14
\end{verbatim}

Lists can be named.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my_list <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DataTypeTok{bool =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{character =} \StringTok{"character"}\NormalTok{, }\DataTypeTok{numeric =} \FloatTok{3.14}\NormalTok{)}
\NormalTok{my_list}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $bool
## [1] TRUE
## 
## $character
## [1] "character"
## 
## $numeric
## [1] 3.14
\end{verbatim}

The \texttt{\$} operator indexes lists.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my_list}\OperatorTok{$}\NormalTok{numeric}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3.14
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my_list}\OperatorTok{$}\NormalTok{numeric }\OperatorTok{+}\StringTok{ }\DecValTok{5}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 8.14
\end{verbatim}

Lists can also be indexed using \texttt{{[}{[}}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my_list[[}\DecValTok{1}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my_list[[}\DecValTok{2}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "character"
\end{verbatim}

Lists can contain vectors, other lists, and any other object.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{everything <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DataTypeTok{vector =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{), }
                   \DataTypeTok{character =} \KeywordTok{c}\NormalTok{(}\StringTok{"a"}\NormalTok{, }\StringTok{"b"}\NormalTok{, }\StringTok{"c"}\NormalTok{), }
                   \DataTypeTok{list =}\NormalTok{ my_list)}
\NormalTok{everything}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $vector
## [1] 1 2 3
## 
## $character
## [1] "a" "b" "c"
## 
## $list
## $list$bool
## [1] TRUE
## 
## $list$character
## [1] "character"
## 
## $list$numeric
## [1] 3.14
\end{verbatim}

To find out the type of an object, use \texttt{class} or \texttt{str} or \texttt{summary}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "numeric"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{(everything)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "list"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(everything)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## List of 3
##  $ vector   : num [1:3] 1 2 3
##  $ character: chr [1:3] "a" "b" "c"
##  $ list     :List of 3
##   ..$ bool     : logi TRUE
##   ..$ character: chr "character"
##   ..$ numeric  : num 3.14
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(everything)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           Length Class  Mode     
## vector    3      -none- numeric  
## character 3      -none- character
## list      3      -none- list
\end{verbatim}

\hypertarget{functions}{%
\section{Functions}\label{functions}}

You only need to understand the very basics of functions. The big picture, though, is that
understanding functions helps you to understand \emph{everything} in R, since R is a
functional \href{http://adv-r.had.co.nz/Functional-programming.html}{programming language},
unlike Python, C, VBA, Java which are all object-oriented, or SQL which isn't
really a language but a series of set-operations.

Functions do things. The convention is to name a function as a verb. The function
\texttt{make\_rainbows()} would create a rainbow. The function \texttt{summarise\_vectors()}
would summarise vectors. Functions may or may not have an input and output.

If you need to do something in R, there is a high probability that someone has
already written a function to do it. That being said, creating simple functions
is quite useful.

Here is an example that has a side effect of printing the input:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{greet_me <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(my_name)\{}
  \KeywordTok{print}\NormalTok{(}\KeywordTok{paste0}\NormalTok{(}\StringTok{"Hello, "}\NormalTok{, my_name))}
\NormalTok{\}}

\KeywordTok{greet_me}\NormalTok{(}\StringTok{"Future Actuary"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Hello, Future Actuary"
\end{verbatim}

\textbf{A function that returns something}

When returning the last evaluated expression, the \texttt{return} statement is optional.
In fact, it is discouraged by convention.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{add_together <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x, y) \{}
\NormalTok{  x }\OperatorTok{+}\StringTok{ }\NormalTok{y}
\NormalTok{\}}

\KeywordTok{add_together}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 7
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{add_together <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x, y) \{}
  \CommentTok{# Works, but bad practice}
  \KeywordTok{return}\NormalTok{(x }\OperatorTok{+}\StringTok{ }\NormalTok{y)}
\NormalTok{\}}

\KeywordTok{add_together}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 7
\end{verbatim}

Binary operations in R are vectorized. In other words, they are applied element-wise.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x_vector <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\NormalTok{y_vector <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{)}
\KeywordTok{add_together}\NormalTok{(x_vector, y_vector)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5 7 9
\end{verbatim}

Many functions in R actually return lists! This is why R objects can be indexed
with dollar sign.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ExamPAData)}
\NormalTok{model <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(charges }\OperatorTok{~}\StringTok{ }\NormalTok{age, }\DataTypeTok{data =}\NormalTok{ health_insurance)}
\NormalTok{model}\OperatorTok{$}\NormalTok{coefficients}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## (Intercept)         age 
##   3165.8850    257.7226
\end{verbatim}

Here's a function that returns a list.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sum_multiply <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x,y) \{}
\NormalTok{  sum <-}\StringTok{ }\NormalTok{x }\OperatorTok{+}\StringTok{ }\NormalTok{y}
\NormalTok{  product <-}\StringTok{ }\NormalTok{x }\OperatorTok{*}\StringTok{ }\NormalTok{y}
  \KeywordTok{list}\NormalTok{(}\StringTok{"Sum"}\NormalTok{ =}\StringTok{ }\NormalTok{sum, }\StringTok{"Product"}\NormalTok{ =}\StringTok{ }\NormalTok{product)}
\NormalTok{\}}

\NormalTok{result <-}\StringTok{ }\KeywordTok{sum_multiply}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\NormalTok{result}\OperatorTok{$}\NormalTok{Sum}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{result}\OperatorTok{$}\NormalTok{Product}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 6
\end{verbatim}

\hypertarget{data-frames}{%
\section{Data frames}\label{data-frames}}

You can think of a data frame as a table that is implemented as a list of vectors.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}
  \DataTypeTok{age =} \KeywordTok{c}\NormalTok{(}\DecValTok{25}\NormalTok{, }\DecValTok{35}\NormalTok{),}
  \DataTypeTok{has_fsa =} \KeywordTok{c}\NormalTok{(}\OtherTok{FALSE}\NormalTok{, }\OtherTok{TRUE}\NormalTok{)}
\NormalTok{)}
\NormalTok{df}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   age has_fsa
## 1  25   FALSE
## 2  35    TRUE
\end{verbatim}

You can also work with tibbles, which are data frames but have nicer printing:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# The tidyverse library has functions for making tibbles}
\KeywordTok{library}\NormalTok{(tidyverse) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## -- Attaching packages --------------------------------------------------------------------------- tidyverse 1.2.1 --
\end{verbatim}

\begin{verbatim}
## v ggplot2 3.2.1     v purrr   0.3.2
## v tibble  2.1.3     v dplyr   0.8.3
## v tidyr   1.0.0     v stringr 1.4.0
## v readr   1.3.1     v forcats 0.4.0
\end{verbatim}

\begin{verbatim}
## -- Conflicts ------------------------------------------------------------------------------ tidyverse_conflicts() --
## x dplyr::filter() masks stats::filter()
## x dplyr::lag()    masks stats::lag()
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}
  \DataTypeTok{age =} \KeywordTok{c}\NormalTok{(}\DecValTok{25}\NormalTok{, }\DecValTok{35}\NormalTok{), }\DataTypeTok{has_fsa =} \KeywordTok{c}\NormalTok{(}\OtherTok{FALSE}\NormalTok{, }\OtherTok{TRUE}\NormalTok{)}
\NormalTok{)}
\NormalTok{df}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 2
##     age has_fsa
##   <dbl> <lgl>  
## 1    25 FALSE  
## 2    35 TRUE
\end{verbatim}

To index columns in a tibble, the same ``\$'' is used as indexing a list.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df}\OperatorTok{$}\NormalTok{age}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 25 35
\end{verbatim}

To find the number of rows and columns, use \texttt{dim}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dim}\NormalTok{(df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2 2
\end{verbatim}

To find a summary, use \texttt{summary}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       age        has_fsa       
##  Min.   :25.0   Mode :logical  
##  1st Qu.:27.5   FALSE:1        
##  Median :30.0   TRUE :1        
##  Mean   :30.0                  
##  3rd Qu.:32.5                  
##  Max.   :35.0
\end{verbatim}

\hypertarget{pipes}{%
\section{Pipes}\label{pipes}}

The pipe operator \texttt{\%\textgreater{}\%} is a way of making code \emph{modular}, meaning that it can
be written and executed in incremental steps. Those familiar with Python's Pandas
will be see that \texttt{\%\textgreater{}\%} is quite similar to ``.''. This also makes code easier to
read.

In five seconds, tell me what the below code is doing.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{log}\NormalTok{(}\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{exp}\NormalTok{(}\KeywordTok{log2}\NormalTok{(}\KeywordTok{sqrt}\NormalTok{((}\KeywordTok{max}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{16}\NormalTok{))))))))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1
\end{verbatim}

Getting to the answer of 1 requires starting from the inner-most nested brackets
and moving outwards from right to left.

The math notation would be slightly easier to read, but still painful.

\[log(\sqrt{e^{log_2(\sqrt{max(3,4,16)})}})\]

Here is the same algebra using the pipe. To read this, replace the \texttt{\%\textgreater{}\%} with
the word \texttt{THEN}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{max}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{16}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{sqrt}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{log2}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{exp}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{sqrt}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{log}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# max(c(3, 4, 16) THEN  # The max of 3, 4, and 16 is 16}
\CommentTok{#  sqrt() THEN          # The square root of 16 is 4}
\CommentTok{#  log2() THEN          # The log in base 2 of 4 is 2}
\CommentTok{#  exp() THEN           # The exponent of 2 is e^2}
\CommentTok{#  sqrt() THEN          # The square root of e^2 is e}
\CommentTok{#  log()                # The natural logarithm of e is 1}
\end{Highlighting}
\end{Shaded}

Pipes are exceptionally useful for data manipulations, which is covered in the
next chapter.

\begin{quote}
\textbf{Tip:} To quickly produce pipes, use \texttt{CTRL\ +\ SHIFT\ +\ M}.
\end{quote}

By highlighting only certain sections, we can run the code in steps as if we were
using a debugger. This makes testing out code much faster.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{max}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{16}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{max}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{16}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{sqrt}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{max}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{16}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{sqrt}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{log2}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{max}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{16}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{sqrt}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{log2}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{exp}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 7.389056
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{max}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{16}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{sqrt}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{log2}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{exp}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{sqrt}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2.718282
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{max}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{16}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{sqrt}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{log2}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{exp}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{sqrt}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{log}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1
\end{verbatim}

\hypertarget{the-soas-code-doesnt-use-pipes-or-dplyr-so-can-i-skip-learning-this}{%
\section{The SOA's code doesn't use pipes or dplyr, so can I skip learning this?}\label{the-soas-code-doesnt-use-pipes-or-dplyr-so-can-i-skip-learning-this}}

Yes, if you really want to.

The advantages to learning pipes, and the reason why this manual uses them are

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  It saves you time.
\item
  It will help you in real life data science projects.
\item
  The majority of the R community uses this style.
\item
  The SOA actuaries who create the Exam PA content will eventually catch on.
\item
  Most modern R software is designed around them. The overall trend is towards greater adoption, as can bee seen from the CRAN download statistics \href{https://hadley.shinyapps.io/cran-downloads/}{here} after filtering to ``magrittr'' which is the library where the pipe comes from.
\end{enumerate}

\hypertarget{data-manipulation}{%
\chapter{Data manipulation}\label{data-manipulation}}

About two hours in this exam will be spent just on data manipulation. Putting in extra practice in this area is garanteed to give you a better score because it will free up time that you can use elsewhere. In addition, a common saying when building models is ``garbage in means garbage out'', on this exam, mistakes on the data manipulation can lead to lost points on the modeling sections.

Suggested reading of \emph{R for Data Science} (\url{https://r4ds.had.co.nz/index.html}):

\begin{longtable}[]{@{}ll@{}}
\toprule
Chapter & Topic\tabularnewline
\midrule
\endhead
9 & Introduction\tabularnewline
10 & Tibbles\tabularnewline
12 & Tidy data\tabularnewline
15 & Factors\tabularnewline
17 & Introduction\tabularnewline
18 & Pipes\tabularnewline
19 & Functions\tabularnewline
20 & Vectors\tabularnewline
\bottomrule
\end{longtable}

All data for this book can be accessed from the package \texttt{ExamPAData}. In the real exam, you will read the file from the Prometric computer. To read files into R, the \href{https://readr.tidyverse.org/articles/readr.html}{readr} package has several tools, one for each data format. For instance, the most common format, comma separated values (csv) are read with the \texttt{read\_csv()} function.

Because the data is already loaded, simply use the below code to access the data.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ExamPAData)}
\end{Highlighting}
\end{Shaded}

\hypertarget{look-at-the-data}{%
\section{Look at the data}\label{look-at-the-data}}

The data that we are using is \texttt{health\_insurance}, which has information on patients and their health care costs.

The descriptions of the columns are below.

\begin{itemize}
\tightlist
\item
  \texttt{age}: Age of the individual
\item
  \texttt{sex}: Sex
\item
  \texttt{bmi}: Body Mass Index
\item
  \texttt{children}: Number of children
\item
  \texttt{smoker}: Is this person a smoker?
\item
  \texttt{region}: Region
\item
  \texttt{charges}: Annual health care costs.
\end{itemize}

\texttt{head()} shows the top n rows. \texttt{head(20)} shows the top 20 rows.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\KeywordTok{head}\NormalTok{(health_insurance)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 7
##     age sex      bmi children smoker region    charges
##   <dbl> <chr>  <dbl>    <dbl> <chr>  <chr>       <dbl>
## 1    19 female  27.9        0 yes    southwest  16885.
## 2    18 male    33.8        1 no     southeast   1726.
## 3    28 male    33          3 no     southeast   4449.
## 4    33 male    22.7        0 no     northwest  21984.
## 5    32 male    28.9        0 no     northwest   3867.
## 6    31 female  25.7        0 no     southeast   3757.
\end{verbatim}

Using a pipe is an alternative way of doing this.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{health_insurance }\OperatorTok{%>%}\StringTok{ }\KeywordTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{quote}
\textbf{Shortcut}: Use \texttt{CTRL} + \texttt{SHFT} + \texttt{M} to create pipes \texttt{\%\textgreater{}\%}
\end{quote}

The \texttt{glimpse} function is a transpose of the \texttt{head()} function, which can be more spatially efficient. This also gives you the dimension (1,338 rows, 7 columns).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{health_insurance }\OperatorTok{%>%}\StringTok{ }\KeywordTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Observations: 1,338
## Variables: 7
## $ age      <dbl> 19, 18, 28, 33, 32, 31, 46, 37, 37, 60, 25, 62, 23, 5...
## $ sex      <chr> "female", "male", "male", "male", "male", "female", "...
## $ bmi      <dbl> 27.900, 33.770, 33.000, 22.705, 28.880, 25.740, 33.44...
## $ children <dbl> 0, 1, 3, 0, 0, 0, 1, 3, 2, 0, 0, 0, 0, 0, 0, 1, 1, 0,...
## $ smoker   <chr> "yes", "no", "no", "no", "no", "no", "no", "no", "no"...
## $ region   <chr> "southwest", "southeast", "southeast", "northwest", "...
## $ charges  <dbl> 16884.924, 1725.552, 4449.462, 21984.471, 3866.855, 3...
\end{verbatim}

One of the most useful data science tools is counting things. The function \texttt{count()} gives the number of records by a categorical feature.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{health_insurance }\OperatorTok{%>%}\StringTok{ }\KeywordTok{count}\NormalTok{(children)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 2
##   children     n
##      <dbl> <int>
## 1        0   574
## 2        1   324
## 3        2   240
## 4        3   157
## 5        4    25
## 6        5    18
\end{verbatim}

Two categories can be counted at once. This creates a table with all combinations of \texttt{region} and \texttt{sex} and shows the number of records in each category.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{health_insurance }\OperatorTok{%>%}\StringTok{ }\KeywordTok{count}\NormalTok{(region, sex)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 8 x 3
##   region    sex        n
##   <chr>     <chr>  <int>
## 1 northeast female   161
## 2 northeast male     163
## 3 northwest female   164
## 4 northwest male     161
## 5 southeast female   175
## 6 southeast male     189
## 7 southwest female   162
## 8 southwest male     163
\end{verbatim}

The \texttt{summary()} function is shows a statistical summary. One caveat is that each column needs to be in it's appropriate type. For example, \texttt{smoker}, \texttt{region}, and \texttt{sex} are all listed as characters when if they were factors, \texttt{summary} would give you count info.

\textbf{With incorrect data types}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{health_insurance }\OperatorTok{%>%}\StringTok{ }\KeywordTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       age            sex                 bmi           children    
##  Min.   :18.00   Length:1338        Min.   :15.96   Min.   :0.000  
##  1st Qu.:27.00   Class :character   1st Qu.:26.30   1st Qu.:0.000  
##  Median :39.00   Mode  :character   Median :30.40   Median :1.000  
##  Mean   :39.21                      Mean   :30.66   Mean   :1.095  
##  3rd Qu.:51.00                      3rd Qu.:34.69   3rd Qu.:2.000  
##  Max.   :64.00                      Max.   :53.13   Max.   :5.000  
##     smoker             region             charges     
##  Length:1338        Length:1338        Min.   : 1122  
##  Class :character   Class :character   1st Qu.: 4740  
##  Mode  :character   Mode  :character   Median : 9382  
##                                        Mean   :13270  
##                                        3rd Qu.:16640  
##                                        Max.   :63770
\end{verbatim}

\textbf{With correct data types}

This tells you that there are 324 patients in the northeast, 325 in the northwest, 364 in the southeast, and so fourth.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{health_insurance <-}\StringTok{ }\NormalTok{health_insurance }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{modify_if}\NormalTok{(is.character, as.factor)}

\NormalTok{health_insurance }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       age            sex           bmi           children     smoker    
##  Min.   :18.00   female:662   Min.   :15.96   Min.   :0.000   no :1064  
##  1st Qu.:27.00   male  :676   1st Qu.:26.30   1st Qu.:0.000   yes: 274  
##  Median :39.00                Median :30.40   Median :1.000             
##  Mean   :39.21                Mean   :30.66   Mean   :1.095             
##  3rd Qu.:51.00                3rd Qu.:34.69   3rd Qu.:2.000             
##  Max.   :64.00                Max.   :53.13   Max.   :5.000             
##        region       charges     
##  northeast:324   Min.   : 1122  
##  northwest:325   1st Qu.: 4740  
##  southeast:364   Median : 9382  
##  southwest:325   Mean   :13270  
##                  3rd Qu.:16640  
##                  Max.   :63770
\end{verbatim}

\hypertarget{transform-the-data}{%
\section{Transform the data}\label{transform-the-data}}

Transforming, manipulating, querying, and wrangling are synonyms in data terminology.

R syntax is designed to be similar to SQL. They begin with a \texttt{SELECT}, use \texttt{GROUP\ BY} to aggregate, and have a \texttt{WHERE} to remove records. Unlike SQL, the ordering of these does not matter. \texttt{SELECT} can come after a \texttt{WHERE}.

\textbf{R to SQL translation}

\begin{verbatim}
select() -> SELECT
mutate() -> user-defined columns
summarize() -> aggregated columns
left_join() -> LEFT JOIN
filter() -> WHERE
group_by() -> GROUP BY
filter() -> HAVING
arrange() -> ORDER BY
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{health_insurance }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(age, region) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 2
##     age region   
##   <dbl> <fct>    
## 1    19 southwest
## 2    18 southeast
## 3    28 southeast
## 4    33 northwest
## 5    32 northwest
## 6    31 southeast
\end{verbatim}

Tip: use \texttt{CTRL\ +\ SHIFT\ +\ M} to create pipes \texttt{\%\textgreater{}\%}.

Let's look at only those in the southeast region. Instead of \texttt{WHERE}, use \texttt{filter}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{health_insurance }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(region }\OperatorTok{==}\StringTok{ "southeast"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(age, region) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 2
##     age region   
##   <dbl> <fct>    
## 1    18 southeast
## 2    28 southeast
## 3    31 southeast
## 4    46 southeast
## 5    62 southeast
## 6    56 southeast
\end{verbatim}

The SQL translation is

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT}\NormalTok{ age, region}
\KeywordTok{FROM}\NormalTok{ health_insurance}
\KeywordTok{WHERE}\NormalTok{ region }\OperatorTok{=} \StringTok{'southeast'}
\end{Highlighting}
\end{Shaded}

Instead of \texttt{ORDER\ BY}, use \texttt{arrange}. Unlike SQL, the order does not matter and \texttt{ORDER\ BY} doesn't need to be last.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{health_insurance }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{arrange}\NormalTok{(age) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(age, region) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 2
##     age region   
##   <dbl> <fct>    
## 1    18 southeast
## 2    18 southeast
## 3    18 northeast
## 4    18 northeast
## 5    18 northeast
## 6    18 southeast
\end{verbatim}

The \texttt{group\_by} comes before the aggregation, unlike in SQL where the \texttt{GROUP\ BY} comes last.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{health_insurance }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(region) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{avg_age =} \KeywordTok{mean}\NormalTok{(age))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 4 x 2
##   region    avg_age
##   <fct>       <dbl>
## 1 northeast    39.3
## 2 northwest    39.2
## 3 southeast    38.9
## 4 southwest    39.5
\end{verbatim}

In SQL, this would be

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT}\NormalTok{ region, }
       \FunctionTok{AVG}\NormalTok{(age) }\KeywordTok{as}\NormalTok{ avg_age}
\KeywordTok{FROM}\NormalTok{ health_insurance}
\KeywordTok{GROUP} \KeywordTok{BY}\NormalTok{ region}
\end{Highlighting}
\end{Shaded}

Just like in SQL, many different aggregate functions can be used such as \texttt{SUM}, \texttt{MEAN}, \texttt{MIN}, \texttt{MAX}, and so forth.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{health_insurance }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(region) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{avg_age =} \KeywordTok{mean}\NormalTok{(age),}
            \DataTypeTok{max_age =} \KeywordTok{max}\NormalTok{(age),}
            \DataTypeTok{median_charges =} \KeywordTok{median}\NormalTok{(charges),}
            \DataTypeTok{bmi_std_dev =} \KeywordTok{sd}\NormalTok{(bmi))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 4 x 5
##   region    avg_age max_age median_charges bmi_std_dev
##   <fct>       <dbl>   <dbl>          <dbl>       <dbl>
## 1 northeast    39.3      64         10058.        5.94
## 2 northwest    39.2      64          8966.        5.14
## 3 southeast    38.9      64          9294.        6.48
## 4 southwest    39.5      64          8799.        5.69
\end{verbatim}

To create new columns, the \texttt{mutate} function is used. For example, if we wanted a column of the person's annual charges divided by their age

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{health_insurance }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{charges_over_age =}\NormalTok{ charges}\OperatorTok{/}\NormalTok{age) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(age, charges, charges_over_age) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{head}\NormalTok{(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 5 x 3
##     age charges charges_over_age
##   <dbl>   <dbl>            <dbl>
## 1    19  16885.            889. 
## 2    18   1726.             95.9
## 3    28   4449.            159. 
## 4    33  21984.            666. 
## 5    32   3867.            121.
\end{verbatim}

We can create as many new columns as we want.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{health_insurance }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{age_squared  =}\NormalTok{ age}\OperatorTok{^}\DecValTok{2}\NormalTok{,}
         \DataTypeTok{age_cubed =}\NormalTok{ age}\OperatorTok{^}\DecValTok{3}\NormalTok{,}
         \DataTypeTok{age_fourth =}\NormalTok{ age}\OperatorTok{^}\DecValTok{4}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{head}\NormalTok{(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 5 x 10
##     age sex     bmi children smoker region charges age_squared age_cubed
##   <dbl> <fct> <dbl>    <dbl> <fct>  <fct>    <dbl>       <dbl>     <dbl>
## 1    19 fema~  27.9        0 yes    south~  16885.         361      6859
## 2    18 male   33.8        1 no     south~   1726.         324      5832
## 3    28 male   33          3 no     south~   4449.         784     21952
## 4    33 male   22.7        0 no     north~  21984.        1089     35937
## 5    32 male   28.9        0 no     north~   3867.        1024     32768
## # ... with 1 more variable: age_fourth <dbl>
\end{verbatim}

The \texttt{CASE\ WHEN} function is quite similar to SQL. For example, we can create a column which is \texttt{0} when \texttt{age\ \textless{}\ 50}, \texttt{1} when \texttt{50\ \textless{}=\ age\ \textless{}=\ 70}, and \texttt{2} when \texttt{age\ \textgreater{}\ 70}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{health_insurance }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{age_bucket =} \KeywordTok{case_when}\NormalTok{(age }\OperatorTok{<}\StringTok{ }\DecValTok{50} \OperatorTok{~}\StringTok{ }\DecValTok{0}\NormalTok{,}
\NormalTok{                                age }\OperatorTok{<=}\StringTok{ }\DecValTok{70} \OperatorTok{~}\StringTok{ }\DecValTok{1}\NormalTok{,}
\NormalTok{                                age }\OperatorTok{>}\StringTok{ }\DecValTok{70} \OperatorTok{~}\StringTok{ }\DecValTok{2}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(age, age_bucket)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1,338 x 2
##      age age_bucket
##    <dbl>      <dbl>
##  1    19          0
##  2    18          0
##  3    28          0
##  4    33          0
##  5    32          0
##  6    31          0
##  7    46          0
##  8    37          0
##  9    37          0
## 10    60          1
## # ... with 1,328 more rows
\end{verbatim}

SQL translation:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT} \ControlFlowTok{CASE} \ControlFlowTok{WHEN}\NormalTok{ AGE }\OperatorTok{<} \DecValTok{50} \ControlFlowTok{THEN} \DecValTok{0}
       \ControlFlowTok{ELSE} \ControlFlowTok{WHEN}\NormalTok{ AGE }\OperatorTok{<=} \DecValTok{70} \ControlFlowTok{THEN} \DecValTok{1}
       \ControlFlowTok{ELSE} \DecValTok{2}
\KeywordTok{FROM}\NormalTok{ health_insurance}
\end{Highlighting}
\end{Shaded}

\hypertarget{exercises}{%
\section{Exercises}\label{exercises}}

Run this code on your computer to answer these exercises.

The data \texttt{actuary\_salaries} contains the salaries of actuaries collected from the DW Simpson survey. Use this data to answer the exercises below.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{actuary_salaries }\OperatorTok{%>%}\StringTok{ }\KeywordTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Observations: 138
## Variables: 6
## $ industry    <chr> "Casualty", "Casualty", "Casualty", "Casualty", "C...
## $ exams       <chr> "1 Exam", "2 Exams", "3 Exams", "4 Exams", "1 Exam...
## $ experience  <dbl> 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5,...
## $ salary      <chr> "48 - 65", "50 - 71", "54 - 77", "58 - 82", "54 - ...
## $ salary_low  <dbl> 48, 50, 54, 58, 54, 57, 62, 63, 65, 70, 72, 85, 55...
## $ salary_high <chr> "65", "71", "77", "82", "72", "81", "87", "91", "9...
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  How many industries are represented?
\item
  The \texttt{salary\_high} column is a character type when it should be numeric. Change this column to numeric.
\item
  What are the highest and lowest salaries for an actuary in Health with 5 exams passed?
\item
  Create a new column called \texttt{salary\_mid} which has the middle of the \texttt{salary\_low} and \texttt{salary\_high} columns.
\item
  When grouping by industry, what is the highest \texttt{salary\_mid}? What about \texttt{salary\_high}? What is the lowest \texttt{salary\_low}?
\item
  There is a mistake when \texttt{salary\_low\ ==\ 11}. Find and fix this mistake, and then rerun the code from the previous task.
\item
  Create a new column, called \texttt{n\_exams}, which is an integer. Use 7 for ASA/ACAS and 10 for FSA/FCAS. Use the code below as a starting point and fill in the \texttt{\_} spaces
\item
  Create a column called \texttt{social\_life}, which is equal to \texttt{n\_exams}/\texttt{experience}. What is the average (mean) \texttt{social\_life} by industry? Bonus question: what is wrong with using this as a statistical measure?
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{actuary_salaries <-}\StringTok{ }\NormalTok{actuary_salaries }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{n_exams =} \KeywordTok{case_when}\NormalTok{(exams }\OperatorTok{==}\StringTok{ "FSA"} \OperatorTok{~}\StringTok{ }\NormalTok{_,}
\NormalTok{                             exams }\OperatorTok{==}\StringTok{ "ASA"} \OperatorTok{~}\StringTok{ }\NormalTok{_,}
\NormalTok{                             exams }\OperatorTok{==}\StringTok{ "FCAS"} \OperatorTok{~}\StringTok{ }\NormalTok{_,}
\NormalTok{                             exams }\OperatorTok{==}\StringTok{ "ACAS"} \OperatorTok{~}\StringTok{ }\NormalTok{_,}
                             \OtherTok{TRUE} \OperatorTok{~}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{substr}\NormalTok{(exams,_,_)))) }
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{7}
\tightlist
\item
  Create a column called \texttt{social\_life}, which is equal to \texttt{n\_exams}/\texttt{experience}. What is the average (mean) \texttt{social\_life} by industry? Bonus question: what is wrong with using this as a statistical measure?
\end{enumerate}

\hypertarget{answers-to-exercises}{%
\section{Answers to exercises}\label{answers-to-exercises}}

Answers to these exercises, along with a video tutorial, are available at \href{https://www.futuroinsight.com/pricing}{futuroinsight.com}.

\hypertarget{visualization}{%
\chapter{Visualization}\label{visualization}}

This sections shows how to create and interpret simple graphs. In past exams, the SOA has provided code for any technical visualizations which are needed.

\hypertarget{create-a-plot-object-ggplot}{%
\section{Create a plot object (ggplot)}\label{create-a-plot-object-ggplot}}

Let's create a histogram of the claims. The first step is to create a blank canvas that holds the columns that are needed. The \texttt{aesthetic} argument, \texttt{aes}, means that the variable shown will the the claims.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ExamPAData)}
\NormalTok{p <-}\StringTok{ }\NormalTok{insurance }\OperatorTok{%>%}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(claims))}
\end{Highlighting}
\end{Shaded}

If we look at \texttt{p}, we see that it is nothing but white space with axis for \texttt{count} and \texttt{income}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p}
\end{Highlighting}
\end{Shaded}

\includegraphics{04-visualization_files/figure-latex/unnamed-chunk-3-1.pdf}

\hypertarget{add-a-plot}{%
\section{Add a plot}\label{add-a-plot}}

We add a histogram

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_histogram}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{04-visualization_files/figure-latex/unnamed-chunk-4-1.pdf}

Different plots are called ``geoms'' for ``geometric objects''. Geometry = Geo (space) + metre (measure), and graphs measure data. For instance, instead of creating a histogram, we can draw a gamma distribution with \texttt{stat\_density}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p }\OperatorTok{+}\StringTok{ }\KeywordTok{stat_density}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{04-visualization_files/figure-latex/unnamed-chunk-5-1.pdf}

Create an xy plot by adding and \texttt{x} and a \texttt{y} argument to \texttt{aesthetic}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{insurance }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ holders, }\DataTypeTok{y =}\NormalTok{ claims)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{04-visualization_files/figure-latex/unnamed-chunk-6-1.pdf}

\hypertarget{data-manipulation-chaining}{%
\section{Data manipulation chaining}\label{data-manipulation-chaining}}

Pipes allow for data manipulations to be chained with visualizations.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{termlife }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(FACE }\OperatorTok{>}\StringTok{ }\DecValTok{0}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{INCOME_AGE_RATIO =}\NormalTok{ INCOME}\OperatorTok{/}\NormalTok{AGE) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(INCOME_AGE_RATIO, FACE)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{theme_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{04-visualization_files/figure-latex/unnamed-chunk-7-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\KeywordTok{library}\NormalTok{(ggplot2)}
\KeywordTok{theme_set}\NormalTok{(}\KeywordTok{theme_bw}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\hypertarget{introduction-to-modeling}{%
\chapter{Introduction to Modeling}\label{introduction-to-modeling}}

About 40-50\% of the exam grade is based on modeling.

\hypertarget{model-notation}{%
\section{Model Notation}\label{model-notation}}

The number of observations will be denoted by \(n\). When we refer to the size of a data set, we are referring to \(n\). We use \(p\) to refer the number of input variables used. The word ``variables'' is synonymous with ``features''. For example, in the \texttt{health\_insurance} data, the variables are \texttt{age}, \texttt{sex}, \texttt{bmi}, \texttt{children}, \texttt{smoker} and \texttt{region}. These 7 variables mean that \(p = 7\). The data is collected from 1,338 patients, which means that \(n = 1,338\).

Scalar numbers are denoted by ordinary variables (i.e., \(x = 2\), \(z = 4\)), and vectors are denoted by bold-faced letters

\[\mathbf{a} = \begin{pmatrix} a_1 \\ a_2 \\ a_3 \end{pmatrix}\]

We use \(\mathbf{y}\) to denote the target variable. This is the variable which we are trying to predict. This can be either a whole number, in which case we are performing \emph{regression}, or a category, in which case we are performing \emph{classification}. In the health insurance example, \texttt{y\ =\ charges}, which are the annual health care costs for a patient.

Both \(n\) and \(p\) are important because they tell us what types of models are likely to work well, and which methods are likely to fail. For the PA exam, we will be dealing with small \(n\) (\textless100,000) due to the limitations of the Prometric computers. We will use a small \(p\) (\textless{} 20) in order to make the data sets easier to interpret.

We organize these variables into matrices. Take an example with \(p\) = 2 columns and 3 observations. The matrix is said to be \(3 \times 2\) (read as ``2-by-3'') matrix.

\[
\mathbf{X} = \begin{pmatrix}x_{11} & x_{21}\\
x_{21} & x_{22}\\
x_{31} & x_{32}
\end{pmatrix}
\]

The target is

\[\mathbf{y} = \begin{pmatrix} y_1 \\ y_2 \\ y_3 \end{pmatrix}\]
This represents the \emph{unknown} quantity that we want to be able to predict. In the health care costs example, \(y_1\) would be the costs of the first patient, \(y_2\) the costs of the second patient, and so forth. The variables \(x_{11}\) and \(x_{12}\) might represent the first patient's age and sex respectively, where \(x_{i1}\) is the patient's age, and \(x_{i2} = 1\) if the ith patient is male and 0 if female.

Machine learning is about using \(X\) to predict \(Y\). We call this ``y-hat'', or simply the prediction. This is based on a function of the data \(X\).

\[\hat{Y} = f(X)\]

This is almost never going to happen perfectly, and so there is always an error term, \(\epsilon\). This can be made smaller, but is never exactly zero.

\[
\hat{Y} + \epsilon = f(X) + \epsilon
\]

In other words, \(\epsilon = y - \hat{y}\). We call this the \emph{residual}. When we predict a person's health care costs, this is the difference between the predicted costs (which we had created the year before) and the actual costs that the patient experienced (of that current year).

\hypertarget{ordinary-least-squares-ols}{%
\section{Ordinary least squares (OLS)}\label{ordinary-least-squares-ols}}

The type of model used refers to the class of function of \(f\). If \(f\) is linear, then we are using a linear model. Linear models are linear in the parameters, \(\beta\).

We observe the data \(X\) and the want to predict the target \(Y\).

We find a \(\mathbf{\beta}\) so that

\[
\hat{Y} = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_p X_p
\]

Which means that each \(y_i\) is a linear combination of the variables \(x_1, ..., x_p\), plus a constant \(\beta_0\) which is called the \emph{intercept} term.

In the one-dimensional case, this creates a line connecting the points. In higher dimensions, this creates a hyperplane.

\includegraphics{05-linear-models_files/figure-latex/unnamed-chunk-2-1.pdf}

The question then is \textbf{how can we choose the best values of} \(\beta?\) First of all, we need to define what we mean by ``best''. Ideally, we will choose these values which will create close predictions of \(\mathbf{y}\) on new, unseen data.

To solve for \(\mathbf{\beta}\), we first need to define a \emph{loss function}. This allows us to compare how well a model is fitting the data. The most commonly used loss function is the residual sum of squares (RSS), also called the \emph{squared error loss} or the L2 norm. When RSS is small, then the predictions are close to the actual values and the model is a good fit. When RSS is large, the model is a poor fit.

\[
\text{RSS} = \sum_i(y_i - \hat{y})^2
\]

When you replace \(\hat{y_i}\) in the above equation with \(\beta_0 + \beta_1 x_1 + ... + \beta_p x_p\), take the derivative with respect to \(\beta\), set equal to zero, and solve, we can find the optimal values. This turns the problem of statistics into a problem of numeric optimization, which computers can do quickly.

You might be asking: why does this need to be the squared error? Why not the absolute error, or the cubed error? Technically, these could be used as well. In fact, the absolute error (L1 norm) is useful in other models. Taking the square has a number of advantages.

\begin{itemize}
\tightlist
\item
  It provides the same solution if we assume that the distribution of \(Y|X\) is guassian and maximize the likelihood function. This method is used for GLMs, in the next chapter.
\item
  Empirically it has been shown to be less likely to overfit as compared to other loss functions
\end{itemize}

\hypertarget{example}{%
\section{Example}\label{example}}

In our health, we can create a linear model using \texttt{bmi}, \texttt{age}, and \texttt{sex} as an inputs.

The \texttt{formula} controls which variables are included. There are a few shortcuts for using R formulas.

\begin{longtable}[]{@{}ll@{}}
\toprule
\begin{minipage}[b]{0.41\columnwidth}\raggedright
Formula\strut
\end{minipage} & \begin{minipage}[b]{0.53\columnwidth}\raggedright
Meaning\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.41\columnwidth}\raggedright
\texttt{charges} \textasciitilde{} \texttt{bmi} + \texttt{age}\strut
\end{minipage} & \begin{minipage}[t]{0.53\columnwidth}\raggedright
Use \texttt{age} and \texttt{bmi} to predict \texttt{charges}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.41\columnwidth}\raggedright
\texttt{charges} \textasciitilde{} \texttt{bmi} + \texttt{age} + \texttt{bmi}*\texttt{age}\strut
\end{minipage} & \begin{minipage}[t]{0.53\columnwidth}\raggedright
Use \texttt{age},\texttt{bmi} as well as an interaction to predict \texttt{charges}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.41\columnwidth}\raggedright
\texttt{charges} \textasciitilde{} (\texttt{bmi\ \textgreater{}\ 20}) + \texttt{age}\strut
\end{minipage} & \begin{minipage}[t]{0.53\columnwidth}\raggedright
Use an indicator variable for \texttt{bmi\ \textgreater{}\ 20} \texttt{age} to predict \texttt{charges}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.41\columnwidth}\raggedright
log(\texttt{charges}) \textasciitilde{} log(\texttt{bmi}) + log(\texttt{age})\strut
\end{minipage} & \begin{minipage}[t]{0.53\columnwidth}\raggedright
Use the logs of \texttt{age} and \texttt{bmi} to predict log(\texttt{charges})\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.41\columnwidth}\raggedright
\texttt{charges} \textasciitilde{} .\strut
\end{minipage} & \begin{minipage}[t]{0.53\columnwidth}\raggedright
Use all variables to predict \texttt{charges}\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

You can use formulas to create new variables (aka feature engineering). This can save you from needing to re-run code to create data.

Below we fit a simple linear model to predict charges.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ExamPAData)}
\KeywordTok{library}\NormalTok{(tidyverse)}

\NormalTok{model <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ health_insurance, }\DataTypeTok{formula =}\NormalTok{ charges }\OperatorTok{~}\StringTok{ }\NormalTok{bmi }\OperatorTok{+}\StringTok{ }\NormalTok{age)}
\end{Highlighting}
\end{Shaded}

The \texttt{summary} function gives details about the model. First, the \texttt{Estimate}, gives you the coefficients. The \texttt{Std.\ Error} is the error of the estimate for the coefficient. Higher standard error means greater uncertainty. This is relative to the average value of that variable. The \texttt{t\ value} tells you how ``big'' this error really is based on standard deviations. A larger \texttt{t\ value} implies a low probability of the null hypothesis being accepted saying that the coefficient is zero. This is the same as having a p-value (\texttt{Pr\ (\textgreater{}\textbar{}t\textbar{}))}) being close to zero.

The little \texttt{*}, \texttt{**}, \texttt{***} indicate that the variable is either somewhat significant, significant, or highly significant. ``significance'' here means that there is a low probability of the coefficient being that size (or larger) if there were \emph{no actual casual relationship}, or if the data was random noise.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = charges ~ bmi + age, data = health_insurance)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -14457  -7045  -5136   7211  48022 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept) -6424.80    1744.09  -3.684 0.000239 ***
## bmi           332.97      51.37   6.481 1.28e-10 ***
## age           241.93      22.30  10.850  < 2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 11390 on 1335 degrees of freedom
## Multiple R-squared:  0.1172, Adjusted R-squared:  0.1159 
## F-statistic:  88.6 on 2 and 1335 DF,  p-value: < 2.2e-16
\end{verbatim}

When evaluating model performance, you should not rely on the \texttt{summary} alone as this is based on the training data. To look at performance, test the model on validation data. This can be done by either using a hold out set, or using cross-validation, which is even better.

Let's create an 80\% training set and 20\% testing set. You don't need to worry about understanding this code as the exam will always give this to you.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\KeywordTok{library}\NormalTok{(caret)}
\CommentTok{#create a train/test split}
\NormalTok{index <-}\StringTok{ }\KeywordTok{createDataPartition}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ health_insurance}\OperatorTok{$}\NormalTok{charges, }
                             \DataTypeTok{p =} \FloatTok{0.8}\NormalTok{, }\DataTypeTok{list =}\NormalTok{ F) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{()}
\NormalTok{train <-}\StringTok{  }\NormalTok{health_insurance }\OperatorTok{%>%}\StringTok{ }\KeywordTok{slice}\NormalTok{(index)}
\NormalTok{test <-}\StringTok{ }\NormalTok{health_insurance }\OperatorTok{%>%}\StringTok{ }\KeywordTok{slice}\NormalTok{(}\OperatorTok{-}\NormalTok{index)}
\end{Highlighting}
\end{Shaded}

Train the model on the \texttt{train} and test on \texttt{test}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ train, }\DataTypeTok{formula =}\NormalTok{ charges }\OperatorTok{~}\StringTok{ }\NormalTok{bmi }\OperatorTok{+}\StringTok{ }\NormalTok{age)}
\NormalTok{pred =}\StringTok{ }\KeywordTok{predict}\NormalTok{(model, test)}
\end{Highlighting}
\end{Shaded}

Let's look at the Root Mean Squared Error (RMSE).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{get_rmse <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(y, y_hat)\{}
  \KeywordTok{sqrt}\NormalTok{(}\KeywordTok{mean}\NormalTok{((y }\OperatorTok{-}\StringTok{ }\NormalTok{y_hat)}\OperatorTok{^}\DecValTok{2}\NormalTok{))}
\NormalTok{\}}

\KeywordTok{get_rmse}\NormalTok{(pred, test}\OperatorTok{$}\NormalTok{charges)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 11421.96
\end{verbatim}

The above number does not tell us if this is a good model or not by itself. We need a comparison. The fastest check is to compare against a prediction of the mean. In other words, all values of the \texttt{y\_hat} are the average of \texttt{charges}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{get_rmse}\NormalTok{(}\KeywordTok{mean}\NormalTok{(test}\OperatorTok{$}\NormalTok{charges), test}\OperatorTok{$}\NormalTok{charges)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 12574.97
\end{verbatim}

The RMSE is \textbf{higher} (worse) when using just the mean, which is what we expect. \textbf{If you ever fit a model and get an error which is worse than the average prediction, something must be wrong.}

The next test is to see if any assumptions have been violated.

First, is there a pattern in the residuals? If there is, this means that the model is missing key information. For the model below, this is a \textbf{yes}, which means that this is a bad model. Because this is just for illustration, I'm going to continue using it, however.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(model, }\DataTypeTok{which =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{05-linear-models_files/figure-latex/unnamed-chunk-9-1.pdf}
\caption{\label{fig:unnamed-chunk-9}Residuals vs.~Fitted}
\end{figure}

The normal QQ shows how well the quantiles of the predictions fit to a theoretical normal distribution. If this is true, then the graph is a straight 45-degree line. In this model, you can definitely see that this is not the case. If this were a good model, this distribution would be closer to normal.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(model, }\DataTypeTok{which =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{05-linear-models_files/figure-latex/unnamed-chunk-10-1.pdf}
\caption{\label{fig:unnamed-chunk-10}Normal Q-Q}
\end{figure}

Once you have chosen your model, you should re-train over the entire data set. This is to make the coefficients more stable because \texttt{n} is larger. Below you can see that the standard error is lower after training over the entire data set.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{all_data <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ health_insurance, }
               \DataTypeTok{formula =}\NormalTok{ charges }\OperatorTok{~}\StringTok{ }\NormalTok{bmi }\OperatorTok{+}\StringTok{ }\NormalTok{age)}
\NormalTok{testing <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ test, }
              \DataTypeTok{formula =}\NormalTok{ charges }\OperatorTok{~}\StringTok{ }\NormalTok{bmi }\OperatorTok{+}\StringTok{ }\NormalTok{age)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lrr@{}}
\toprule
term & full\_data\_std\_error & test\_data\_std\_error\tabularnewline
\midrule
\endhead
(Intercept) & 1744.1 & 3824.2\tabularnewline
bmi & 51.4 & 111.1\tabularnewline
age & 22.3 & 47.8\tabularnewline
\bottomrule
\end{longtable}

All interpretations should be based on the model which was trained on the entire data set. Obviously, this only makes a difference if you are interpreting the precise values of the coefficients. If you are just looking at which variables are included, or at the size and sign of the coefficients, then this would not change.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{coefficients}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## (Intercept)         bmi         age 
##  -4526.5284    286.8283    228.4372
\end{verbatim}

Translating the above into an equation we have

\[\hat{y_i} = -4,526 + 287 \space\text{bmi} + 228\space \text{age}\]

For example, if a patient has \texttt{bmi\ =\ 27.9} and \texttt{age\ =\ 19} then predicted value is

\[\hat{y_1} = 4,526 + (287)(27.9) + (228)(19) = 16,865\]

This model structure implies that each of the variables \(x_1, ..., x_p\) each change the predicted \(\hat{y}\). If \(x_{ij}\) increases by one unit, then \(y_i\) increases by \(\beta_j\) units, regardless of what happens to all of the other variables. This is one of the main assumptions of linear models: \emph{variable indepdendence}. If the variables are correlated, say, then this assumption will be violated.

\begin{longtable}[]{@{}ll@{}}
\toprule
Readings &\tabularnewline
\midrule
\endhead
ISLR 2.1 What is statistical learning? &\tabularnewline
ISLR 2.2 Assessing model accuracy &\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{generalized-linear-models-glms}{%
\chapter{Generalized linear models (GLMs)}\label{generalized-linear-models-glms}}

The linear model that we have considered up to this point, what we called ``OLS'', assumes that the response is a linear combination of the predictor variables. For an error term \(\epsilon_i \sim N(0,\sigma^2)\), this is assumes that

\[
Y = \beta_0 + \beta_1 X_1 + ... + \beta_p X_p + \epsilon
\]

In matrix notation, if \(X\) is the matrix made up of columns \(X_1, ..., X_p\), then

\[
\mathbf{Y} = \mathbf{X} \mathbf{\beta} + \mathbf{\epsilon}
\]

Another way of saying this is that ``after we adjust for the data, the error is normally distributed and the variance is constant.'' If \(I\) is an n-by-in identity matrix, and \(\sigma^2 I\) is the covariance matrix, then

\[
\mathbf{Y|X} \sim N( \mathbf{X \beta}, \mathbf{\sigma^2} I)
\]

Because this notation is getting too cumbersome, we're going to stop using bold letters to denote matrices and just use non-bold characters. From now on, \(\mathbf{X}\) is the same as \(X\).

These assumptions can be expressed in two parts:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  A \emph{random component}: The response variable \(Y|X\) is normally distributed with mean \(\mu = \mu(X) = E(Y|X)\)
\item
  A link between the response and the covariates (also known as the systemic component) \(\mu(X) = X\beta\)
\end{enumerate}

In words, this is saying that each observation follows a normal distribution which has a mean that is equal to the linear predictor.

\hypertarget{the-generalized-linear-model}{%
\section{The generalized linear model}\label{the-generalized-linear-model}}

Just as the name implies, GLMs are more \emph{general} in that they are more flexible. We relax these two assumptions by saying that the model is defined by

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  A random component: \(Y|X \sim \text{some exponential family distribution}\)
\item
  A link: between the random component and covariates:
\end{enumerate}

\[g(\mu(X)) = X\beta\]
where \(g\) is called the \emph{link function} and \(\mu = E[Y|X]\).

In words, this is saying that each observation follows \emph{some type of exonential distrubution} (Gamma, Inverse Gaussian, Poisson, etc.) and that distribution has a mean which is related to the linear predictor through the link function. Additionally, there is a \emph{dispersion} parameter, but that is more more info that is needed here. For an explanation, see \href{https://contentpreview.s3.us-east-2.amazonaws.com/CAS+Monograph+5+-+Generalized+Linear+Models+for+Insurance+Ratemaking.pdf}{Ch. 2.2 of CAS Monograph 5}.

The possible combinations of link functions and distribution families are summarized nicely on \href{https://en.wikipedia.org/wiki/Generalized_linear_model\#Link_function}{Wikipedia}.

\begin{figure}
\includegraphics[width=22.32in]{images/glm_links} \caption{Distribution-Link Function Combinations}\label{fig:unnamed-chunk-14}
\end{figure}

For this exam, a common question is to ask candiates to choose the best distribution and link function. There is no all-encompasing answer, but a few suggestions are

\begin{itemize}
\tightlist
\item
  If \(Y\) is counting something, such as the number of claims, number of accidents, or some other discrete and positive counting sequence, use the Poisson;
\item
  If \(Y\) contains negative values, then do not use the Exponential, Gamma, or Inverse Gaussian as these are strictly positive. Conversely, if \(Y\) is only positive, such as the price of a policy (price is always \textgreater{} 0), or the claim costs, then these are good choices;
\item
  If \(Y\) is binary, the the binomial response with either a Probit or Logit link. The Logit is more common.
\item
  If \(Y\) has more than two categories, the multinomial distribution with either the Probit or Logic link (See Logistic Regression)
\end{itemize}

\hypertarget{interpretation}{%
\section{Interpretation}\label{interpretation}}

The exam will always ask you to interpret the GLM. These questions can usually be answered by inverting the link function and interpreting the coefficients. In the case of the log link, simply take the exponent of the coefficients and each of these represents a ``relativity'' factor.

\[
log(\mathbf{\hat{y}}) = \mathbf{X} \mathbf{\beta} \Rightarrow \mathbf{\hat{y}} = e^{\mathbf{X} \mathbf{\beta}}
\]

For a single observation \(y_i\), this is

\[
\text{exp}(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + ... + \beta_p x_{ip}) = \\
e^{\beta_0} e^{\beta_1 x_{i1}}e^{\beta_2 x_{i2}} ...  e^{\beta_p x_{ip}} = 
R_0 R_2 R_3 ... R_{p}
\]

Where \(R_k\) is the \emph{relativity} of the kth variable. This terminology is from insurance ratemaking, where actuaries need to be able to explain the impact of each variable in pricing insurance. The data science community does not use this language.

For binary outcomes with logit or probit link, there is no easy interpretation. This has come up in at least one past sample exam, and the solution was to create ``psuedo'' observations and observe how changing each \(x_k\) would change the predicted value. Due to the time requirements, this is unlikely to come up on an exam. So if you are asked to use a logit or probit link, saying that the result is not easy to interpret should suffice.

\hypertarget{residuals}{%
\section{Residuals}\label{residuals}}

The word ``residual'' by itself actually means the ``raw residual'' in GLM language. This is the difference in actual vs.~predicted values.

\[\text{Raw Residual} = y_i - \hat{y_i}\]

This are not meaningful for GLMs with non-Gaussian response families because the distribution changes depending on the response family chosen. To adjust for this, we need the concept of \emph{deviance residual}.

To paraphrase from this paper from the University of Oxford:

www.stats.ox.ac.uk/pub/bdr/IAUL/ModellingLecture5.pdf

Deviance is a way of assessing the adequacy of a model by comparing it with a more general
model with the maximum number of parameters that can be estimated. It is referred to
as the saturated model. In the saturated model there is basically one parameter per
observation. The deviance assesses the goodness of fit for the model by looking at the
difference between the log-likelihood functions of the saturated model and the model
under investigation, i.e.~\(l(b_{sat},y) - l(b,y)\). Here sat \(b_{sat}\) denotes the maximum likelihood
estimator of the parameter vector of the saturated model, \(\beta_{sat}\) , and \(b\) is the maximum
likelihood estimator of the parameters of the model under investigation, \(\beta\). The maximum likelihood estimator is the estimator that maximises the likelihood function. \textbf{The deviance is defined as}

\[D = 2[l(b_{sat},y) - l(b,y)]\]
The deviance residual uses the deviance of the ith observation \(d_i\) and then takes the square root and applies the same sign (aka, the + or - part) of the raw residual.

\[\text{Deviance Residual} = \text{sign}(y_i - \hat{y_i})\sqrt{d_i}\]

\hypertarget{example-1}{%
\section{Example}\label{example-1}}

Just as with OLS, there is a \texttt{formula} and \texttt{data\ argument}. In addition, we need to specify the response distribution and link function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model =}\StringTok{ }\KeywordTok{glm}\NormalTok{(}\DataTypeTok{formula =}\NormalTok{ charges }\OperatorTok{~}\StringTok{ }\NormalTok{age }\OperatorTok{+}\StringTok{ }\NormalTok{sex }\OperatorTok{+}\StringTok{ }\NormalTok{smoker, }
            \DataTypeTok{family =} \KeywordTok{Gamma}\NormalTok{(}\DataTypeTok{link =} \StringTok{"log"}\NormalTok{),}
            \DataTypeTok{data =}\NormalTok{ health_insurance)}
\end{Highlighting}
\end{Shaded}

We see that \texttt{age}, \texttt{sex}, and \texttt{smoker} are all significant (p \textless0.01). Reading off the coefficient signs, we see that claims

\begin{itemize}
\tightlist
\item
  Increase as age increases
\item
  Are higher for women
\item
  Are higher for smokers
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OperatorTok{%>%}\StringTok{ }\KeywordTok{tidy}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 4 x 5
##   term        estimate std.error statistic   p.value
##   <chr>          <dbl>     <dbl>     <dbl>     <dbl>
## 1 (Intercept)   7.82     0.0600     130.   0.       
## 2 age           0.0290   0.00134     21.6  3.40e- 89
## 3 sexmale      -0.0468   0.0377      -1.24 2.15e-  1
## 4 smokeryes     1.50     0.0467      32.1  3.25e-168
\end{verbatim}

Below you can see graph of deviance residuals vs.~the predicted values.

\textbf{If this were a perfect model, all of these below assumptions would be met:}

\begin{itemize}
\tightlist
\item
  Scattered around zero?
\item
  Constant variance?
\item
  No obvious pattern?
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(model, }\DataTypeTok{which =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{05-linear-models_files/figure-latex/unnamed-chunk-17-1.pdf}

The quantile-quantile (QQ) plot shows the quantiles of the deviance residuals (i.e., after adjusting for the Gamma distribution) against theoretical Gaussian quantiles.

\textbf{In a perfect model, all of these assumptions would be met:}

\begin{itemize}
\tightlist
\item
  Points lie on a straight line?\\
\item
  Tails are not significantly above or below line? Some tail deviation is ok.
\item
  No sudden ``jumps''? This indicates many \(Y\)'s which have the same value, such as insurance claims which all have the exact value of \$100.00 or \$0.00.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(model, }\DataTypeTok{which =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{05-linear-models_files/figure-latex/unnamed-chunk-18-1.pdf}

\hypertarget{combinations-of-link-and-response-family-examples}{%
\section{Combinations of Link and Response Family Examples}\label{combinations-of-link-and-response-family-examples}}

What is an example of when to use a log link with a guassian response? What about a Gamma family with an inverse link? What about an inverse Gaussian response and an inverse square link? As these questions illustrate, there are many combinations of link and response family. In the real world, a model rarely fits perfectly, and so often these choices come down to the judgement of the modeler - which model is the best fit and meets the business objectives?

However, there is one way that we can know for certain which link and response family is the best, and that is if we generate the data ourselves.

Recall that a GLM has two parts:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  A \textbf{random component}: \(Y|X \sim \text{some exponential family distribution}\)
\item
  A \textbf{link function}: between the random component and the covariates: \(g(\mu(X)) = X\beta\) where \(\mu = E[Y|X]\)
\end{enumerate}

\textbf{Following this recipe, we can simulate data from any combination of link function and response family. This helps us to understand the GLM framework very clearly.}

\hypertarget{gaussian-response-with-log-link}{%
\subsection{Gaussian Response with Log Link}\label{gaussian-response-with-log-link}}

We create a function that takes in data \(x\) and returns a guassian random variable that has mean equal to the inverse link, which in the case of a log link is the exponent. We add 10 to \(x\) so that the values will always be positive, as will be described later on.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim_norm <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) \{}
  \KeywordTok{rnorm}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DataTypeTok{mean =} \KeywordTok{exp}\NormalTok{(}\DecValTok{10} \OperatorTok{+}\StringTok{ }\NormalTok{x), }\DataTypeTok{sd =} \DecValTok{1}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

The values of \(X\) do not need to be normal. The above assumption is merely that the mean of the response \(Y\) is related to \(X\) through the link function, \texttt{mean\ =\ exp(10\ +\ x)}, and that the distribution is normal. This has been accomplished with \texttt{rnorm} already. For illustration, here we use \(X\)'s from a uniform distribution.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{runif}\NormalTok{(}\DecValTok{1000}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ x }\OperatorTok{%>%}\StringTok{ }\KeywordTok{map_dbl}\NormalTok{(sim_norm))}
\end{Highlighting}
\end{Shaded}

We already know what the answer is: a gaussian response with a log link. We fit a GLM and see a perfect fit.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{glm <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\NormalTok{x, }\DataTypeTok{family =} \KeywordTok{gaussian}\NormalTok{(}\DataTypeTok{link =} \StringTok{"log"}\NormalTok{), }\DataTypeTok{data =}\NormalTok{ data)}

\KeywordTok{summary}\NormalTok{(glm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## glm(formula = y ~ x, family = gaussian(link = "log"), data = data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -3.0004  -0.6964   0.0005   0.7266   3.0718  
## 
## Coefficients:
##              Estimate Std. Error t value Pr(>|t|)    
## (Intercept) 1.000e+01  2.195e-06 4554982   <2e-16 ***
## x           1.000e+00  3.085e-06  324117   <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for gaussian family taken to be 1.067056)
## 
##     Null deviance: 1.2235e+11  on 999  degrees of freedom
## Residual deviance: 1.0649e+03  on 998  degrees of freedom
## AIC: 2906.8
## 
## Number of Fisher Scoring iterations: 2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(glm, }\DataTypeTok{cex =} \FloatTok{0.4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{05-linear-models_files/figure-latex/unnamed-chunk-21-1.pdf}

\hypertarget{gaussian-response-with-inverse-link}{%
\subsection{Gaussian Response with Inverse Link}\label{gaussian-response-with-inverse-link}}

The same steps are repeated except the link function is now the inverse, \texttt{mean\ =\ 1/x}. We see that some values of \(Y\) are negative, which is ok.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim_norm <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) \{}
  \KeywordTok{rnorm}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DataTypeTok{mean =} \DecValTok{1}\OperatorTok{/}\NormalTok{x, }\DecValTok{1}\NormalTok{)}
\NormalTok{\}}

\NormalTok{data <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{runif}\NormalTok{(}\DecValTok{10000}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ x }\OperatorTok{%>%}\StringTok{ }\KeywordTok{map_dbl}\NormalTok{(sim_norm))}
\KeywordTok{summary}\NormalTok{(data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        x                   y           
##  Min.   :0.0001064   Min.   :  -1.957  
##  1st Qu.:0.2532864   1st Qu.:   1.259  
##  Median :0.5028875   Median :   2.334  
##  Mean   :0.5018599   Mean   :  10.214  
##  3rd Qu.:0.7507694   3rd Qu.:   4.232  
##  Max.   :0.9998552   Max.   :9394.790
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{glm <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\NormalTok{x, }\DataTypeTok{family =} \KeywordTok{gaussian}\NormalTok{(}\DataTypeTok{link =} \StringTok{"inverse"}\NormalTok{), }\DataTypeTok{data =}\NormalTok{ data)}

\KeywordTok{summary}\NormalTok{(glm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## glm(formula = y ~ x, family = gaussian(link = "inverse"), data = data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -3.5186  -0.6747   0.0105   0.6883   3.3764  
## 
## Coefficients:
##              Estimate Std. Error t value Pr(>|t|)    
## (Intercept) 3.596e-08  2.587e-08    1.39    0.165    
## x           9.998e-01  2.072e-04 4824.13   <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for gaussian family taken to be 1.007483)
## 
##     Null deviance: 203905704  on 9999  degrees of freedom
## Residual deviance:     10073  on 9998  degrees of freedom
## AIC: 28457
## 
## Number of Fisher Scoring iterations: 4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(glm, }\DataTypeTok{cex =} \FloatTok{0.4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{05-linear-models_files/figure-latex/unnamed-chunk-23-1.pdf}

\hypertarget{gaussian-response-with-identity-link}{%
\subsection{Gaussian Response with Identity Link}\label{gaussian-response-with-identity-link}}

And now the link is the identity, \texttt{mean\ =\ x}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim_norm <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) \{}
  \KeywordTok{rnorm}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DataTypeTok{mean =}\NormalTok{ x, }\DecValTok{1}\NormalTok{)}
\NormalTok{\}}

\NormalTok{data <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{rnorm}\NormalTok{(}\DecValTok{10000}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ x }\OperatorTok{%>%}\StringTok{ }\KeywordTok{map_dbl}\NormalTok{(sim_norm))}

\NormalTok{glm <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\NormalTok{x, }\DataTypeTok{family =} \KeywordTok{gaussian}\NormalTok{(}\DataTypeTok{link =} \StringTok{"identity"}\NormalTok{), }\DataTypeTok{data =}\NormalTok{ data)}

\KeywordTok{summary}\NormalTok{(glm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## glm(formula = y ~ x, family = gaussian(link = "identity"), data = data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -4.4461  -0.6853   0.0129   0.6794   3.6661  
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)  0.01393    0.01010   1.379    0.168    
## x            0.98236    0.01005  97.727   <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for gaussian family taken to be 1.020901)
## 
##     Null deviance: 19957  on 9999  degrees of freedom
## Residual deviance: 10207  on 9998  degrees of freedom
## AIC: 28590
## 
## Number of Fisher Scoring iterations: 2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(glm, }\DataTypeTok{cex =} \FloatTok{0.4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{05-linear-models_files/figure-latex/unnamed-chunk-24-1.pdf}

\hypertarget{gaussian-response-with-log-link-and-negative-values}{%
\subsection{Gaussian Response with Log Link and Negative Values}\label{gaussian-response-with-log-link-and-negative-values}}

By Gaussian response we say that the \emph{mean} of the response is Gaussian. The range of a normal random variable is \((-\infty, +\infty)\), which means that negative values are always possible. Now, if the mean is a large positive number, than negative values are much less likely but still possible: about 95\% of the observations will be within 2 standard deviations of the mean.

We see below that there are some \(Y\) values which are negative.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim_norm <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) \{}
  \KeywordTok{rnorm}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DataTypeTok{mean =} \KeywordTok{exp}\NormalTok{(x), }\DataTypeTok{sd =} \DecValTok{1}\NormalTok{)}
\NormalTok{\}}

\NormalTok{data <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{runif}\NormalTok{(}\DecValTok{1000}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ x }\OperatorTok{%>%}\StringTok{ }\KeywordTok{map_dbl}\NormalTok{(sim_norm))}
\KeywordTok{summary}\NormalTok{(data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        x                   y          
##  Min.   :0.0000122   Min.   :-1.8709  
##  1st Qu.:0.2354295   1st Qu.: 0.9815  
##  Median :0.5055838   Median : 1.6969  
##  Mean   :0.4968540   Mean   : 1.7275  
##  3rd Qu.:0.7611768   3rd Qu.: 2.4854  
##  Max.   :0.9993455   Max.   : 5.0233
\end{verbatim}

We can also see this from the histogram.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OperatorTok{%>%}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(y)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_density}\NormalTok{( }\DataTypeTok{fill =} \DecValTok{1}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{05-linear-models_files/figure-latex/unnamed-chunk-26-1} \end{center}

If we try to fit a GLM with a log link, there is an error.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{glm <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\NormalTok{x, }\DataTypeTok{family =} \KeywordTok{gaussian}\NormalTok{(}\DataTypeTok{link =} \StringTok{"log"}\NormalTok{), }\DataTypeTok{data =}\NormalTok{ data)}
\end{Highlighting}
\end{Shaded}

\texttt{Error\ in\ eval(family\$initialize)\ :\ cannot\ find\ valid\ starting\ values:\ please\ specify\ some}

This is because the domain of the natural logarithm only includes positive numbers, and we just tried to take the log of negative numbers.

Our initial reaction might be to add some constant to each \(Y\), say 10 for instance, so that they are all positive. This does produce a model which is a good fit.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{glm <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(y }\OperatorTok{+}\StringTok{ }\DecValTok{10} \OperatorTok{~}\StringTok{ }\NormalTok{x, }\DataTypeTok{family =} \KeywordTok{gaussian}\NormalTok{(}\DataTypeTok{link =} \StringTok{"log"}\NormalTok{), }\DataTypeTok{data =}\NormalTok{ data)}
\KeywordTok{summary}\NormalTok{(glm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## glm(formula = y + 10 ~ x, family = gaussian(link = "log"), data = data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -3.1527  -0.6538  -0.0336   0.6753   3.3219  
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept) 2.394232   0.005463  438.25   <2e-16 ***
## x           0.134685   0.009158   14.71   <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for gaussian family taken to be 0.987688)
## 
##     Null deviance: 1198.70  on 999  degrees of freedom
## Residual deviance:  985.71  on 998  degrees of freedom
## AIC: 2829.5
## 
## Number of Fisher Scoring iterations: 4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(glm, }\DataTypeTok{cex =} \FloatTok{0.4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{05-linear-models_files/figure-latex/unnamed-chunk-28-1.pdf}

We see that on average, the predictions are 10 higher than the target. This is no surprise since \(E[Y + 10] = E[Y] + 10\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y <-}\StringTok{ }\NormalTok{data}\OperatorTok{$}\NormalTok{y }
\NormalTok{y_hat <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(glm, }\DataTypeTok{type =} \StringTok{"response"}\NormalTok{)}
\KeywordTok{mean}\NormalTok{(y_hat) }\OperatorTok{-}\StringTok{ }\KeywordTok{mean}\NormalTok{(y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 9.99995
\end{verbatim}

But we see that the actual predictions are bad. If we were to loot at the R-squared, MAE, RMSE, or any other metric it would tell us the same story. This is because our GLM assumption is \textbf{not} that \(Y\) is related to the link function of \(X\), but that the \textbf{mean} of \(Y\) is.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ y, }\DataTypeTok{y_hat =}\NormalTok{ y_hat }\OperatorTok{-}\StringTok{ }\DecValTok{10}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(y, y_hat)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{05-linear-models_files/figure-latex/unnamed-chunk-30-1.pdf}

One solution is to adjust the \(X\) which the model is based on. Add a constant term to \(X\) so that the mean of \(Y\) is larger, and hence \(Y\) is non zero. While is a viable approach in the case of only one predictor variable, with more predictors this would not be easy to do.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{runif}\NormalTok{(}\DecValTok{1000}\NormalTok{) }\OperatorTok{+}\StringTok{ }\DecValTok{10}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ x }\OperatorTok{%>%}\StringTok{ }\KeywordTok{map_dbl}\NormalTok{(sim_norm))}
\KeywordTok{summary}\NormalTok{(data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        x               y        
##  Min.   :10.00   Min.   :22028  
##  1st Qu.:10.25   1st Qu.:28291  
##  Median :10.52   Median :36893  
##  Mean   :10.51   Mean   :38160  
##  3rd Qu.:10.77   3rd Qu.:47441  
##  Max.   :11.00   Max.   :59842
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{glm <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\NormalTok{x, }\DataTypeTok{family =} \KeywordTok{gaussian}\NormalTok{(}\DataTypeTok{link =} \StringTok{"log"}\NormalTok{), }\DataTypeTok{data =}\NormalTok{ data)}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(glm, }\DataTypeTok{cex =} \FloatTok{0.4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{05-linear-models_files/figure-latex/unnamed-chunk-31-1.pdf}

A better approach may be to use an inverse link even though the data was generated from a log link. This is a good illustration of the saying ``all models are wrong, but some are useful'' in that the statistical assumption of the model is not correct but the model still works.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{runif}\NormalTok{(}\DecValTok{1000}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ x }\OperatorTok{%>%}\StringTok{ }\KeywordTok{map_dbl}\NormalTok{(sim_norm))}
\NormalTok{glm <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\NormalTok{x, }\DataTypeTok{family =} \KeywordTok{gaussian}\NormalTok{(}\DataTypeTok{link =} \StringTok{"inverse"}\NormalTok{), }\DataTypeTok{data =}\NormalTok{ data)}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(glm, }\DataTypeTok{cex =} \FloatTok{0.4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{05-linear-models_files/figure-latex/unnamed-chunk-32-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(glm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## glm(formula = y ~ x, family = gaussian(link = "inverse"), data = data)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -3.10622  -0.63739  -0.00542   0.63167   3.06741  
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)  0.94957    0.03515   27.02   <2e-16 ***
## x           -0.58667    0.04360  -13.46   <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for gaussian family taken to be 0.9967997)
## 
##     Null deviance: 1218.78  on 999  degrees of freedom
## Residual deviance:  994.82  on 998  degrees of freedom
## AIC: 2838.7
## 
## Number of Fisher Scoring iterations: 6
\end{verbatim}

\hypertarget{gamma-response-with-log-link}{%
\subsection{Gamma Response with Log Link}\label{gamma-response-with-log-link}}

The gamma distribution with rate parameter \(\alpha\) and scale parameter \(\theta\) is density.

\[f(y) = \frac{(y/\theta)^\alpha}{x \Gamma(\alpha)}e^{-x/\theta}\]

The mean is \(\alpha\theta\).

Let's use a gamma with shape 2 and scale 0.5, which has mean 1.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gammas <-}\StringTok{ }\KeywordTok{rgamma}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\DataTypeTok{shape=}\DecValTok{2}\NormalTok{, }\DataTypeTok{scale =} \FloatTok{0.5}\NormalTok{)}
\KeywordTok{mean}\NormalTok{(gammas)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.9873887
\end{verbatim}

We then generate random gamma values. Because the mean now depends on two paramters instead of one, which was just \(\mu\) in the Guassian case, we need to use a slightly different approach to simulate the random values. The link function here is seen in \texttt{exp(x)}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#random component}
\NormalTok{x <-}\StringTok{ }\KeywordTok{runif}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\DataTypeTok{min=}\DecValTok{0}\NormalTok{, }\DataTypeTok{max=}\DecValTok{100}\NormalTok{)}

\CommentTok{#relate Y to X with a log link function}
\NormalTok{y <-}\StringTok{ }\NormalTok{gammas}\OperatorTok{*}\KeywordTok{exp}\NormalTok{(x)}

\NormalTok{data <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ x, }\DataTypeTok{y  =}\NormalTok{ y)}
\KeywordTok{summary}\NormalTok{(data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        x                 y            
##  Min.   : 0.2452   Min.   :0.000e+00  
##  1st Qu.:27.0464   1st Qu.:4.946e+11  
##  Median :51.0196   Median :1.057e+22  
##  Mean   :51.0666   Mean   :2.531e+41  
##  3rd Qu.:75.3442   3rd Qu.:4.239e+32  
##  Max.   :99.9213   Max.   :2.693e+43
\end{verbatim}

As expected, the residual plots are all perfect because the model is perfect.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{glm <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\NormalTok{x, }\DataTypeTok{family =} \KeywordTok{Gamma}\NormalTok{(}\DataTypeTok{link =} \StringTok{"log"}\NormalTok{), }\DataTypeTok{data =}\NormalTok{ data)}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(glm, }\DataTypeTok{cex =} \FloatTok{0.4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{05-linear-models_files/figure-latex/unnamed-chunk-35-1.pdf}

If we had tried using an inverse instead of the log, the residual plots would look much worse.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{glm <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\NormalTok{x, }\DataTypeTok{family =} \KeywordTok{Gamma}\NormalTok{(}\DataTypeTok{link =} \StringTok{"inverse"}\NormalTok{), }\DataTypeTok{data =}\NormalTok{ data)}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(glm, }\DataTypeTok{cex =} \FloatTok{0.4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in sqrt(crit * p * (1 - hh)/hh): NaNs produced

## Warning in sqrt(crit * p * (1 - hh)/hh): NaNs produced
\end{verbatim}

\includegraphics{05-linear-models_files/figure-latex/unnamed-chunk-36-1.pdf}

\hypertarget{gamma-with-inverse-link}{%
\subsection{Gamma with Inverse Link}\label{gamma-with-inverse-link}}

With the inverse link, the mean has a factor \texttt{1/(x\ +\ 1)}. Note that we need to add 1 to x to avoid dividing by zero.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#relate Y to X with a log link function}
\NormalTok{y <-}\StringTok{ }\NormalTok{gammas}\OperatorTok{*}\DecValTok{1}\OperatorTok{/}\NormalTok{(x }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{)}

\NormalTok{data <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ x, }\DataTypeTok{y  =}\NormalTok{ y)}
\KeywordTok{summary}\NormalTok{(data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        x                 y            
##  Min.   : 0.2452   Min.   :0.0005277  
##  1st Qu.:27.0464   1st Qu.:0.0084840  
##  Median :51.0196   Median :0.0167640  
##  Mean   :51.0666   Mean   :0.0485119  
##  3rd Qu.:75.3442   3rd Qu.:0.0365838  
##  Max.   :99.9213   Max.   :1.7125489
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{glm <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\NormalTok{x, }\DataTypeTok{family =} \KeywordTok{Gamma}\NormalTok{(}\DataTypeTok{link =} \StringTok{"inverse"}\NormalTok{), }\DataTypeTok{data =}\NormalTok{ data)}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(glm, }\DataTypeTok{cex =} \FloatTok{0.4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{05-linear-models_files/figure-latex/unnamed-chunk-38-1.pdf}

\hypertarget{log-transforms-of-continuous-predictors}{%
\section{Log transforms of continuous predictors}\label{log-transforms-of-continuous-predictors}}

When a log link is used, taking the natural logs of continuous variables allows for the scale of each predictor to match the scale of the thing that they are predicting, the log of the mean of the response. In addition, when the distribution of the continuous variable is skewed, taking the log helps to make it more symmetric.

After taking the log of a predictor, the interpretation becomes a \emph{power transform} of the original variable.

For \(\mu\) the mean response,

\[log(\mu) = \beta_0 + \beta_1 log(X)\]
To solve for \(\mu\), take the exonent of both sides

\[\mu = e^{\beta_1} e^{\beta_1 log(X)} = e^{\beta_0} X^{\beta_1}\]

\hypertarget{reference-levels}{%
\section{Reference levels}\label{reference-levels}}

When a categorical variable is used in a GLM, the model actually uses indicator variables for each level. The default reference level is the order of the R factors. For the \texttt{sex} variable, the order is \texttt{female} and then \texttt{male}. This means that the base level is \texttt{female} by default.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{health_insurance}\OperatorTok{$}\NormalTok{sex }\OperatorTok{%>%}\StringTok{ }\KeywordTok{as.factor}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }\KeywordTok{levels}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "female" "male"
\end{verbatim}

Why does this matter? Statistically, the coefficients are most stable when there are more observations.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{health_insurance}\OperatorTok{$}\NormalTok{sex }\OperatorTok{%>%}\StringTok{ }\KeywordTok{as.factor}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }\KeywordTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## female   male 
##    662    676
\end{verbatim}

There is already a function to do this in the \texttt{tidyverse} called \texttt{fct\_infreq}. Let's quickly fix the \texttt{sex} column so that these factor levels are in order of frequency.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{health_insurance <-}\StringTok{ }\NormalTok{health_insurance }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{sex =} \KeywordTok{fct_infreq}\NormalTok{(sex))}
\end{Highlighting}
\end{Shaded}

Now \texttt{male} is the base level.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{health_insurance}\OperatorTok{$}\NormalTok{sex }\OperatorTok{%>%}\StringTok{ }\KeywordTok{as.factor}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }\KeywordTok{levels}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "male"   "female"
\end{verbatim}

\hypertarget{interactions}{%
\section{Interactions}\label{interactions}}

An interaction occurs when the effect of a variable on the response is different depending on the level of other variables in the model.

Consider this model:

Let \(x_2\) be an indicator variable, which is 1 for some records and 0 otherwise.

\[\hat{y_i} = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2\]

There are now two different linear models dependong on whether \texttt{x\_1} is 0 or 1.

When \(x_1 = 0\),

\[\hat{y_i} = \beta_0  + \beta_2 x_2\]

and when \(x_1 = 1\)

\[\hat{y_i} = \beta_0 + \beta_1 + \beta_2 x_2 + \beta_3 x_2\]
By rewriting this we can see that the intercept changes from \(\beta_0\) to \(\beta_0^*\) and the slope changes from \(\beta_1\) to \(\beta_1^*\)

\[
(\beta_0 + \beta_1) + (\beta_2 + \beta_3 ) x_2 \\
 = \beta_0^* + \beta_1^* x_2
\]

The SOA's modules give an example with the using age and gender as below. This is not a very strong interaction, as the slopes are almost identical across \texttt{gender}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{interactions }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(age, actual, }\DataTypeTok{color =}\NormalTok{ gender)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_line}\NormalTok{() }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{title =} \StringTok{"Age vs. Actual by Gender"}\NormalTok{, }
       \DataTypeTok{subtitle =} \StringTok{"Interactions imply different slopes"}\NormalTok{,}
       \DataTypeTok{caption=} \StringTok{"data: interactions"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{05-linear-models_files/figure-latex/unnamed-chunk-43-1.pdf}
\caption{\label{fig:unnamed-chunk-43}Example of weak interaction}
\end{figure}

Here is a clearer example from the \texttt{auto\_claim} data. The lines show the slope of a linear model, assuming that only \texttt{BLUEBOOK} and \texttt{CAR\_TYPE} were predictors in the model. You can see that the slope for Sedans and Sports Cars is higher than for Vans and Panel Trucks.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{auto_claim }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\KeywordTok{log}\NormalTok{(CLM_AMT), }\KeywordTok{log}\NormalTok{(BLUEBOOK), }\DataTypeTok{color =}\NormalTok{ CAR_TYPE)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{0.3}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{, }\DataTypeTok{se =}\NormalTok{ F) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{title =} \StringTok{"Kelly Bluebook Value vs Claim Amount"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{05-linear-models_files/figure-latex/unnamed-chunk-44-1.pdf}
\caption{\label{fig:unnamed-chunk-44}Example of strong interaction}
\end{figure}

Any time that the effect that one variable has on the response is different depending on the value of other variables we say that there is an interaction. We can also use an hypothesis test with a GLM to check this. Simply include an interaction term and see if the coefficient is zero at the desired significance level.

\hypertarget{poisson-regression}{%
\section{Poisson Regression}\label{poisson-regression}}

When counting something, numbers can only be positive and increase by increments of 1. Statistically, the name for this is a Poisson Process, which is a model for a serious of discrete events where the average time between events is known, called the ``rate'' \(\lambda\), but the exact timing of events is unknown. We could just fit a single rate for all observations, but this would often be a simplification. For a time interval of length \(m\), the expected number of events is \(\lambda m\).

By using a GLM, we can fit a different rate for each observation. Because the response is a count, the appropriate response distribution is the Poisson.

\[Y_i|X_i \sim \text{Poisson}(\lambda_i m_i)\]

When all observations have the same exposure, \(m = 1\). When the mean of the data is far from the variance, an additional parameter known as the \emph{dispersion parameter} is used. A classic example is when modeling insurance claim counts which have a lot of zero claims. Then the model is said to be an ``over-dispersed Poisson'' or ``zero-inflated'' model.

\hypertarget{offsets}{%
\section{Offsets}\label{offsets}}

In certain situations, it is convenient to include a constant term in the linear predictor. This is the same as including a variable that has a coefficient equal to 1. We call this an \emph{offset}.

\[g(\mu) = \beta_0 + \beta_1 X_1 + ... + \beta_p X_p + \text{offset}\]

\hypertarget{tweedie-regression}{%
\section{Tweedie regression}\label{tweedie-regression}}

While this topic is briefly mentioned on the modules, the only R libraries which support Tweedie Regression (\texttt{statmod} and \texttt{tweedie}) are not on the syllabus, and so there is no way that the SOA could ask you to build a tweedie model. This means that you can be safely skip this section.

\hypertarget{stepwise-subset-selection}{%
\section{Stepwise subset selection}\label{stepwise-subset-selection}}

In theory, we could test all possible combinations of variables and interaction terms. This includes all \(p\) models with one predictor, all p-choose-2 models with two predictors, all p-choose-3 models with three predictors, and so forth. Then we take whichever model has the best performance as the final model.

This ``brute force'' approach is statistically ineffective: the more variables which are searched, the higher the chance of finding models that overfit.

A subtler method, known as \emph{stepwise selection}, reduces the chances of overfitting by only looking at the most promising models.

\textbf{Forward Stepwise Selection:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Start with no predictors in the model;
\item
  Evaluate all \(p\) models which use only one predictor and choose the one with the best performance (highest \(R^2\) or lowest \(\text{RSS}\));
\item
  Repeat the process when adding one additional predictor, and continue until there is a model with one predictor, a model with two predictors, a model with three predictors, and so forth until there are \(p\) models;
\item
  Select the single best model which has the best \(\text{AIC}\),\(\text{BIC}\), or adjusted \(R^2\).
\end{enumerate}

\textbf{Backward Stepwise Selection:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Start with a model that contains all predictors;
\item
  Create a model which removes all predictors;
\item
  Choose the best model which removes all-but-one predictor;
\item
  Choose the best model which removes all-but-two predictors;
\item
  Continue until there are \(p\) models;
\item
  Select the single best model which has the best \(\text{AIC}\),\(\text{BIC}\), or adjusted \(R^2\).
\end{enumerate}

\textbf{Both Forward \& Backward Selection:}

A hybrid approach is to consider use both forward and backward selection. This is done by creating two lists of variables at each step, one from forward and one from backward selection. Then variables from \emph{both} lists are tested to see if adding or subtracting from the current model would improve the fit or not. ISLR does not mention this directly, however, by default the \texttt{stepAIC} function uses a default of \texttt{both}.

\begin{quote}
\textbf{Tip}: Always load the \texttt{MASS} library before \texttt{dplyr} or \texttt{tidyverse}. Otherwise there will be conflicts as there are functions named \texttt{select()} and \texttt{filter()} in both. Alternatively, specify the library in the function call with \texttt{dplyr::select()}.
\end{quote}

\begin{longtable}[]{@{}ll@{}}
\toprule
Readings &\tabularnewline
\midrule
\endhead
\href{https://contentpreview.s3.us-east-2.amazonaws.com/CAS+Monograph+5+-+Generalized+Linear+Models+for+Insurance+Ratemaking.pdf}{CAS Monograph 5 Chapter 2} &\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{advantages-and-disadvantages}{%
\section{Advantages and disadvantages}\label{advantages-and-disadvantages}}

There is usually at least one question on the PA exam which asks you to ``list some of the advantages and disadvantages of using this particular model'', and so here is one such list. It is unlikely that the grader will take off points for including too many comments and so a good strategy is to include everything that comes to mind.

\textbf{GLM Advantages}

\begin{itemize}
\tightlist
\item
  Easy to interpret
\item
  Can easily be deployed in spreadsheet format
\item
  Handles skewed data through different response distributions
\item
  Models the average response which leads to stable predictions on new data
\item
  Handles continuous and categorical data
\end{itemize}

\textbf{GLM Disadvantages}

\begin{itemize}
\tightlist
\item
  Does not select features (without stepwise selection)
\item
  Strict assumptions around distribution shape, randomness of error terms, and variable correlations
\item
  Unable to detect non-linearity directly (although this can manually be addressed through feature engineering)
\item
  Sensitive to outliers
\item
  Low predictive power
\end{itemize}

\hypertarget{logistic-regression}{%
\chapter{Logistic Regression}\label{logistic-regression}}

\hypertarget{model-form}{%
\section{Model form}\label{model-form}}

Logistic regression is a special type of GLM. The name is confusing because the objective is \emph{classification} and not regression. While most examples focus on binary classification, logistic regression also works for multiclass classification.

The model form is as before

\[g(\mathbf{\hat{y}}) = \mathbf{X} \mathbf{\beta}\]

However, now the target \(y_i\) is a category. Our objective is to predict a probability of being in each category. For regression, \(\hat{y_i}\) can be any number, but now we need \(0 \leq \hat{y_i} \leq 1\).

We can use a special link function, known as the \emph{standard logistic function}, \emph{sigmoid}, or \emph{logit}, to force the output to be in this range of \(\{0,1\}\).

\[\mathbf{\hat{y}} = g^{-1}(\mathbf{X} \mathbf{\beta}) = \frac{1}{1 + e^{-\mathbf{X} \mathbf{\beta}}}\]

\begin{figure}

{\centering \includegraphics{05-linear-models_files/figure-latex/unnamed-chunk-45-1} 

}

\caption{Standard Logistic Function}\label{fig:unnamed-chunk-45}
\end{figure}

Other link functions for classification problems are possible as well, although the logistic function is the most common. If a problem asks for an alternative link, such as the \emph{probit}, fit both models and compare the performance.

\hypertarget{example-2}{%
\section{Example}\label{example-2}}

Using the \texttt{auto\_claim} data, we predict whether or not a policy has a claim. This is also known as the \emph{claim frequency}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{auto_claim }\OperatorTok{%>%}\StringTok{ }\KeywordTok{count}\NormalTok{(CLM_FLAG)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 2
##   CLM_FLAG     n
##   <chr>    <int>
## 1 No        7556
## 2 Yes       2740
\end{verbatim}

About 40\% do not have a claim while 60\% have at least one claim.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}
\NormalTok{index <-}\StringTok{ }\KeywordTok{createDataPartition}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ auto_claim}\OperatorTok{$}\NormalTok{CLM_FLAG, }
                             \DataTypeTok{p =} \FloatTok{0.8}\NormalTok{, }\DataTypeTok{list =}\NormalTok{ F) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{()}
\NormalTok{auto_claim <-}\StringTok{ }\NormalTok{auto_claim }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{target =} \KeywordTok{as.factor}\NormalTok{(}\KeywordTok{ifelse}\NormalTok{(CLM_FLAG }\OperatorTok{==}\StringTok{ "Yes"}\NormalTok{, }\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{)))}
\NormalTok{train <-}\StringTok{  }\NormalTok{auto_claim }\OperatorTok{%>%}\StringTok{ }\KeywordTok{slice}\NormalTok{(index)}
\NormalTok{test <-}\StringTok{ }\NormalTok{auto_claim }\OperatorTok{%>%}\StringTok{ }\KeywordTok{slice}\NormalTok{(}\OperatorTok{-}\NormalTok{index)}

\NormalTok{frequency <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(target }\OperatorTok{~}\StringTok{ }\NormalTok{AGE }\OperatorTok{+}\StringTok{ }\NormalTok{GENDER }\OperatorTok{+}\StringTok{ }\NormalTok{MARRIED }\OperatorTok{+}\StringTok{ }\NormalTok{CAR_USE }\OperatorTok{+}\StringTok{ }
\StringTok{                   }\NormalTok{BLUEBOOK }\OperatorTok{+}\StringTok{ }\NormalTok{CAR_TYPE }\OperatorTok{+}\StringTok{ }\NormalTok{AREA, }
                 \DataTypeTok{data=}\NormalTok{train, }
                 \DataTypeTok{family =} \KeywordTok{binomial}\NormalTok{(}\DataTypeTok{link=}\StringTok{"logit"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

All of the variables except for the \texttt{CAR\_TYPE} and \texttt{GENDERM} are highly significant. The car types \texttt{SPORTS\ CAR} and \texttt{SUV} appear to be significant, and so if we wanted to make the model simpler we could create indicator variables for \texttt{CAR\_TYPE\ ==\ SPORTS\ CAR} and \texttt{CAR\_TYPE\ ==\ SUV}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{frequency }\OperatorTok{%>%}\StringTok{ }\KeywordTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## glm(formula = target ~ AGE + GENDER + MARRIED + CAR_USE + BLUEBOOK + 
##     CAR_TYPE + AREA, family = binomial(link = "logit"), data = train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.8431  -0.8077  -0.5331   0.9575   3.0441  
## 
## Coefficients:
##                      Estimate Std. Error z value Pr(>|z|)    
## (Intercept)        -3.523e-01  2.517e-01  -1.400  0.16160    
## AGE                -2.289e-02  3.223e-03  -7.102 1.23e-12 ***
## GENDERM            -1.124e-02  9.304e-02  -0.121  0.90383    
## MARRIEDYes         -6.028e-01  5.445e-02 -11.071  < 2e-16 ***
## CAR_USEPrivate     -1.008e+00  6.569e-02 -15.350  < 2e-16 ***
## BLUEBOOK           -4.025e-05  4.699e-06  -8.564  < 2e-16 ***
## CAR_TYPEPickup     -6.687e-02  1.390e-01  -0.481  0.63048    
## CAR_TYPESedan      -3.689e-01  1.383e-01  -2.667  0.00765 ** 
## CAR_TYPESports Car  6.159e-01  1.891e-01   3.256  0.00113 ** 
## CAR_TYPESUV         2.982e-01  1.772e-01   1.683  0.09240 .  
## CAR_TYPEVan        -8.983e-03  1.319e-01  -0.068  0.94569    
## AREAUrban           2.128e+00  1.064e-01  19.993  < 2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 9544.3  on 8236  degrees of freedom
## Residual deviance: 8309.6  on 8225  degrees of freedom
## AIC: 8333.6
## 
## Number of Fisher Scoring iterations: 5
\end{verbatim}

The signs of the coefficients tell if the probability of having a claim is either increasing or decreasing by each variable. For example, the likelihood of an accident

\begin{itemize}
\tightlist
\item
  Decreases as the age of the car increases
\item
  Is lower for men
\item
  Is higher for sports cars and SUVs
\end{itemize}

The p-values tell us if the variable is significant.

\begin{itemize}
\tightlist
\item
  \texttt{Age}, \texttt{MarriedYes}, \texttt{CAR\_USEPrivate}, \texttt{BLUEBOOK}, and \texttt{AreaUrban} are significant.
\item
  Certain values of \texttt{CAR\_TYPE} are significant but others are not.
\end{itemize}

The output is a predicted probability. We can see that this is centered around a probability of about 0.5.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{preds <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(frequency, }\DataTypeTok{newdat=}\NormalTok{test,}\DataTypeTok{type=}\StringTok{"response"}\NormalTok{)}
\KeywordTok{qplot}\NormalTok{(preds) }
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{05-linear-models_files/figure-latex/unnamed-chunk-49-1.pdf}
\caption{\label{fig:unnamed-chunk-49}Distribution of Predicted Probability}
\end{figure}

In order to convert these values to predicted 0's and 1's, we assign a \emph{cutoff} value so that if \(\hat{y}\) is above this threshold we use a 1 and 0 othersise. The default cutoff is 0.5. We change this to 0.3 and see that there are 763 policies predicted to have claims.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test <-}\StringTok{ }\NormalTok{test }\OperatorTok{%>%}\StringTok{ }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{pred_zero_one =} \KeywordTok{as.factor}\NormalTok{(}\DecValTok{1}\OperatorTok{*}\NormalTok{(preds}\OperatorTok{>}\NormalTok{.}\DecValTok{3}\NormalTok{)))}
\KeywordTok{summary}\NormalTok{(test}\OperatorTok{$}\NormalTok{pred_zero_one)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    0    1 
## 1296  763
\end{verbatim}

How do we decide on this cutoff value? We need to compare cutoff values based on some evaluation metric. For example, we can use \emph{accuracy}.

\[\text{Accuracy} = \frac{\text{Correct Guesses}}{\text{Total Guesses}}\]

This results in an accuracy of 70\%. But is this good?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test }\OperatorTok{%>%}\StringTok{ }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{accuracy =} \KeywordTok{mean}\NormalTok{(pred_zero_one }\OperatorTok{==}\StringTok{ }\NormalTok{target))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 1
##   accuracy
##      <dbl>
## 1    0.699
\end{verbatim}

Consider what would happen if we just predicted all 0's. The accuracy is 74\%.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test }\OperatorTok{%>%}\StringTok{ }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{accuracy =} \KeywordTok{mean}\NormalTok{(}\DecValTok{0} \OperatorTok{==}\StringTok{ }\NormalTok{target))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 1
##   accuracy
##      <dbl>
## 1    0.734
\end{verbatim}

For policies which experience claims the accuracy is 63\%.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(target }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{accuracy =} \KeywordTok{mean}\NormalTok{(pred_zero_one }\OperatorTok{==}\StringTok{ }\NormalTok{target))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 1
##   accuracy
##      <dbl>
## 1    0.631
\end{verbatim}

But for policies that don't actually experience claims this is 72\%.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(target }\OperatorTok{==}\StringTok{ }\DecValTok{0}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{accuracy =} \KeywordTok{mean}\NormalTok{(pred_zero_one }\OperatorTok{==}\StringTok{ }\NormalTok{target))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 1
##   accuracy
##      <dbl>
## 1    0.724
\end{verbatim}

How do we know if this is a good model? We can repeat this process with a different cutoff value and get different accuracy metrics for these groups. Let's use a cutoff of 0.6.

75\%

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test <-}\StringTok{ }\NormalTok{test }\OperatorTok{%>%}\StringTok{ }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{pred_zero_one =} \KeywordTok{as.factor}\NormalTok{(}\DecValTok{1}\OperatorTok{*}\NormalTok{(preds}\OperatorTok{>}\NormalTok{.}\DecValTok{6}\NormalTok{)))}
\NormalTok{test }\OperatorTok{%>%}\StringTok{ }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{accuracy =} \KeywordTok{mean}\NormalTok{(pred_zero_one }\OperatorTok{==}\StringTok{ }\NormalTok{target))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 1
##   accuracy
##      <dbl>
## 1    0.752
\end{verbatim}

10\% for policies with claims and 98\% for policies without claims.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(target }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{accuracy =} \KeywordTok{mean}\NormalTok{(pred_zero_one }\OperatorTok{==}\StringTok{ }\NormalTok{target))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 1
##   accuracy
##      <dbl>
## 1    0.108
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(target }\OperatorTok{==}\StringTok{ }\DecValTok{0}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{accuracy =} \KeywordTok{mean}\NormalTok{(pred_zero_one }\OperatorTok{==}\StringTok{ }\NormalTok{target))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 1
##   accuracy
##      <dbl>
## 1    0.985
\end{verbatim}

The punchline is that the accuracy depends on the cutoff value, and changing the cutoff value changes whether the model is accuracy for the ``true = 1'' classes (policies with actual claims) vs.~the ``false = 0'' classes (policies without claims).

\hypertarget{classification-metrics}{%
\section{Classification metrics}\label{classification-metrics}}

For regression problems, when the output is a whole number, we can use the sum of squares \(\text{RSS}\), the r-squared \(R^2\), the mean absolute error \(\text{MAE}\), and the likelihood. For classification problems where the output is in \(\{0,1\}\), we need to a new set of metrics.

A \emph{confusion matrix} shows is a table that summarises how the model classifies each group.

\begin{itemize}
\tightlist
\item
  No claims and predicted to not have claims - \textbf{True Negatives (TN) = 1,489}
\item
  Had claims and predicted to have claims - \textbf{True Positives (TP) = 59}
\item
  No claims but predited to have claims - \textbf{False Positives (FP) = 22}
\item
  Had claims but predicted not to - \textbf{False Negatives (FN) = 489}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{confusionMatrix}\NormalTok{(test}\OperatorTok{$}\NormalTok{pred_zero_one,}\KeywordTok{factor}\NormalTok{(test}\OperatorTok{$}\NormalTok{target))}\OperatorTok{$}\NormalTok{table}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           Reference
## Prediction    0    1
##          0 1489  489
##          1   22   59
\end{verbatim}

These definitions allow us to measure performance on the different groups.

\emph{Precision} answers the question ``out of all of the positive predictions, what percentage were correct?''

\[\text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}\]

\emph{Recall} answers the question ``out of all of positive examples in the data set, what percentage were correct?''

\[\text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}\]

The choice of using precision vs.~recall depends on the relative cost of making a FP or a FN error. If FP errors are expensive, then use precision; if FN errors are expensive, then use recall.

\textbf{Example A:} the model trying to detect a deadly disease, which only 1 out of every 1000 patient's survive without early detection. Then the goal should be to optimize \emph{recall}, because we would want every patient that has the disease to get detected.

\textbf{Example B:} the model is detecting which emails are spam or not. If an important email is flagged as spam incorrectly, the cost is 5 hours of lost productivity. In this case, \emph{precision} is the main concern.

In some cases we can compare this ``cost'' in actual values. For example, if a federal court is predicting if a criminal will recommit or not, they can agree that ``1 out of every 20 guilty individuals going free'' in exchange for ``90\% of those who are guilty being convicted''. When money is involed, this a dollar amount can be used: flagging non-spam as spam may cost \$100 whereas missing a spam email may cost \$2. Then the cost-weighted accuracy is

\[\text{Cost} = (100)(\text{FN}) + (2)(\text{FP})\]

Then the cutoff value can be tuned in order to find the minimum cost.

Fortunately, all of this is handled in a single function called \texttt{confusionMatrix}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{confusionMatrix}\NormalTok{(test}\OperatorTok{$}\NormalTok{pred_zero_one,}\KeywordTok{factor}\NormalTok{(test}\OperatorTok{$}\NormalTok{target))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    0    1
##          0 1489  489
##          1   22   59
##                                           
##                Accuracy : 0.7518          
##                  95% CI : (0.7326, 0.7704)
##     No Information Rate : 0.7339          
##     P-Value [Acc > NIR] : 0.03366         
##                                           
##                   Kappa : 0.1278          
##                                           
##  Mcnemar's Test P-Value : < 2e-16         
##                                           
##             Sensitivity : 0.9854          
##             Specificity : 0.1077          
##          Pos Pred Value : 0.7528          
##          Neg Pred Value : 0.7284          
##              Prevalence : 0.7339          
##          Detection Rate : 0.7232          
##    Detection Prevalence : 0.9607          
##       Balanced Accuracy : 0.5466          
##                                           
##        'Positive' Class : 0               
## 
\end{verbatim}

\hypertarget{area-under-the-roc-curv-auc}{%
\subsection{Area Under the ROC Curv (AUC)}\label{area-under-the-roc-curv-auc}}

What if we look at both the true-positive rate (TPR) and false positive rate (FPR) simultaneously? That is, for each value of the cutoff, we can calculate the TPR and TNR.

For example, say that we have 10 cutoff values, \(\{k_1, k_2, ..., k_{10}\}\). Then for each value of \(k\) we calculate both the true positive rates

\[\text{TPR} = \{\text{TPR}(k_1), \text{TPR}(k_2), .., \text{TPR}(k_{10})\} \]

and the true negative rates

\[\{\text{FNR} = \{\text{FNR}(k_1), \text{FNR}(k_2), .., \text{FNR}(k_{10})\}\]

Then we set \texttt{x\ =\ TPR} and \texttt{y\ =\ FNR} and graph x against y. The plot below shows the ROC for the \texttt{auto\_claims} data. The Area Under the Curv of 0.6795 is what we would get if we integrated under the curve.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(pROC)}
\KeywordTok{roc}\NormalTok{(test}\OperatorTok{$}\NormalTok{target, preds, }\DataTypeTok{plot =}\NormalTok{ T)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{05-linear-models_files/figure-latex/unnamed-chunk-59-1.pdf}
\caption{\label{fig:unnamed-chunk-59}AUC for auto\_claim}
\end{figure}

\begin{verbatim}
## 
## Call:
## roc.default(response = test$target, predictor = preds, plot = T)
## 
## Data: preds in 1511 controls (test$target 0) < 548 cases (test$target 1).
## Area under the curve: 0.7558
\end{verbatim}

If we just randomly guess, the AUC would be 0.5, which is represented by the 45-degree line. A perfect model would maximize the curve to the upper-left corner.

AUC is preferred over Accuracy when there are a lot more ``true'' classes than ``false'' classes, which is known as having **class imbalance*. An example is bank fraud detection: 99.99\% of bank transactions are ``false'' or ``0'' classes, and so optimizing for accuracy alone will result in a low sensitivity for detecting actual fraud.

\hypertarget{additional-reading}{%
\subsection{Additional reading}\label{additional-reading}}

\begin{longtable}[]{@{}ll@{}}
\toprule
Title & Source\tabularnewline
\midrule
\endhead
An Overview of Classification & ISL 4.1\tabularnewline
\href{https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5\#:~:targetText=What\%20is\%20AUC\%20\%2D\%20ROC\%20Curve\%3F,capable\%20of\%20distinguishing\%20between\%20classes.}{Understanding AUC - ROC Curv} & Sarang Narkhede, Towards Data Science\tabularnewline
\href{https://towardsdatascience.com/precision-vs-recall-386cf9f89488\#:~:targetText=Precision\%20and\%20recall\%20are\%20two,correctly\%20classified\%20by\%20your\%20algorithm.}{Precision vs.~Recall} & Shruti Saxena, Towards Data Science\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{penalized-linear-models}{%
\chapter{Penalized Linear Models}\label{penalized-linear-models}}

One of the main weaknesses of the GLM, including all linear models in this chapter, is that the features need to be selected by hand. Stepwise selection helps to improve this process, but fails when the inputs are correlated and often has a strong dependence on seemingly arbitrary choices of evaluation metrics such as using AIC or BIC and forward or backwise directions.

The Bias Variance Tradoff is about finding the lowest error by changing the flexibility of the model. Penalization methods use a parameter to control for this flexibility directly.

Earlier on we said that the linear model minimizes the sum of square terms, known as the residual sum of squares (RSS)

\[
\text{RSS} = \sum_i(y_i - \hat{y})^2 = \sum_i(y_i - \beta_0 - \sum_{j = 1}^p\beta_j x_{ij})^2
\]

This loss function can be modified so that models which include more (and larger) coefficients are considered as worse. In other words, when there are more \(\beta\)'s, or \(\beta\)'s which are larger, the RSS is higher.

\hypertarget{ridge-regression}{%
\section{Ridge Regression}\label{ridge-regression}}

Ridge regression adds a penalty term which is proportional to the square of the sum of the coefficients. This is known as the ``L2'' norm.

\[
\sum_i(y_i - \beta_0 - \sum_{j = 1}^p\beta_j x_{ij})^2 + \lambda \sum_{j = 1}^p\beta_j^2
\]

This \(\lambda\) controls how much of a penalty is imposed on the size of the coefficients. When \(\lambda\) is high, simpler models are treated more favorably because the \(\sum_{j = 1}^p\beta_j^2\) carries more weight. Conversely, then \(\lambda\) is low, complex models are more favored. When \(\lambda = 0\), we have an ordinary GLM.

\hypertarget{lasso}{%
\section{Lasso}\label{lasso}}

The official name is the Least Absolute Shrinkage and Selection Operator, but the common name is just ``the lasso''. Just as with Ridge regression, we want to favor simpler models; however, we also want to \emph{select} variables. This is the same as forcing some coefficients to be equal to 0.

Instead of taking the square of the coefficients (L2 norm), we take the absolute value (L1 norm).

\[
\sum_i(y_i - \beta_0 - \sum_{j = 1}^p\beta_j x_{ij})^2 + \lambda \sum_{j = 1}^p|\beta_j|
\]

In ISLR, Hastie et al show that this results in coefficients being forced to be exactly 0. This is extremely useful because it means that by changing \(\lambda\), we can select how many variables to use in the model.

\textbf{Note}: While any response family is possible with penalized regression, in R, only the Gaussian family is possible in the library \texttt{glmnet}, and so this is the only type of question that the SOA can ask.

\hypertarget{elastic-net}{%
\section{Elastic Net}\label{elastic-net}}

The Elastic Net uses a penalty term which is between the L1 and L2 norms. The penalty term is a weighted average using the mixing parameter \(0 \leq \alpha \leq 1\). The loss fucntion is then

\[\text{RSS} + (1 - \alpha)/2 \sum_{j = 1}^{p}\beta_j^2 + \alpha \sum_{j = 1}^p |\beta_j|\]
When \(\alpha = 1\) is turns into a Lasso; when \(\alpha = 1\) this is the Ridge model.

Luckily, none of this needs to be memorized. On the exam, read the documentation in R to refresh your memory. For the Elastic Net, the function is \texttt{glmnet}, and so running \texttt{?glmnet} will give you this info.

\begin{quote}
\textbf{Shortcut}: When using complicated functions on the exam, use \texttt{?function\_name} to get the documentation.
\end{quote}

\hypertarget{advantages-and-disadvantages-1}{%
\section{Advantages and disadvantages}\label{advantages-and-disadvantages-1}}

\textbf{Elastic Net/Lasso/Ridge Advantages}

\begin{itemize}
\tightlist
\item
  All benefits from GLMS
\item
  Automatic variable selection for Lasso; smaller coefficients for Ridge
\item
  Better predictive power than GLM
\end{itemize}

\textbf{Elastic Net/Lasso/Ridge Disadvantages}

\begin{itemize}
\tightlist
\item
  All cons of GLMs
\end{itemize}

\begin{longtable}[]{@{}ll@{}}
\toprule
Readings &\tabularnewline
\midrule
\endhead
ISLR 6.1 Subset Selection &\tabularnewline
ISLR 6.2 Shrinkage Methods &\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{example-ridge-regression}{%
\section{Example: Ridge Regression}\label{example-ridge-regression}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ISLR)}
\KeywordTok{library}\NormalTok{(glmnet)}
\KeywordTok{library}\NormalTok{(dplyr)}
\KeywordTok{library}\NormalTok{(tidyr)}
\end{Highlighting}
\end{Shaded}

We will use the \texttt{glmnet} package in order to perform ridge regression and
the lasso. The main function in this package is \texttt{glmnet()}, which can be used
to fit ridge regression models, lasso models, and more. This function has
slightly different syntax from other model-fitting functions that we have
encountered thus far in this book. In particular, we must pass in an \(x\)
matrix as well as a \(y\) vector, and we do not use the \(y \sim x\) syntax.

Before proceeding, let's first ensure that the missing values have
been removed from the data, as described in the previous lab.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Hitters =}\StringTok{ }\KeywordTok{na.omit}\NormalTok{(Hitters)}
\end{Highlighting}
\end{Shaded}

We will now perform ridge regression and the lasso in order to predict \texttt{Salary} on
the \texttt{Hitters} data. Let's set up our data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x =}\StringTok{ }\KeywordTok{model.matrix}\NormalTok{(Salary}\OperatorTok{~}\NormalTok{., Hitters)[,}\OperatorTok{-}\DecValTok{1}\NormalTok{] }\CommentTok{# trim off the first column}
                                         \CommentTok{# leaving only the predictors}
\NormalTok{y =}\StringTok{ }\NormalTok{Hitters }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(Salary) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{unlist}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{as.numeric}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

The \texttt{model.matrix()} function is particularly useful for creating \(x\); not only
does it produce a matrix corresponding to the 19 predictors but it also
automatically transforms any qualitative variables into dummy variables.
The latter property is important because \texttt{glmnet()} can only take numerical,
quantitative inputs.

The \texttt{glmnet()} function has an alpha argument that determines what type
of model is fit. If \texttt{alpha\ =\ 0} then a ridge regression model is fit, and if \texttt{alpha\ =\ 1}
then a lasso model is fit. We first fit a ridge regression model:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grid =}\StringTok{ }\DecValTok{10}\OperatorTok{^}\KeywordTok{seq}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{-2}\NormalTok{, }\DataTypeTok{length =} \DecValTok{100}\NormalTok{)}
\NormalTok{ridge_mod =}\StringTok{ }\KeywordTok{glmnet}\NormalTok{(x, y, }\DataTypeTok{alpha =} \DecValTok{0}\NormalTok{, }\DataTypeTok{lambda =}\NormalTok{ grid)}
\end{Highlighting}
\end{Shaded}

By default the \texttt{glmnet()} function performs ridge regression for an automatically
selected range of \(\lambda\) values. However, here we have chosen to implement
the function over a grid of values ranging from \(\lambda = 10^10\) to \(\lambda = 10^{-2}\), essentially covering the full range of scenarios from the null model containing
only the intercept, to the least squares fit.

As we will see, we can also compute
model fits for a particular value of \(\lambda\) that is not one of the original
grid values. Note that by default, the \texttt{glmnet()} function standardizes the
variables so that they are on the same scale. To turn off this default setting,
use the argument \texttt{standardize\ =\ FALSE}.

Associated with each value of \(\lambda\) is a vector of ridge regression coefficients,
stored in a matrix that can be accessed by \texttt{coef()}. In this case, it is a \(20 \times 100\)
matrix, with 20 rows (one for each predictor, plus an intercept) and 100
columns (one for each value of \(\lambda\)).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dim}\NormalTok{(}\KeywordTok{coef}\NormalTok{(ridge_mod))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  20 100
\end{verbatim}

We expect the coefficient estimates to be much smaller, in terms of \(l_2\) norm,
when a large value of \(\lambda\) is used, as compared to when a small value of \(\lambda\) is
used. These are the coefficients when \(\lambda = 11498\), along with their \(l_2\) norm:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ridge_mod}\OperatorTok{$}\NormalTok{lambda[}\DecValTok{50}\NormalTok{] }\CommentTok{#Display 50th lambda value}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 11497.57
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{coef}\NormalTok{(ridge_mod)[,}\DecValTok{50}\NormalTok{] }\CommentTok{# Display coefficients associated with 50th lambda value}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   (Intercept)         AtBat          Hits         HmRun          Runs 
## 407.356050200   0.036957182   0.138180344   0.524629976   0.230701523 
##           RBI         Walks         Years        CAtBat         CHits 
##   0.239841459   0.289618741   1.107702929   0.003131815   0.011653637 
##        CHmRun         CRuns          CRBI        CWalks       LeagueN 
##   0.087545670   0.023379882   0.024138320   0.025015421   0.085028114 
##     DivisionW       PutOuts       Assists        Errors    NewLeagueN 
##  -6.215440973   0.016482577   0.002612988  -0.020502690   0.301433531
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{sum}\NormalTok{(}\KeywordTok{coef}\NormalTok{(ridge_mod)[}\OperatorTok{-}\DecValTok{1}\NormalTok{,}\DecValTok{50}\NormalTok{]}\OperatorTok{^}\DecValTok{2}\NormalTok{)) }\CommentTok{# Calculate l2 norm}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 6.360612
\end{verbatim}

In contrast, here are the coefficients when \(\lambda = 705\), along with their \(l_2\)
norm. Note the much larger \(l_2\) norm of the coefficients associated with this
smaller value of \(\lambda\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ridge_mod}\OperatorTok{$}\NormalTok{lambda[}\DecValTok{60}\NormalTok{] }\CommentTok{#Display 60th lambda value}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 705.4802
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{coef}\NormalTok{(ridge_mod)[,}\DecValTok{60}\NormalTok{] }\CommentTok{# Display coefficients associated with 60th lambda value}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  (Intercept)        AtBat         Hits        HmRun         Runs 
##  54.32519950   0.11211115   0.65622409   1.17980910   0.93769713 
##          RBI        Walks        Years       CAtBat        CHits 
##   0.84718546   1.31987948   2.59640425   0.01083413   0.04674557 
##       CHmRun        CRuns         CRBI       CWalks      LeagueN 
##   0.33777318   0.09355528   0.09780402   0.07189612  13.68370191 
##    DivisionW      PutOuts      Assists       Errors   NewLeagueN 
## -54.65877750   0.11852289   0.01606037  -0.70358655   8.61181213
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{sum}\NormalTok{(}\KeywordTok{coef}\NormalTok{(ridge_mod)[}\OperatorTok{-}\DecValTok{1}\NormalTok{,}\DecValTok{60}\NormalTok{]}\OperatorTok{^}\DecValTok{2}\NormalTok{)) }\CommentTok{# Calculate l2 norm}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 57.11001
\end{verbatim}

We can use the \texttt{predict()} function for a number of purposes. For instance,
we can obtain the ridge regression coefficients for a new value of \(\lambda\), say 50:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{predict}\NormalTok{(ridge_mod, }\DataTypeTok{s=}\DecValTok{50}\NormalTok{, }\DataTypeTok{type=}\StringTok{"coefficients"}\NormalTok{)[}\DecValTok{1}\OperatorTok{:}\DecValTok{20}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   (Intercept)         AtBat          Hits         HmRun          Runs 
##  4.876610e+01 -3.580999e-01  1.969359e+00 -1.278248e+00  1.145892e+00 
##           RBI         Walks         Years        CAtBat         CHits 
##  8.038292e-01  2.716186e+00 -6.218319e+00  5.447837e-03  1.064895e-01 
##        CHmRun         CRuns          CRBI        CWalks       LeagueN 
##  6.244860e-01  2.214985e-01  2.186914e-01 -1.500245e-01  4.592589e+01 
##     DivisionW       PutOuts       Assists        Errors    NewLeagueN 
## -1.182011e+02  2.502322e-01  1.215665e-01 -3.278600e+00 -9.496680e+00
\end{verbatim}

We now split the samples into a training set and a test set in order
to estimate the test error of ridge regression and the lasso.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}

\NormalTok{train =}\StringTok{ }\NormalTok{Hitters }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{sample_frac}\NormalTok{(}\FloatTok{0.5}\NormalTok{)}

\NormalTok{test =}\StringTok{ }\NormalTok{Hitters }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{setdiff}\NormalTok{(train)}

\NormalTok{x_train =}\StringTok{ }\KeywordTok{model.matrix}\NormalTok{(Salary}\OperatorTok{~}\NormalTok{., train)[,}\OperatorTok{-}\DecValTok{1}\NormalTok{]}
\NormalTok{x_test =}\StringTok{ }\KeywordTok{model.matrix}\NormalTok{(Salary}\OperatorTok{~}\NormalTok{., test)[,}\OperatorTok{-}\DecValTok{1}\NormalTok{]}

\NormalTok{y_train =}\StringTok{ }\NormalTok{train }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(Salary) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{unlist}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{as.numeric}\NormalTok{()}

\NormalTok{y_test =}\StringTok{ }\NormalTok{test }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(Salary) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{unlist}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{as.numeric}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Next we fit a ridge regression model on the training set, and evaluate
its MSE on the test set, using \(\lambda = 4\). Note the use of the \texttt{predict()}
function again: this time we get predictions for a test set, by replacing
\texttt{type="coefficients"} with the \texttt{newx} argument.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ridge_mod =}\StringTok{ }\KeywordTok{glmnet}\NormalTok{(x_train, y_train, }\DataTypeTok{alpha=}\DecValTok{0}\NormalTok{, }\DataTypeTok{lambda =}\NormalTok{ grid, }\DataTypeTok{thresh =} \FloatTok{1e-12}\NormalTok{)}
\NormalTok{ridge_pred =}\StringTok{ }\KeywordTok{predict}\NormalTok{(ridge_mod, }\DataTypeTok{s =} \DecValTok{4}\NormalTok{, }\DataTypeTok{newx =}\NormalTok{ x_test)}
\KeywordTok{mean}\NormalTok{((ridge_pred }\OperatorTok{-}\StringTok{ }\NormalTok{y_test)}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 139858.6
\end{verbatim}

The test MSE is 101242.7. Note that if we had instead simply fit a model
with just an intercept, we would have predicted each test observation using
the mean of the training observations. In that case, we could compute the
test set MSE like this:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{((}\KeywordTok{mean}\NormalTok{(y_train) }\OperatorTok{-}\StringTok{ }\NormalTok{y_test)}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 224692.1
\end{verbatim}

We could also get the same result by fitting a ridge regression model with
a very large value of \(\lambda\). Note that \texttt{1e10} means \(10^{10}\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ridge_pred =}\StringTok{ }\KeywordTok{predict}\NormalTok{(ridge_mod, }\DataTypeTok{s =} \FloatTok{1e10}\NormalTok{, }\DataTypeTok{newx =}\NormalTok{ x_test)}
\KeywordTok{mean}\NormalTok{((ridge_pred }\OperatorTok{-}\StringTok{ }\NormalTok{y_test)}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 224692.1
\end{verbatim}

So fitting a ridge regression model with \(\lambda = 4\) leads to a much lower test
MSE than fitting a model with just an intercept. We now check whether
there is any benefit to performing ridge regression with \(\lambda = 4\) instead of
just performing least squares regression. Recall that least squares is simply
ridge regression with \(\lambda = 0\).

* Note: In order for \texttt{glmnet()} to yield the \textbf{exact} least squares coefficients when \(\lambda = 0\),
we use the argument \texttt{exact=T} when calling the \texttt{predict()} function. Otherwise, the
\texttt{predict()} function will interpolate over the grid of \(\lambda\) values used in fitting the
\texttt{glmnet()} model, yielding approximate results. Even when we use \texttt{exact\ =\ T}, there remains
a slight discrepancy in the third decimal place between the output of \texttt{glmnet()} when
\(\lambda = 0\) and the output of \texttt{lm()}; this is due to numerical approximation on the part of
\texttt{glmnet()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ridge_pred =}\StringTok{ }\KeywordTok{predict}\NormalTok{(ridge_mod, }\DataTypeTok{s =} \DecValTok{0}\NormalTok{, }\DataTypeTok{x =}\NormalTok{ x_train, }\DataTypeTok{y =}\NormalTok{ y_train, }\DataTypeTok{newx =}\NormalTok{ x_test, }\DataTypeTok{exact =}\NormalTok{ T)}
\KeywordTok{mean}\NormalTok{((ridge_pred }\OperatorTok{-}\StringTok{ }\NormalTok{y_test)}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 175051.7
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{lm}\NormalTok{(Salary}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ train)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = Salary ~ ., data = train)
## 
## Coefficients:
## (Intercept)        AtBat         Hits        HmRun         Runs  
##   2.398e+02   -1.639e-03   -2.179e+00    6.337e+00    7.139e-01  
##         RBI        Walks        Years       CAtBat        CHits  
##   8.735e-01    3.594e+00   -1.309e+01   -7.136e-01    3.316e+00  
##      CHmRun        CRuns         CRBI       CWalks      LeagueN  
##   3.407e+00   -5.671e-01   -7.525e-01    2.347e-01    1.322e+02  
##   DivisionW      PutOuts      Assists       Errors   NewLeagueN  
##  -1.346e+02    2.099e-01    6.229e-01   -4.616e+00   -8.330e+01
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{predict}\NormalTok{(ridge_mod, }\DataTypeTok{s =} \DecValTok{0}\NormalTok{, }\DataTypeTok{x =}\NormalTok{ x_train, }\DataTypeTok{y =}\NormalTok{ y_train, }\DataTypeTok{exact =}\NormalTok{ T, }\DataTypeTok{type=}\StringTok{"coefficients"}\NormalTok{)[}\DecValTok{1}\OperatorTok{:}\DecValTok{20}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   (Intercept)         AtBat          Hits         HmRun          Runs 
##  239.83274953   -0.00175359   -2.17853087    6.33694957    0.71369687 
##           RBI         Walks         Years        CAtBat         CHits 
##    0.87329878    3.59421378  -13.09231408   -0.71351092    3.31523605 
##        CHmRun         CRuns          CRBI        CWalks       LeagueN 
##    3.40701392   -0.56709530   -0.75240961    0.23467433  132.15949536 
##     DivisionW       PutOuts       Assists        Errors    NewLeagueN 
## -134.58503816    0.20992473    0.62288126   -4.61583857  -83.29432536
\end{verbatim}

It looks like we are indeed improving over regular least-squares! Side note: in general, if we want to fit a (unpenalized) least squares model, then
we should use the \texttt{lm()} function, since that function provides more useful
outputs, such as standard errors and \(p\)-values for the coefficients.

Instead of arbitrarily choosing \(\lambda = 4\), it would be better to
use cross-validation to choose the tuning parameter \(\lambda\). We can do this using
the built-in cross-validation function, \texttt{cv.glmnet()}. By default, the function
performs 10-fold cross-validation, though this can be changed using the
argument \texttt{folds}. Note that we set a random seed first so our results will be
reproducible, since the choice of the cross-validation folds is random.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{cv.out =}\StringTok{ }\KeywordTok{cv.glmnet}\NormalTok{(x_train, y_train, }\DataTypeTok{alpha =} \DecValTok{0}\NormalTok{) }\CommentTok{# Fit ridge regression model on training data}
\KeywordTok{plot}\NormalTok{(cv.out) }\CommentTok{# Draw plot of training MSE as a function of lambda}
\end{Highlighting}
\end{Shaded}

\includegraphics{05-linear-models_files/figure-latex/unnamed-chunk-73-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bestlam =}\StringTok{ }\NormalTok{cv.out}\OperatorTok{$}\NormalTok{lambda.min  }\CommentTok{# Select lamda that minimizes training MSE}
\NormalTok{bestlam}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 326.1406
\end{verbatim}

Therefore, we see that the value of \(\lambda\) that results in the smallest cross-validation
error is 339.1845 What is the test MSE associated with this value of
\(\lambda\)?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ridge_pred =}\StringTok{ }\KeywordTok{predict}\NormalTok{(ridge_mod, }\DataTypeTok{s =}\NormalTok{ bestlam, }\DataTypeTok{newx =}\NormalTok{ x_test) }\CommentTok{# Use best lambda to predict test data}
\KeywordTok{mean}\NormalTok{((ridge_pred }\OperatorTok{-}\StringTok{ }\NormalTok{y_test)}\OperatorTok{^}\DecValTok{2}\NormalTok{) }\CommentTok{# Calculate test MSE}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 140056.2
\end{verbatim}

This represents a further improvement over the test MSE that we got using
\(\lambda = 4\). Finally, we refit our ridge regression model on the full data set,
using the value of \(\lambda\) chosen by cross-validation, and examine the coefficient
estimates.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{out =}\StringTok{ }\KeywordTok{glmnet}\NormalTok{(x, y, }\DataTypeTok{alpha =} \DecValTok{0}\NormalTok{) }\CommentTok{# Fit ridge regression model on full dataset}
\KeywordTok{predict}\NormalTok{(out, }\DataTypeTok{type =} \StringTok{"coefficients"}\NormalTok{, }\DataTypeTok{s =}\NormalTok{ bestlam)[}\DecValTok{1}\OperatorTok{:}\DecValTok{20}\NormalTok{,] }\CommentTok{# Display coefficients using lambda chosen by CV}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  (Intercept)        AtBat         Hits        HmRun         Runs 
##  15.44835008   0.07716945   0.85906253   0.60120339   1.06366687 
##          RBI        Walks        Years       CAtBat        CHits 
##   0.87936073   1.62437580   1.35296287   0.01134998   0.05746377 
##       CHmRun        CRuns         CRBI       CWalks      LeagueN 
##   0.40678422   0.11455696   0.12115916   0.05299953  22.08942749 
##    DivisionW      PutOuts      Assists       Errors   NewLeagueN 
## -79.03490973   0.16618830   0.02941513  -1.36075644   9.12528398
\end{verbatim}

As expected, none of the coefficients are exactly zero - ridge regression does not
perform variable selection!

\hypertarget{example-the-lasso}{%
\section{Example: The Lasso}\label{example-the-lasso}}

We saw that ridge regression with a wise choice of \(\lambda\) can outperform least
squares as well as the null model on the Hitters data set. We now ask
whether the lasso can yield either a more accurate or a more interpretable
model than ridge regression. In order to fit a lasso model, we once again
use the \texttt{glmnet()} function; however, this time we use the argument \texttt{alpha=1}.
Other than that change, we proceed just as we did in fitting a ridge model:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lasso_mod =}\StringTok{ }\KeywordTok{glmnet}\NormalTok{(x_train, y_train, }\DataTypeTok{alpha =} \DecValTok{1}\NormalTok{, }\DataTypeTok{lambda =}\NormalTok{ grid) }\CommentTok{# Fit lasso model on training data}
\KeywordTok{plot}\NormalTok{(lasso_mod)                                          }\CommentTok{# Draw plot of coefficients}
\end{Highlighting}
\end{Shaded}

\includegraphics{05-linear-models_files/figure-latex/unnamed-chunk-76-1.pdf}

Notice that in the coefficient plot that depending on the choice of tuning
parameter, some of the coefficients are exactly equal to zero. We now
perform cross-validation and compute the associated test error:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{cv.out =}\StringTok{ }\KeywordTok{cv.glmnet}\NormalTok{(x_train, y_train, }\DataTypeTok{alpha =} \DecValTok{1}\NormalTok{) }\CommentTok{# Fit lasso model on training data}
\KeywordTok{plot}\NormalTok{(cv.out) }\CommentTok{# Draw plot of training MSE as a function of lambda}
\end{Highlighting}
\end{Shaded}

\includegraphics{05-linear-models_files/figure-latex/unnamed-chunk-77-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bestlam =}\StringTok{ }\NormalTok{cv.out}\OperatorTok{$}\NormalTok{lambda.min }\CommentTok{# Select lamda that minimizes training MSE}
\NormalTok{lasso_pred =}\StringTok{ }\KeywordTok{predict}\NormalTok{(lasso_mod, }\DataTypeTok{s =}\NormalTok{ bestlam, }\DataTypeTok{newx =}\NormalTok{ x_test) }\CommentTok{# Use best lambda to predict test data}
\KeywordTok{mean}\NormalTok{((lasso_pred }\OperatorTok{-}\StringTok{ }\NormalTok{y_test)}\OperatorTok{^}\DecValTok{2}\NormalTok{) }\CommentTok{# Calculate test MSE}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 143273
\end{verbatim}

This is substantially lower than the test set MSE of the null model and of
least squares, and very similar to the test MSE of ridge regression with \(\lambda\)
chosen by cross-validation.

However, the lasso has a substantial advantage over ridge regression in
that the resulting coefficient estimates are sparse. Here we see that 12 of
the 19 coefficient estimates are exactly zero:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{out =}\StringTok{ }\KeywordTok{glmnet}\NormalTok{(x, y, }\DataTypeTok{alpha =} \DecValTok{1}\NormalTok{, }\DataTypeTok{lambda =}\NormalTok{ grid) }\CommentTok{# Fit lasso model on full dataset}
\NormalTok{lasso_coef =}\StringTok{ }\KeywordTok{predict}\NormalTok{(out, }\DataTypeTok{type =} \StringTok{"coefficients"}\NormalTok{, }\DataTypeTok{s =}\NormalTok{ bestlam)[}\DecValTok{1}\OperatorTok{:}\DecValTok{20}\NormalTok{,] }\CommentTok{# Display coefficients using lambda chosen by CV}
\NormalTok{lasso_coef}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   (Intercept)         AtBat          Hits         HmRun          Runs 
##    1.27429897   -0.05490834    2.18012455    0.00000000    0.00000000 
##           RBI         Walks         Years        CAtBat         CHits 
##    0.00000000    2.29189433   -0.33767315    0.00000000    0.00000000 
##        CHmRun         CRuns          CRBI        CWalks       LeagueN 
##    0.02822467    0.21627609    0.41713051    0.00000000   20.28190194 
##     DivisionW       PutOuts       Assists        Errors    NewLeagueN 
## -116.16524424    0.23751978    0.00000000   -0.85604181    0.00000000
\end{verbatim}

Selecting only the predictors with non-zero coefficients, we see that the lasso model with \(\lambda\)
chosen by cross-validation contains only seven variables:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lasso_coef[lasso_coef}\OperatorTok{!=}\DecValTok{0}\NormalTok{] }\CommentTok{# Display only non-zero coefficients}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   (Intercept)         AtBat          Hits         Walks         Years 
##    1.27429897   -0.05490834    2.18012455    2.29189433   -0.33767315 
##        CHmRun         CRuns          CRBI       LeagueN     DivisionW 
##    0.02822467    0.21627609    0.41713051   20.28190194 -116.16524424 
##       PutOuts        Errors 
##    0.23751978   -0.85604181
\end{verbatim}

Practice questions:

\begin{itemize}
\tightlist
\item
  How do ridge regression and the lasso improve on simple least squares?
\item
  In what cases would you expect ridge regression outperform the lasso, and vice versa?
\end{itemize}

\hypertarget{references}{%
\section{References}\label{references}}

These examples of the Ridge and Lasso are an adaptation of p.~251-255 of ``Introduction to
Statistical Learning with Applications in R'' by Gareth James, Daniela Witten, Trevor Hastie and Robert
Tibshirani. Adapted by R. Jordan Crouser at Smith College for SDS293: Machine Learning (Spring 2016), and re-implemented in Fall 2016 in \texttt{tidyverse} format by Amelia McNamara and R. Jordan Crouser at Smith College.

Used with permission from Jordan Crouser at Smith College. Additional Thanks to the following contributors on github:

\begin{itemize}
\tightlist
\item
  github.com/jcrouser
\item
  github.com/AmeliaMN
\item
  github.com/mhusseinmidd
\item
  github.com/rudeboybert
\item
  github.com/ijlyttle
\end{itemize}

\hypertarget{tree-based-models}{%
\chapter{Tree-based models}\label{tree-based-models}}

\hypertarget{decision-trees}{%
\section{Decision Trees}\label{decision-trees}}

\hypertarget{model-form-1}{%
\subsection{Model form}\label{model-form-1}}

Decision trees can be used for either classification or regression problems. The model structure is a series of yes/no questions. Depending on how each observation answers these questions, a prediction is made.

The below example shows how a single tree can predict health claims.

\begin{itemize}
\tightlist
\item
  For non-smokers, the predicted annual claims are 8,434. This represents 80\% of the observations
\item
  For smokers with a \texttt{bmi} of less than 30, the predicted annual claims are 21,000. 10\% of patients fall into this bucket.
\item
  For smokers with a \texttt{bmi} of more than 30, the prediction is 42,000. This bucket accounts for 11\% of patients.
\end{itemize}

\includegraphics{06-tree-based-models_files/figure-latex/unnamed-chunk-2-1.pdf}

We can cut the data set up into these groups and look at the claim costs. From this grouping, we can see that \texttt{smoker} is the most important variable as the difference in average claims is about 20,000.

\begin{longtable}[]{@{}lllr@{}}
\toprule
smoker & bmi\_30 & mean\_claims & percent\tabularnewline
\midrule
\endhead
no & bmi \textless{} 30 & \$7,977.03 & 0.38\tabularnewline
no & bmi \textgreater= 30 & \$8,842.69 & 0.42\tabularnewline
yes & bmi \textless{} 30 & \$21,363.22 & 0.10\tabularnewline
yes & bmi \textgreater= 30 & \$41,557.99 & 0.11\tabularnewline
\bottomrule
\end{longtable}

This was a very simple example because there were only two variables. If we have more variables, the tree will get large very quickly. This will result in overfitting; there will be good performance on the training data but poor performance on the test data.

The step-by-step process of building a tree is

\textbf{Step 1: Choose a variable at random.}

This could be any variable in \texttt{age}, \texttt{children}, \texttt{charges}, \texttt{sex}, \texttt{smoker}, \texttt{age\_bucket}, \texttt{bmi}, or \texttt{region}.

\textbf{Step 2: Find the split point which best seperates observations out based on the value of \(y\). A good split is one where the \(y\)'s are very different. * }

In this case, \texttt{smoker} was chosen. Then we can only split this in one way: \texttt{smoker\ =\ 1} or \texttt{smoker\ =\ 0}.

Then for each of these groups, smokers and non-smokers, choose another variable at random. In this case, for no-smokers, \texttt{age} was chosen. To find the best cut point of \texttt{age}, look at all possible age cut points from 18, 19, 20, 21, \ldots, 64 and choose the one which best separates the data.

There are three ways of deciding where to split

\begin{itemize}
\tightlist
\item
  \emph{Entropy} (aka, information gain)
\item
  \emph{Gini}
\item
  \emph{Classification error}
\end{itemize}

Of these, only the first two are commonly used. The exam is not going to ask you to calculate either of these. Just know that neither method will work better on all data sets, and so the best practice is to test both and compare the performance.

\textbf{Step 3: Continue doing this until a stopping criteria is reached. For example, the minimum number of observations is 5 or less.}

As you can see, this results in a very deep tree.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tree <-}\StringTok{ }\KeywordTok{rpart}\NormalTok{(}\DataTypeTok{formula =}\NormalTok{ charges }\OperatorTok{~}\StringTok{  }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ health_insurance,}
              \DataTypeTok{control =} \KeywordTok{rpart.control}\NormalTok{(}\DataTypeTok{cp =} \FloatTok{0.003}\NormalTok{))}
\KeywordTok{rpart.plot}\NormalTok{(tree, }\DataTypeTok{type =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{06-tree-based-models_files/figure-latex/unnamed-chunk-4-1.pdf}

\textbf{Step 4: Apply cost comlexity pruning to simplify the tree}

Intuitively, we know that the above model would perform poorly due to overfitting. We want to make it simpler by removing nodes. This is very similar to how in linear models we reduce complexity by reducing the number of coefficients.

A measure of the depth of the tree is the \emph{complexity}. A simple way of measuring this from the number of terminal nodes, called \(|T|\). This is similar to the ``degrees of freedom'' in a linear model. In the above example, \(|T| = 8\). The amount of penalization is controlled by \(\alpha\). This is very similar to \(\lambda\) in the Lasso.

Intuitively, merely only looking at the number of nodes by itself is too simple because not all data sets will have the same characteristics such as \(n\), \(p\), the number of categorical variables, correlations between variables, and so fourth. In addition, if we just looked at the error (squared error in this case) we would overfit very easily. To address this issue, we use a cost function which takes into account the error as well as \(|T|\).

To calculate the cost of a tree, number the terminal nodes from \(1\) to \(|T|\), and let the set of observations that fall into the \(mth\) bucket be \(R_m\). Then add up the squared error over all terminal nodes to the penalty term.

\[
\text{Cost}_\alpha(T) = \sum_{m=1}^{|T|} \sum_{R_m}(y_i - \hat{y}_{Rm})^2 + \alpha |T|
\]

\textbf{Step 5: Use cross-validation to select the best alpha}

The cost is controlled by the \texttt{CP} parameter. In the above example, did you notice the line \texttt{rpart.control(cp\ =\ 0.003)}? This is telling \texttt{rpart} to continue growing the tree until the CP reaches 0.003. At each subtree, we can measure the cost \texttt{CP} as well as the cross-validation error \texttt{xerror}.

This is stored in the \texttt{cptable}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tree <-}\StringTok{ }\KeywordTok{rpart}\NormalTok{(}\DataTypeTok{formula =}\NormalTok{ charges }\OperatorTok{~}\StringTok{  }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ health_insurance,}
              \DataTypeTok{control =} \KeywordTok{rpart.control}\NormalTok{(}\DataTypeTok{cp =} \FloatTok{0.0001}\NormalTok{))}
\NormalTok{cost <-}\StringTok{ }\NormalTok{tree}\OperatorTok{$}\NormalTok{cptable }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{as_tibble}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(nsplit, CP, xerror) }

\NormalTok{cost }\OperatorTok{%>%}\StringTok{ }\KeywordTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 3
##   nsplit      CP xerror
##    <dbl>   <dbl>  <dbl>
## 1      0 0.620    1.00 
## 2      1 0.144    0.383
## 3      2 0.0637   0.240
## 4      3 0.00967  0.178
## 5      4 0.00784  0.175
## 6      5 0.00712  0.169
\end{verbatim}

As more splits are added, the cost continues to decrease, reaches a minimum, and then begins to increase.

\includegraphics{06-tree-based-models_files/figure-latex/unnamed-chunk-6-1.pdf}

To optimize performance, choose the number of splits which has the lowest error. Often, though, the goal of using a decision tree is to create a simple model. In this case, we can err or the side of a lower \texttt{nsplit} so that the tree is shorter and more interpretable. All of the questions on so far have only used decision trees for interpretability, and a different model method has been used when predictive power is needed.

Once we have selected \(\alpha\), the tree is pruned. Sometimes the CP with the lowest error has a large number of splits, such as the case is here.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tree}\OperatorTok{$}\NormalTok{cptable }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{as_tibble}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(nsplit, CP, xerror) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{arrange}\NormalTok{(xerror) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 3
##   nsplit       CP xerror
##    <dbl>    <dbl>  <dbl>
## 1     18 0.000910  0.149
## 2     19 0.000837  0.150
## 3     25 0.000681  0.150
## 4     17 0.000913  0.150
## 5     22 0.000759  0.151
## 6     35 0.000497  0.151
\end{verbatim}

The SOA will give you code to find the lowest CP value such as below. This may or may not be useful depending on if they are asking for predictive performance or interpretability.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pruned_tree <-}\StringTok{ }\KeywordTok{prune}\NormalTok{(tree,}
                     \DataTypeTok{cp =}\NormalTok{ tree}\OperatorTok{$}\NormalTok{cptable[}\KeywordTok{which.min}\NormalTok{(tree}\OperatorTok{$}\NormalTok{cptable[, }\StringTok{"xerror"}\NormalTok{]), }\StringTok{"CP"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

To make a simple tree, there are a few options

\begin{itemize}
\tightlist
\item
  Set the maximum depth of a tree with \texttt{maxdepth}
\item
  Manually set \texttt{cp} to be higher
\item
  Use fewer input variables and avoid categories with many levels
\item
  Force a high number of minimum observations per terminal node with \texttt{minbucket}
\end{itemize}

For instance, using these suggestions allows for a simpler tree to be fit.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(caret)}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}
\NormalTok{index <-}\StringTok{ }\KeywordTok{createDataPartition}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ health_insurance}\OperatorTok{$}\NormalTok{charges, }
                             \DataTypeTok{p =} \FloatTok{0.8}\NormalTok{, }\DataTypeTok{list =}\NormalTok{ F)}
\NormalTok{train <-}\StringTok{ }\NormalTok{health_insurance }\OperatorTok{%>%}\StringTok{ }\KeywordTok{slice}\NormalTok{(index)}
\NormalTok{test <-}\StringTok{ }\NormalTok{health_insurance }\OperatorTok{%>%}\StringTok{ }\KeywordTok{slice}\NormalTok{(}\OperatorTok{-}\NormalTok{index)}

\NormalTok{simple_tree <-}\StringTok{ }\KeywordTok{rpart}\NormalTok{(}\DataTypeTok{formula =}\NormalTok{ charges }\OperatorTok{~}\StringTok{  }\NormalTok{., }
              \DataTypeTok{data =}\NormalTok{ train,}
              \DataTypeTok{control =} \KeywordTok{rpart.control}\NormalTok{(}\DataTypeTok{cp =} \FloatTok{0.0001}\NormalTok{, }
                                      \DataTypeTok{minbucket =} \DecValTok{200}\NormalTok{,}
                                      \DataTypeTok{maxdepth =} \DecValTok{10}\NormalTok{))}
\KeywordTok{rpart.plot}\NormalTok{(simple_tree, }\DataTypeTok{type =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{06-tree-based-models_files/figure-latex/unnamed-chunk-9-1.pdf}

We evaluate the performance on the test set. Because the target variable \texttt{charges} is highly skewed, we use the Root Mean Squared Log Error (RMSLE). We see that the complex tree has the best (lowest) error, but also has 8 terminal nodes. The simple tree with only three terminal nodes has worse (higher) error, but this is still an improvement over the mean prediction.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tree_pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(tree, test)}
\NormalTok{simple_tree_pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(simple_tree, test)}

\NormalTok{get_rmsle <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(y, y_hat)\{}
  \KeywordTok{sqrt}\NormalTok{(}\KeywordTok{mean}\NormalTok{((}\KeywordTok{log}\NormalTok{(y) }\OperatorTok{-}\StringTok{ }\KeywordTok{log}\NormalTok{(y_hat))}\OperatorTok{^}\DecValTok{2}\NormalTok{))}
\NormalTok{\}}

\KeywordTok{get_rmsle}\NormalTok{(test}\OperatorTok{$}\NormalTok{charges, tree_pred)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.3920546
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{get_rmsle}\NormalTok{(test}\OperatorTok{$}\NormalTok{charges, simple_tree_pred)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5678457
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{get_rmsle}\NormalTok{(test}\OperatorTok{$}\NormalTok{charges, }\KeywordTok{mean}\NormalTok{(train}\OperatorTok{$}\NormalTok{charges))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.9996513
\end{verbatim}

\hypertarget{advantages-and-disadvantages-2}{%
\subsection{Advantages and disadvantages}\label{advantages-and-disadvantages-2}}

\textbf{Advantages}

\begin{itemize}
\tightlist
\item
  Easy to interpret
\item
  Captures interaction effects
\item
  Captures non-linearities
\item
  Handles continuous and categorical data
\item
  Handles missing values
\end{itemize}

\textbf{Disadvantages}

\begin{itemize}
\tightlist
\item
  Is a ``weak learner'' because of low predictive power
\item
  Does not work on small data sets
\item
  Is often a simplification of the underlying process because all observations at terminal nodes have equal predicted values
\item
  Is biased towards selecting high-cardinality features because more possible split points for these features tend to lead to overfitting
\item
  High variance (which can be alleviated with stricter parameters) leads the ``easy to interpret results'' to change upon retraining
  Unable to predict beyond the range of the training data for regression (because each predicted value is an average of training samples)
\end{itemize}

\begin{longtable}[]{@{}ll@{}}
\toprule
Readings &\tabularnewline
\midrule
\endhead
ISLR 8.1.1 Basics of Decision Trees &\tabularnewline
ISLR 8.1.2 Classification Trees &\tabularnewline
\href{https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf}{rpart Documentation (Optional)} &\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{ensemble-learning}{%
\section{Ensemble learning}\label{ensemble-learning}}

The ``wisdom of crowds'' says that often many are smater than the few. In the context of modeling, the models which we have looked at so far have been single guesses; however, often the underlying process is more complex than any single model can explain. If we build separate models and then combine them, known as \emph{ensembling}, performance can be improved. Instead of trying to create a single perfect model, many simple models, known as \emph{weak learners} are combined into a \emph{meta-model}.

The two main ways that models are combined are through \emph{bagging} and \emph{boosting}.

\hypertarget{bagging}{%
\subsection{Bagging}\label{bagging}}

To start, we create many ``copies'' of the training data by sampling with replacement. Then we fit a simple model, typically a decision tree or linear model, to each of the data sets. Because each model is looking at different areas of the data, the predictions are different. The final model is a weighted average of each of the individual models.

\hypertarget{boosting}{%
\subsection{Boosting}\label{boosting}}

Boosting always uses the original training data and iteratively fits models to the error of the prior models. These weak learners are ineffective by themselves but powerful when added together. Unlike with bagging, the computer must train these weak learners \emph{sequentially} instead of in parallel.

\hypertarget{random-forests}{%
\section{Random Forests}\label{random-forests}}

\hypertarget{model-form-2}{%
\subsection{Model form}\label{model-form-2}}

A random forest is the most common example of bagging. As the name implies, a forest is made up of \emph{trees}. Seperate trees are fit to sampled datasets. For random forests, there is one minor modification: in order to make each model even more different, each tree selects a \emph{random subset of variables}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Assume that the underlying process, \(Y\), has some signal within the data \(\mathbf{X}\).
\item
  Introduce randomness (variance) to capture the signal.
\item
  Remove the variance by taking an average.
\end{enumerate}

When using only a single tree, there can only be as many predictions as there are terminal nodes. In a random forest, predictions can be more granular due to the contribution of each of the trees.

The below graph illustrates this. A single tree (left) has stair-like, step-wise predictions whereas a random forest is free to predict any value. The color represents the predicted value (yellow = highest, black = lowest).

\includegraphics{06-tree-based-models_files/figure-latex/unnamed-chunk-11-1.pdf}

Unlike decision trees, random forest trees do not need to be pruned. This is because overfitting is less of a problem: if one tree overfits, there are other trees which overfit in other areas to compensate.

In most applications, only the \texttt{mtry} parameter, which controls how many variables to consider at each split, needs to be tuned. Tuning the \texttt{ntrees} parameter is not required; however, the soa may still ask you to.

\hypertarget{example-3}{%
\subsection{Example}\label{example-3}}

Using the basic \texttt{randomForest} package we fit a model with 500 trees.

This expects only numeric values. We create dummy (indicator) columns.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rf_data <-}\StringTok{ }\NormalTok{health_insurance }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{sex =} \KeywordTok{ifelse}\NormalTok{(sex }\OperatorTok{==}\StringTok{ "male"}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{),}
         \DataTypeTok{smoker =} \KeywordTok{ifelse}\NormalTok{(smoker }\OperatorTok{==}\StringTok{ "yes"}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{),}
         \DataTypeTok{region_ne =} \KeywordTok{ifelse}\NormalTok{(region }\OperatorTok{==}\StringTok{ "northeast"}\NormalTok{, }\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{),}
         \DataTypeTok{region_nw =} \KeywordTok{ifelse}\NormalTok{(region }\OperatorTok{==}\StringTok{ "northwest"}\NormalTok{, }\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{),}
         \DataTypeTok{region_se =} \KeywordTok{ifelse}\NormalTok{(region }\OperatorTok{==}\StringTok{ "southeast"}\NormalTok{, }\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{),}
         \DataTypeTok{region_sw =} \KeywordTok{ifelse}\NormalTok{(region }\OperatorTok{==}\StringTok{ "southwest"}\NormalTok{, }\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{region)}
\NormalTok{rf_data }\OperatorTok{%>%}\StringTok{ }\KeywordTok{glimpse}\NormalTok{(}\DecValTok{50}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Observations: 1,338
## Variables: 10
## $ age       <dbl> 19, 18, 28, 33, 32, 31, 46,...
## $ sex       <dbl> 0, 1, 1, 1, 1, 0, 0, 0, 1, ...
## $ bmi       <dbl> 27.900, 33.770, 33.000, 22....
## $ children  <dbl> 0, 1, 3, 0, 0, 0, 1, 3, 2, ...
## $ smoker    <dbl> 1, 0, 0, 0, 0, 0, 0, 0, 0, ...
## $ charges   <dbl> 16884.924, 1725.552, 4449.4...
## $ region_ne <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 1, ...
## $ region_nw <dbl> 0, 0, 0, 1, 1, 0, 0, 1, 0, ...
## $ region_se <dbl> 0, 1, 1, 0, 0, 1, 1, 0, 0, ...
## $ region_sw <dbl> 1, 0, 0, 0, 0, 0, 0, 0, 0, ...
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(caret)}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}
\NormalTok{index <-}\StringTok{ }\KeywordTok{createDataPartition}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ rf_data}\OperatorTok{$}\NormalTok{charges, }
                             \DataTypeTok{p =} \FloatTok{0.8}\NormalTok{, }\DataTypeTok{list =}\NormalTok{ F)}
\NormalTok{train <-}\StringTok{ }\NormalTok{rf_data }\OperatorTok{%>%}\StringTok{ }\KeywordTok{slice}\NormalTok{(index)}
\NormalTok{test <-}\StringTok{ }\NormalTok{rf_data }\OperatorTok{%>%}\StringTok{ }\KeywordTok{slice}\NormalTok{(}\OperatorTok{-}\NormalTok{index)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rf <-}\StringTok{ }\KeywordTok{randomForest}\NormalTok{(charges }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ train, }\DataTypeTok{ntree =} \DecValTok{500}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(rf)}
\end{Highlighting}
\end{Shaded}

\includegraphics{06-tree-based-models_files/figure-latex/unnamed-chunk-14-1.pdf}

We again use RMSLE. This is lower (better) than a model that uses the average as a baseline.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(rf, test)}
\NormalTok{get_rmsle <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(y, y_hat)\{}
  \KeywordTok{sqrt}\NormalTok{(}\KeywordTok{mean}\NormalTok{((}\KeywordTok{log}\NormalTok{(y) }\OperatorTok{-}\StringTok{ }\KeywordTok{log}\NormalTok{(y_hat))}\OperatorTok{^}\DecValTok{2}\NormalTok{))}
\NormalTok{\}}

\KeywordTok{get_rmsle}\NormalTok{(test}\OperatorTok{$}\NormalTok{charges, pred)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.4772576
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{get_rmsle}\NormalTok{(test}\OperatorTok{$}\NormalTok{charges, }\KeywordTok{mean}\NormalTok{(train}\OperatorTok{$}\NormalTok{charges))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.9996513
\end{verbatim}

\hypertarget{variable-importance}{%
\subsection{Variable Importance}\label{variable-importance}}

\emph{Variable importance} is a way of measuring how each variable contributes to overall model. For single decision trees, if a variable was ``higher'' up in the tree, then this variable would have greater influence. Statistically, there are two ways of measuring this:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  Look at the reduction in error when a the variable is randomly permuted verses using the actual values. This is done with \texttt{type\ =\ 1}.
\item
  Use the total decrease in node impurities from splitting on the variable, averaged over all trees. For classification, the node impurity is measured by the Gini index; for regression, it is measured by the residual sum of squares \(\text{RSS}\). This is \texttt{type\ =\ 2}.
\end{enumerate}

\texttt{smoker}, \texttt{bmi}, and \texttt{age} are the most importance predictors of charges. As you can imagine, variable importance is a highly useful tool for building models. We could use this to test out newly engineered features, or perform feature selection by taking the top-n features and use them in a different model. Random forests can handle very high dimensional data which allows for many tests to be run at once.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{varImpPlot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ rf)}
\end{Highlighting}
\end{Shaded}

\includegraphics{06-tree-based-models_files/figure-latex/unnamed-chunk-16-1.pdf}

\hypertarget{partial-dependence}{%
\subsection{Partial dependence}\label{partial-dependence}}

We know which variables are important, but what about the direction of the change? In a linear model we would be able to just look at the sign of the coefficient. In tree-based models, we have a tool called \emph{partial dependence}. This attempts to measure the change in the predicted value by taking the average \(\hat{\mathbf{y}}\) after removing the effects of all other predictors.

Although this is commonly used for trees, this approach is model-agnostic in that any model could be used.

Take a model of two predictors, \(\hat{\mathbf{y}} = f(\mathbf{X}_1, \mathbf{X_2})\). For simplicity, say that \(f(x_1, x_2) = 2x_1 + 3x_2\).

The data looks like this

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{x1 =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{), }\DataTypeTok{x2 =} \KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{6}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{f =} \DecValTok{2}\OperatorTok{*}\NormalTok{x1 }\OperatorTok{+}\StringTok{ }\DecValTok{3}\OperatorTok{*}\NormalTok{x2)}
\NormalTok{df}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 4 x 3
##      x1    x2     f
##   <dbl> <dbl> <dbl>
## 1     1     3    11
## 2     1     4    14
## 3     2     5    19
## 4     2     6    22
\end{verbatim}

Here is the partial dependence of \texttt{x1} on to \texttt{f}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df }\OperatorTok{%>%}\StringTok{ }\KeywordTok{group_by}\NormalTok{(x1) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{f =} \KeywordTok{mean}\NormalTok{(f))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 2
##      x1     f
##   <dbl> <dbl>
## 1     1  12.5
## 2     2  20.5
\end{verbatim}

This method of using the mean is know as the \emph{Monte Carlo} method. There are other methods for partial dependence that are not on the syllabus.

For the RandomForest, this is done with \texttt{pdp::partial()}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(pdp)}
\NormalTok{bmi <-}\StringTok{ }\NormalTok{pdp}\OperatorTok{::}\KeywordTok{partial}\NormalTok{(rf, }\DataTypeTok{pred.var =} \StringTok{"bmi"}\NormalTok{, }
                    \DataTypeTok{grid.resolution =} \DecValTok{20}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{autoplot}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_bw}\NormalTok{()}
\NormalTok{age <-}\StringTok{ }\NormalTok{pdp}\OperatorTok{::}\KeywordTok{partial}\NormalTok{(rf, }\DataTypeTok{pred.var =} \StringTok{"age"}\NormalTok{, }
                    \DataTypeTok{grid.resolution =} \DecValTok{20}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{autoplot}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_bw}\NormalTok{()}

\KeywordTok{ggarrange}\NormalTok{(bmi, age)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{06-tree-based-models_files/figure-latex/unnamed-chunk-19-1.pdf}
\caption{\label{fig:unnamed-chunk-19}Partial Dependence}
\end{figure}

\hypertarget{advantages-and-disadvantages-3}{%
\subsection{Advantages and disadvantages}\label{advantages-and-disadvantages-3}}

\textbf{Advantages}

\begin{itemize}
\tightlist
\item
  Resilient to overfitting due to bagging
\item
  Only one parameter to tune (mtry, the number of features considered at each split)
\item
  Very good a multi-class prediction
\item
  Nonlinearities
\item
  Interaction effects
\item
  Handles missing data
\item
  Deals with unbalanced after over/undersampling
\end{itemize}

\textbf{Disadvantages}

\begin{itemize}
\tightlist
\item
  Does not work on small data sets
\item
  Weaker performance than other methods (GBM, NN)
\item
  Unable to predict beyond training data for regression
\end{itemize}

\begin{longtable}[]{@{}ll@{}}
\toprule
Readings &\tabularnewline
\midrule
\endhead
ISLR 8.2.1 Bagging &\tabularnewline
ISLR 8.1.2 Random Forests &\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{gradient-boosted-trees}{%
\section{Gradient Boosted Trees}\label{gradient-boosted-trees}}

Another ensemble learning method is \emph{gradient boosting}, also known as the Gradient Boosted Machine (GBM). Although this is unlikely to get significant attention on the PA exam due to the complexity, this is the most widely-used and powerful machine learning algorithms that are in use today.

We start with an initial model, which is just a constant prediction of the mean.

\[f = f_0(\mathbf{x_i}) = \frac{1}{n}\sum_{i=1}^ny_i\]

Then we update the target (what the model is predicting) by subtracting off the previously predicted value.

\[ \hat{y_i} \leftarrow y_i - f_0(\mathbf{x_i})\]

This \(\hat{y_i}\) is called the \emph{residual}. In our example, instead of predicting \texttt{charges}, this would be predicting the residual of \(\text{charges}_i - \text{Mean}(\text{charges})\). We now use this model for the residuals to update the prediction.

If we updated each prediction with the prior residual directly, the algorithm would be unstable. To make this process more gradual, we use a \emph{learning rate} parameter.

At step 2, we have

\[f = f_0 + \alpha f_1\]

Then we go back and fit another weak learner to this residual and repeat.

\[f = f_0 + \alpha f_1 + \alpha f_2\]

We then iterate through this process hundreds or thousands of times, slowly improving the prediction.

Because each new tree is fit to \emph{residuals} instead of the response itself, the process continuously improves the prediction. As the prediction improves, the residuals get smaller and smaller. In random forests, or other bagging algorithms, the model performance is more limited by the individual trees because each only contributes to the overall average. The name is \emph{gradient boosting} because the residuals are an approximation of the gradient, and gradient descent is how the loss functions are optimized.

Similarly to how GLMs can be used for classification problems through a logit transform (aka logistic regression), GBMs can also be used for classification.

\hypertarget{parameters}{%
\subsection{Parameters}\label{parameters}}

For random forests, the individual tree parameters do not get tuned. For GBMs, however, these parameters can make a significant difference in model performance.

\textbf{Boosting parameters:}

\begin{itemize}
\item
  \texttt{n.trees}: Integer specifying the total number of trees to fit. This is equivalent to the number of iterations and the number of basis functions in the additive expansion. Default is 100.
\item
  \texttt{shrinkage}: a shrinkage parameter applied to each tree in the expansion. Also known as the learning rate or step-size reduction; 0.001 to 0.1 usually work, but a smaller learning rate typically requires more trees. Default is 0.1.
\end{itemize}

\textbf{Tree parameters:}

\begin{itemize}
\item
  \texttt{interaction.depth}: Integer specifying the maximum depth of each tree (i.e., the highest level of variable interactions allowed). A value of 1 implies an additive model, a value of 2 implies a model with up to 2-way interactions, etc. Default is 1.
\item
  \texttt{n.minobsinnode}: Integer specifying the minimum number of observations in the terminal nodes of the trees. Note that this is the actual number of observations, not the total weight.
\end{itemize}

GBMs are easy to overfit, and the parameters need to be carefully tuned using cross-validation. In the Examples section we go through how to do this.

\begin{quote}
\textbf{Tip:} Whenever fitting a model, use \texttt{?model\_name} to get the documentation. The parameters below are from \texttt{?gbm}.
\end{quote}

\hypertarget{example-4}{%
\subsection{Example}\label{example-4}}

We fit a gbm below without tuning the parameters for the sake of example.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(gbm)}
\NormalTok{gbm <-}\StringTok{ }\KeywordTok{gbm}\NormalTok{(charges }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ train,}
           \DataTypeTok{n.trees =} \DecValTok{100}\NormalTok{,}
           \DataTypeTok{interaction.depth =} \DecValTok{2}\NormalTok{,}
           \DataTypeTok{n.minobsinnode =} \DecValTok{50}\NormalTok{,}
           \DataTypeTok{shrinkage =} \FloatTok{0.1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Distribution not specified, assuming gaussian ...
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(gbm, test, }\DataTypeTok{n.trees =} \DecValTok{100}\NormalTok{)}

\KeywordTok{get_rmsle}\NormalTok{(test}\OperatorTok{$}\NormalTok{charges, pred)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.4411494
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{get_rmsle}\NormalTok{(test}\OperatorTok{$}\NormalTok{charges, }\KeywordTok{mean}\NormalTok{(train}\OperatorTok{$}\NormalTok{charges))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.9996513
\end{verbatim}

\hypertarget{advantages-and-disadvantages-4}{%
\subsection{Advantages and disadvantages}\label{advantages-and-disadvantages-4}}

This exam covers the basics of GBMs. There are many variations of GBMs not covered in detail such as \texttt{xgboost}.

\textbf{Advantages}

\begin{itemize}
\tightlist
\item
  High prediction accuracy
\item
  Shown to work empirically well on many types of problems
\item
  Nonlinearities, interaction effects, resilient to outliers, corrects for missing values
\item
  Deals with class imbalance directly by weighting observations
\end{itemize}

\textbf{Disadvantages}

\begin{itemize}
\tightlist
\item
  Requires large sample size
\item
  Longer training time
\item
  Does not detect linear combinations of features. These must be engineered
  Can overfit if not tuned correctly
\end{itemize}

\begin{longtable}[]{@{}ll@{}}
\toprule
Readings &\tabularnewline
\midrule
\endhead
ISLR 8.2.3 Boosting &\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{exercises-1}{%
\section{Exercises}\label{exercises-1}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ExamPAData)}
\KeywordTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

Run this code on your computer to answer these exercises.

\hypertarget{rf-with-randomforest}{%
\subsection{\texorpdfstring{1. RF with \texttt{randomForest}}{1. RF with randomForest}}\label{rf-with-randomforest}}

(Part 1 of 2)

The below code is set up to fit a random forest to the \texttt{soa\_mortality} data set to predict \texttt{actual\_cnt}.

There is a problem: all of the predictions are coming out to be 1. Find out why this is happening and fix it.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}
\CommentTok{#For the sake of this example, only take 20% of the records}
\NormalTok{df <-}\StringTok{ }\NormalTok{soa_mortality }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{sample_frac}\NormalTok{(}\FloatTok{0.2}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{target =} \KeywordTok{as.factor}\NormalTok{(}\KeywordTok{ifelse}\NormalTok{(actual_cnt }\OperatorTok{==}\StringTok{ }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{))) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(target, prodcat, distchan, smoker, sex, issage, uwkey) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate_if}\NormalTok{(is.character, }\OperatorTok{~}\KeywordTok{as.factor}\NormalTok{(.x))}

\CommentTok{#check that the target has 0's and 1's}
\NormalTok{df }\OperatorTok{%>%}\StringTok{ }\KeywordTok{count}\NormalTok{(target)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(caret)}
\KeywordTok{library}\NormalTok{(randomForest)}
\NormalTok{index <-}\StringTok{ }\KeywordTok{createDataPartition}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ df}\OperatorTok{$}\NormalTok{target, }\DataTypeTok{p =} \FloatTok{0.8}\NormalTok{, }\DataTypeTok{list =}\NormalTok{ F)}

\NormalTok{train <-}\StringTok{ }\NormalTok{df }\OperatorTok{%>%}\StringTok{ }\KeywordTok{slice}\NormalTok{(index)}
\NormalTok{test <-}\StringTok{ }\NormalTok{df }\OperatorTok{%>%}\StringTok{ }\KeywordTok{slice}\NormalTok{(}\OperatorTok{-}\NormalTok{index)}

\NormalTok{k =}\StringTok{ }\FloatTok{0.5}
\NormalTok{cutoff=}\KeywordTok{c}\NormalTok{(k,}\DecValTok{1}\OperatorTok{-}\NormalTok{k) }

\NormalTok{model <-}\StringTok{ }\KeywordTok{randomForest}\NormalTok{(}
  \DataTypeTok{formula =}\NormalTok{ target }\OperatorTok{~}\StringTok{ }\NormalTok{., }
  \DataTypeTok{data =}\NormalTok{ train,}
  \DataTypeTok{ntree =} \DecValTok{100}\NormalTok{,}
  \DataTypeTok{cutoff =}\NormalTok{ cutoff}
\NormalTok{  )}

\NormalTok{pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(model, test)}
\KeywordTok{confusionMatrix}\NormalTok{(pred, test}\OperatorTok{$}\NormalTok{target)}
\end{Highlighting}
\end{Shaded}

(Part 2 of 2)

Downsample the majority class and refit the model, and then choose between the original data and the downsampled data based on the model performance. Use your own judgement when choosing how to evaluate the model based on accuracy, sensitivity, specificity, and Kappa.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{down_train <-}\StringTok{ }\KeywordTok{downSample}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ train }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{target),}
                         \DataTypeTok{y =}\NormalTok{ train}\OperatorTok{$}\NormalTok{target)}

\NormalTok{down_test <-}\StringTok{ }\KeywordTok{downSample}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ test }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{target),}
                         \DataTypeTok{y =}\NormalTok{ test}\OperatorTok{$}\NormalTok{target)}

\NormalTok{down_train }\OperatorTok{%>%}\StringTok{ }\KeywordTok{count}\NormalTok{(Class)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model <-}\StringTok{ }\KeywordTok{randomForest}\NormalTok{(}
  \DataTypeTok{formula =}\NormalTok{ Class }\OperatorTok{~}\StringTok{ }\NormalTok{., }
  \DataTypeTok{data =}\NormalTok{ down_train,}
  \DataTypeTok{ntree =} \DecValTok{100}\NormalTok{,}
  \DataTypeTok{cutoff =}\NormalTok{ cutoff}
\NormalTok{  )}

\NormalTok{down_pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(model, down_test)}
\KeywordTok{confusionMatrix}\NormalTok{(down_pred, down_test}\OperatorTok{$}\NormalTok{Class)}
\end{Highlighting}
\end{Shaded}

Now up-sample the minority class and repeat the same procedure.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{up_train <-}\StringTok{ }\KeywordTok{upSample}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ train }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{target),}
                         \DataTypeTok{y =}\NormalTok{ train}\OperatorTok{$}\NormalTok{target)}

\NormalTok{up_test <-}\StringTok{ }\KeywordTok{upSample}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ test }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{target),}
                         \DataTypeTok{y =}\NormalTok{ test}\OperatorTok{$}\NormalTok{target)}

\NormalTok{up_train }\OperatorTok{%>%}\StringTok{ }\KeywordTok{count}\NormalTok{(Class)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model <-}\StringTok{ }\KeywordTok{randomForest}\NormalTok{(}
  \DataTypeTok{formula =}\NormalTok{ Class }\OperatorTok{~}\StringTok{ }\NormalTok{., }
  \DataTypeTok{data =}\NormalTok{ up_train,}
  \DataTypeTok{ntree =} \DecValTok{100}\NormalTok{,}
  \DataTypeTok{cutoff =}\NormalTok{ cutoff}
\NormalTok{  )}

\NormalTok{up_pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(model, up_test)}
\KeywordTok{confusionMatrix}\NormalTok{(up_pred, up_test}\OperatorTok{$}\NormalTok{Class)}
\end{Highlighting}
\end{Shaded}

\hypertarget{rf-tuning-with-caret}{%
\subsection{\texorpdfstring{2. RF tuning with \texttt{caret}}{2. RF tuning with caret}}\label{rf-tuning-with-caret}}

The best practice of tuning a model is with cross-validation. This can only be done in the \texttt{caret} library. If the SOA asks you to use \texttt{caret}, they will likely ask you a question related to cross validation as below.

An actuary has trained a predictive model and chosen the best hyperparameters, cleaned the data, and performed feature engineering. They have one problem, however: the error on the training data is far lower than on new, unseen test data. Read the code below and determine their problem. Find a way to lower the error on the test data \emph{without changing the model or the data.} Explain the rational behind your method.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}
\CommentTok{#Take only 1000 records }
\CommentTok{#Uncomment this when completing this exercise}
\NormalTok{data <-}\StringTok{ }\NormalTok{health_insurance }\OperatorTok{%>%}\StringTok{ }\KeywordTok{sample_n}\NormalTok{(}\DecValTok{1000}\NormalTok{) }

\NormalTok{index <-}\StringTok{ }\KeywordTok{createDataPartition}\NormalTok{(}
  \DataTypeTok{y =}\NormalTok{ data}\OperatorTok{$}\NormalTok{charges, }\DataTypeTok{p =} \FloatTok{0.8}\NormalTok{, }\DataTypeTok{list =}\NormalTok{ F) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{as.numeric}\NormalTok{()}
\NormalTok{train <-}\StringTok{  }\NormalTok{health_insurance }\OperatorTok{%>%}\StringTok{ }\KeywordTok{slice}\NormalTok{(index)}
\NormalTok{test <-}\StringTok{ }\NormalTok{health_insurance }\OperatorTok{%>%}\StringTok{ }\KeywordTok{slice}\NormalTok{(}\OperatorTok{-}\NormalTok{index)}

\NormalTok{control <-}\StringTok{ }\KeywordTok{trainControl}\NormalTok{(}
  \DataTypeTok{method=}\StringTok{'boot'}\NormalTok{, }
  \DataTypeTok{number=}\DecValTok{2}\NormalTok{, }
  \DataTypeTok{p =} \FloatTok{0.2}\NormalTok{)}

\NormalTok{tunegrid <-}\StringTok{ }\KeywordTok{expand.grid}\NormalTok{(}\DataTypeTok{.mtry=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{5}\NormalTok{))}
\NormalTok{rf <-}\StringTok{ }\KeywordTok{train}\NormalTok{(charges }\OperatorTok{~}\StringTok{ }\NormalTok{.,}
            \DataTypeTok{data =}\NormalTok{ train,}
            \DataTypeTok{method=}\StringTok{'rf'}\NormalTok{, }
            \DataTypeTok{tuneGrid=}\NormalTok{tunegrid, }
            \DataTypeTok{trControl=}\NormalTok{control)}

\NormalTok{pred_train <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(rf, train)}
\NormalTok{pred_test <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(rf, test)}

\NormalTok{get_rmse <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(y, y_hat)\{}
  \KeywordTok{sqrt}\NormalTok{(}\KeywordTok{mean}\NormalTok{((y }\OperatorTok{-}\StringTok{ }\NormalTok{y_hat)}\OperatorTok{^}\DecValTok{2}\NormalTok{))}
\NormalTok{\}}

\KeywordTok{get_rmse}\NormalTok{(pred_train, train}\OperatorTok{$}\NormalTok{charges)}
\KeywordTok{get_rmse}\NormalTok{(pred_test, test}\OperatorTok{$}\NormalTok{charges)}
\end{Highlighting}
\end{Shaded}

\hypertarget{tuning-a-gbm-with-caret}{%
\subsection{\texorpdfstring{3. Tuning a GBM with \texttt{caret}}{3. Tuning a GBM with caret}}\label{tuning-a-gbm-with-caret}}

If the SOA asks you to tune a GBM, they will need to give you starting hyperparameters which are close to the ``best'' values due to how slow the Prometric computers are. Another possibility is that they pre-train a GBM model object and ask that you use it.

This example looks at 135 combinations of hyper parameters.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}
\NormalTok{index <-}\StringTok{ }\KeywordTok{createDataPartition}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ health_insurance}\OperatorTok{$}\NormalTok{charges, }
                             \DataTypeTok{p =} \FloatTok{0.8}\NormalTok{, }\DataTypeTok{list =}\NormalTok{ F)}
\NormalTok{train <-}\StringTok{ }\NormalTok{health_insurance }\OperatorTok{%>%}\StringTok{ }\KeywordTok{slice}\NormalTok{(index)}
\NormalTok{test <-}\StringTok{ }\NormalTok{health_insurance }\OperatorTok{%>%}\StringTok{ }\KeywordTok{slice}\NormalTok{(}\OperatorTok{-}\NormalTok{index)}

\NormalTok{tunegrid <-}\StringTok{ }\KeywordTok{expand.grid}\NormalTok{(}
    \DataTypeTok{interaction.depth =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{),}
    \DataTypeTok{n.trees =} \KeywordTok{c}\NormalTok{(}\DecValTok{100}\NormalTok{, }\DecValTok{200}\NormalTok{, }\DecValTok{300}\NormalTok{, }\DecValTok{400}\NormalTok{, }\DecValTok{500}\NormalTok{), }
    \DataTypeTok{shrinkage =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\FloatTok{0.0001}\NormalTok{),}
    \DataTypeTok{n.minobsinnode =} \KeywordTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{100}\NormalTok{)}
\NormalTok{    )}
\KeywordTok{nrow}\NormalTok{(tunegrid)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 135
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{control <-}\StringTok{ }\KeywordTok{trainControl}\NormalTok{(}
  \DataTypeTok{method=}\StringTok{'repeatedcv'}\NormalTok{, }
  \DataTypeTok{number=}\DecValTok{5}\NormalTok{, }
  \DataTypeTok{p =} \FloatTok{0.8}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gbm <-}\StringTok{ }\KeywordTok{train}\NormalTok{(charges }\OperatorTok{~}\StringTok{ }\NormalTok{.,}
            \DataTypeTok{data =}\NormalTok{ train,}
            \DataTypeTok{method=}\StringTok{'gbm'}\NormalTok{, }
            \DataTypeTok{tuneGrid=}\NormalTok{tunegrid, }
            \DataTypeTok{trControl=}\NormalTok{control,}
            \CommentTok{#Show detailed output}
            \DataTypeTok{verbose =} \OtherTok{FALSE}
\NormalTok{            )}
\end{Highlighting}
\end{Shaded}

The output shows the RMSE for each of the 135 models tested.

(Part 1 of 3)

Identify the hyperpameter combination that has the lowest training error.

(Part 2 of 3)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Suppose that the optimization measure was RMSE. The below table shows the results from three models. Explain why some sets of parameters have better RMSE than the others.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results <-}\StringTok{ }\NormalTok{gbm}\OperatorTok{$}\NormalTok{results }\OperatorTok{%>%}\StringTok{ }\KeywordTok{arrange}\NormalTok{(RMSE)}
\NormalTok{top_result <-}\StringTok{ }\NormalTok{results }\OperatorTok{%>%}\StringTok{ }\KeywordTok{slice}\NormalTok{(}\DecValTok{1}\NormalTok{)}\OperatorTok{%>%}\StringTok{ }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{param_rank =} \DecValTok{1}\NormalTok{)}
\NormalTok{tenth_result <-}\StringTok{ }\NormalTok{results }\OperatorTok{%>%}\StringTok{ }\KeywordTok{slice}\NormalTok{(}\DecValTok{10}\NormalTok{)}\OperatorTok{%>%}\StringTok{ }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{param_rank =} \DecValTok{10}\NormalTok{)}
\NormalTok{twenty_seventh_result <-}\StringTok{ }\NormalTok{results }\OperatorTok{%>%}\StringTok{ }\KeywordTok{slice}\NormalTok{(}\DecValTok{135}\NormalTok{)}\OperatorTok{%>%}\StringTok{ }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{param_rank =} \DecValTok{135}\NormalTok{)}

\KeywordTok{rbind}\NormalTok{(top_result, tenth_result, twenty_seventh_result) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(param_rank, }\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   param_rank shrinkage interaction.depth n.minobsinnode n.trees      RMSE
## 1          1     1e-01                 5             30     100  4396.814
## 2         10     1e-01                10             30     300  4630.433
## 3        135     1e-04                 1            100     100 12108.185
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  The partial dependence of \texttt{bmi} onto \texttt{charges} makes it appear as if \texttt{charges} increases monotonically as \texttt{bmi} increases.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pdp}\OperatorTok{::}\KeywordTok{partial}\NormalTok{(gbm, }\DataTypeTok{pred.var =} \StringTok{"bmi"}\NormalTok{, }\DataTypeTok{grid.resolution =} \DecValTok{15}\NormalTok{, }\DataTypeTok{plot =}\NormalTok{ T)}
\end{Highlighting}
\end{Shaded}

\includegraphics{06-tree-based-models_files/figure-latex/unnamed-chunk-32-1.pdf}

However, when we add in the \texttt{ice} curves, we see that there is something else going on. Explain this graph. Why are there two groups of lines?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pdp}\OperatorTok{::}\KeywordTok{partial}\NormalTok{(gbm, }\DataTypeTok{pred.var =} \StringTok{"bmi"}\NormalTok{, }\DataTypeTok{grid.resolution =} \DecValTok{15}\NormalTok{, }\DataTypeTok{plot =}\NormalTok{ T, }\DataTypeTok{ice =}\NormalTok{ T, }\DataTypeTok{alpha =} \FloatTok{0.1}\NormalTok{, }\DataTypeTok{palette =} \StringTok{"viridis"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{06-tree-based-models_files/figure-latex/unnamed-chunk-33-1.pdf}

\hypertarget{answers-to-exercises-1}{%
\section{Answers to Exercises}\label{answers-to-exercises-1}}

Answers to these exercises are available at \href{https://www.futuroinsight.com/pricing}{futuroinsight.com}.

\hypertarget{unsupervised-learning}{%
\chapter{Unsupervised Learning}\label{unsupervised-learning}}

Up to this point we have been trying to predict something. We use data \(X\) to predict an unknown \(Y\), which is a number in the case of regression and a category in the case of classification. These problems are known as \emph{supvervised} because we are ``supervising'' how the model learns by giving it a target.

Here is an easy way to remember this: If you are in kindergarden class, and the teacher provides cards with pictures and words, and then asks each kid to look at the picture and then say the word, this is supervised learning. The label is the name of the picture, and the data is the picture itself. Instead, if the teacher gives out finger paints and says ``express yourself!'', then this is \emph{unsupervised learning}. There is no label but only data.

\hypertarget{principal-componant-analysis-pca}{%
\section{Principal Componant Analysis (PCA)}\label{principal-componant-analysis-pca}}

Often there are a lot of columns in the data that contain redundant information. For example, if your data consists of city traffic, such as (1) the number of cars on the road, (2) number of taxis, (3) number of pedestrians, and (4) number of Ubers, then knowing any one of these values will tell you about how busy the given road is. If there are a lot of Ubers, then there are probably also a lot of Taxis. Intuitively, we probably don't need four variables to measure this info and we could have a single variable called ``traffic level''. This would be reducing the dimension from 4 to 1.

PCA is a dimensionality reduction method which reduces the number of variables needed to retain most of the information in a matrix. If there are predictor variables \(X_1, X_2, X_3, X_4, X5\), then running PCA and choosing the first three Principal Components (PCs) will reduce the dimension from 5 to 3.

\begin{center}\includegraphics[width=10.47in]{images/PCA} \end{center}

Each PC is a linear combination of the original \(X\)s. For example, PC1 might be

\[PC_1 = 0.2X_1 + 0.3X_2 - 0.2X_3 + 0X_5 + 0.3X_5\]
The weights here are also called ``loadings'' or ``rotations'', and are (0.2, 0.3, -0.2, 0, 0.3). Each of the PCs can be interpreted as explaining part of the data. In the traffic example, PC1 might explain the traffic level, PC2 the weather, and PC3 the time of day.

\begin{longtable}[]{@{}ll@{}}
\toprule
Readings &\tabularnewline
\midrule
\endhead
ISLR 10.2 Principal Component Analysis &\tabularnewline
ISLR 10.3 Clustering Methods &\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{example-pca-on-us-arrests}{%
\subsection{Example: PCA on US Arrests}\label{example-pca-on-us-arrests}}

In this example, we perform PCA on the \texttt{USArrests} data set, which is part of
the base \texttt{R} package. The rows of the data set contain the 50 states, in
alphabetical order:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\NormalTok{states=}\KeywordTok{row.names}\NormalTok{(USArrests)}
\NormalTok{states}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "Alabama"        "Alaska"         "Arizona"        "Arkansas"      
##  [5] "California"     "Colorado"       "Connecticut"    "Delaware"      
##  [9] "Florida"        "Georgia"        "Hawaii"         "Idaho"         
## [13] "Illinois"       "Indiana"        "Iowa"           "Kansas"        
## [17] "Kentucky"       "Louisiana"      "Maine"          "Maryland"      
## [21] "Massachusetts"  "Michigan"       "Minnesota"      "Mississippi"   
## [25] "Missouri"       "Montana"        "Nebraska"       "Nevada"        
## [29] "New Hampshire"  "New Jersey"     "New Mexico"     "New York"      
## [33] "North Carolina" "North Dakota"   "Ohio"           "Oklahoma"      
## [37] "Oregon"         "Pennsylvania"   "Rhode Island"   "South Carolina"
## [41] "South Dakota"   "Tennessee"      "Texas"          "Utah"          
## [45] "Vermont"        "Virginia"       "Washington"     "West Virginia" 
## [49] "Wisconsin"      "Wyoming"
\end{verbatim}

The columns of the data set contain four variables relating to various crimes:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{glimpse}\NormalTok{(USArrests)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Observations: 50
## Variables: 4
## $ Murder   <dbl> 13.2, 10.0, 8.1, 8.8, 9.0, 7.9, 3.3, 5.9, 15.4, 17.4,...
## $ Assault  <int> 236, 263, 294, 190, 276, 204, 110, 238, 335, 211, 46,...
## $ UrbanPop <int> 58, 48, 80, 50, 91, 78, 77, 72, 80, 60, 83, 54, 83, 6...
## $ Rape     <dbl> 21.2, 44.5, 31.0, 19.5, 40.6, 38.7, 11.1, 15.8, 31.9,...
\end{verbatim}

Let's start by taking a quick look at the column means of the data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{USArrests }\OperatorTok{%>%}\StringTok{ }\KeywordTok{summarise_all}\NormalTok{(mean)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Murder Assault UrbanPop   Rape
## 1  7.788  170.76    65.54 21.232
\end{verbatim}

We see right away the the data have \textbf{vastly} different means. We can also examine the variances of the four variables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{USArrests }\OperatorTok{%>%}\StringTok{ }\KeywordTok{summarise_all}\NormalTok{(var)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     Murder  Assault UrbanPop     Rape
## 1 18.97047 6945.166 209.5188 87.72916
\end{verbatim}

Not surprisingly, the variables also have vastly different variances: the
\texttt{UrbanPop} variable measures the percentage of the population in each state
living in an urban area, which is not a comparable number to the number
of crimes committeed in each state per 100,000 individuals. If we failed to scale the
variables before performing PCA, then most of the principal components
that we observed would be driven by the \texttt{Assault} variable, since it has by
far the largest mean and variance.

Thus, it is important to standardize the
variables to have mean zero and standard deviation 1 before performing
PCA. We'll perform principal components analysis using the \texttt{prcomp()} function, which is one of several functions that perform PCA. By default, this centers the variables to have mean zero. By using the option \texttt{scale=TRUE}, we scale the variables to have standard
deviation 1:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pca =}\StringTok{ }\KeywordTok{prcomp}\NormalTok{(USArrests, }\DataTypeTok{scale=}\OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The output from \texttt{prcomp()} contains a number of useful quantities:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{names}\NormalTok{(pca)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "sdev"     "rotation" "center"   "scale"    "x"
\end{verbatim}

The \texttt{center} and \texttt{scale} components correspond to the means and standard
deviations of the variables that were used for scaling prior to implementing
PCA:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pca}\OperatorTok{$}\NormalTok{center}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Murder  Assault UrbanPop     Rape 
##    7.788  170.760   65.540   21.232
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pca}\OperatorTok{$}\NormalTok{scale}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Murder   Assault  UrbanPop      Rape 
##  4.355510 83.337661 14.474763  9.366385
\end{verbatim}

The rotation matrix provides the principal component loadings; each column
of \texttt{pr.out\textbackslash{}\$rotation} contains the corresponding principal component
loading vector:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pca}\OperatorTok{$}\NormalTok{rotation}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                 PC1        PC2        PC3         PC4
## Murder   -0.5358995  0.4181809 -0.3412327  0.64922780
## Assault  -0.5831836  0.1879856 -0.2681484 -0.74340748
## UrbanPop -0.2781909 -0.8728062 -0.3780158  0.13387773
## Rape     -0.5434321 -0.1673186  0.8177779  0.08902432
\end{verbatim}

We see that there are four distinct principal components. This is to be
expected because there are in general \texttt{min(n\ \ 1,\ p)} informative principal
components in a data set with \(n\) observations and \(p\) variables.

Using the \texttt{prcomp()} function, we do not need to explicitly multiply the
data by the principal component loading vectors in order to obtain the
principal component score vectors. Rather the 50  4 matrix \(x\) has as its
columns the principal component score vectors. That is, the \$k\^{}\{th\texttt{column\ is\ the\ \$k\^{}\{th} principal component score vector. We'll take a look at the first few states:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(pca}\OperatorTok{$}\NormalTok{x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                   PC1        PC2         PC3          PC4
## Alabama    -0.9756604  1.1220012 -0.43980366  0.154696581
## Alaska     -1.9305379  1.0624269  2.01950027 -0.434175454
## Arizona    -1.7454429 -0.7384595  0.05423025 -0.826264240
## Arkansas    0.1399989  1.1085423  0.11342217 -0.180973554
## California -2.4986128 -1.5274267  0.59254100 -0.338559240
## Colorado   -1.4993407 -0.9776297  1.08400162  0.001450164
\end{verbatim}

We can plot the first two principal components using the \texttt{biplot()} function:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{biplot}\NormalTok{(pca, }\DataTypeTok{scale=}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{07-unsupervised-learning_files/figure-latex/unnamed-chunk-11-1.pdf}

The \texttt{scale=0} argument to \texttt{biplot()} ensures that the arrows are scaled to
represent the loadings; other values for \texttt{scale} give slightly different biplots
with different interpretations.

The \texttt{prcomp()} function also outputs the standard deviation of each principal
component. We can access these standard deviations as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pca}\OperatorTok{$}\NormalTok{sdev}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1.5748783 0.9948694 0.5971291 0.4164494
\end{verbatim}

The variance explained by each principal component is obtained by squaring
these:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pca_var=pca}\OperatorTok{$}\NormalTok{sdev}\OperatorTok{^}\DecValTok{2}
\NormalTok{pca_var}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2.4802416 0.9897652 0.3565632 0.1734301
\end{verbatim}

To compute the proportion of variance explained by each principal component,
we simply divide the variance explained by each principal component
by the total variance explained by all four principal components:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pve=pca_var}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(pca_var)}
\NormalTok{pve}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.62006039 0.24744129 0.08914080 0.04335752
\end{verbatim}

We see that the first principal component explains 62.0\% of the variance
in the data, the next principal component explains 24.7\% of the variance,
and so forth. We can plot the PVE explained by each component as follows:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(pve, }\DataTypeTok{xlab=}\StringTok{"Principal Component"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"Proportion of Variance Explained"}\NormalTok{, }\DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{),}\DataTypeTok{type=}\StringTok{'b'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{07-unsupervised-learning_files/figure-latex/unnamed-chunk-15-1.pdf}

We can also use the function \texttt{cumsum()}, which computes the cumulative sum of the elements of a numeric vector, to plot the cumulative PVE:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(}\KeywordTok{cumsum}\NormalTok{(pve), }\DataTypeTok{xlab=}\StringTok{"Principal Component"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"Cumulative Proportion of Variance Explained"}\NormalTok{, }\DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{),}\DataTypeTok{type=}\StringTok{'b'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{07-unsupervised-learning_files/figure-latex/unnamed-chunk-16-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{8}\NormalTok{,}\OperatorTok{-}\DecValTok{3}\NormalTok{)}
\KeywordTok{cumsum}\NormalTok{(a)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  1  3 11  8
\end{verbatim}

\hypertarget{example-pca-on-cancel-cells}{%
\subsection{Example: PCA on Cancel Cells}\label{example-pca-on-cancel-cells}}

The data \texttt{NCI60} contins expression levels of 6,830 genes from 64 cancel cell lines. Cancer type is also recorded.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ISLR)}
\NormalTok{nci_labs=NCI60}\OperatorTok{$}\NormalTok{labs}
\NormalTok{nci_data=NCI60}\OperatorTok{$}\NormalTok{data}
\end{Highlighting}
\end{Shaded}

We first perform PCA on the data after scaling the variables (genes) to
have standard deviation one, although one could reasonably argue that it
is better not to scale the genes:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pca =}\StringTok{ }\KeywordTok{prcomp}\NormalTok{(nci_data, }\DataTypeTok{scale=}\OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We now plot the first few principal component score vectors, in order to
visualize the data. The observations (cell lines) corresponding to a given
cancer type will be plotted in the same color, so that we can see to what
extent the observations within a cancer type are similar to each other. We
first create a simple function that assigns a distinct color to each element
of a numeric vector. The function will be used to assign a color to each of
the 64 cell lines, based on the cancer type to which it corresponds.
We'll make use of the \texttt{rainbow()} function, which takes as its argument a positive integer,
and returns a vector containing that number of distinct colors.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Cols=}\ControlFlowTok{function}\NormalTok{(vec)\{}
\NormalTok{    cols=}\KeywordTok{rainbow}\NormalTok{(}\KeywordTok{length}\NormalTok{(}\KeywordTok{unique}\NormalTok{(vec)))}
    \KeywordTok{return}\NormalTok{(cols[}\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(vec))])}
\NormalTok{  \}}
\end{Highlighting}
\end{Shaded}

We now can plot the principal component score vectors:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(pca}\OperatorTok{$}\NormalTok{x[,}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{], }\DataTypeTok{col=}\KeywordTok{Cols}\NormalTok{(nci_labs), }\DataTypeTok{pch=}\DecValTok{19}\NormalTok{,}\DataTypeTok{xlab=}\StringTok{"Z1"}\NormalTok{,}\DataTypeTok{ylab=}\StringTok{"Z2"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(pca}\OperatorTok{$}\NormalTok{x[,}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{)], }\DataTypeTok{col=}\KeywordTok{Cols}\NormalTok{(nci_labs), }\DataTypeTok{pch=}\DecValTok{19}\NormalTok{,}\DataTypeTok{xlab=}\StringTok{"Z1"}\NormalTok{,}\DataTypeTok{ylab=}\StringTok{"Z3"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{07-unsupervised-learning_files/figure-latex/unnamed-chunk-20-1.pdf}

On the whole, cell lines corresponding to a single cancer type do tend to have similar values on the
first few principal component score vectors. This indicates that cell lines
from the same cancer type tend to have pretty similar gene expression
levels.

We can obtain a summary of the proportion of variance explained (PVE)
of the first few principal components using the \texttt{summary()} method for a
\texttt{prcomp} object:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(pca)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Importance of components:
##                            PC1      PC2      PC3      PC4      PC5
## Standard deviation     27.8535 21.48136 19.82046 17.03256 15.97181
## Proportion of Variance  0.1136  0.06756  0.05752  0.04248  0.03735
## Cumulative Proportion   0.1136  0.18115  0.23867  0.28115  0.31850
##                             PC6      PC7      PC8      PC9     PC10
## Standard deviation     15.72108 14.47145 13.54427 13.14400 12.73860
## Proportion of Variance  0.03619  0.03066  0.02686  0.02529  0.02376
## Cumulative Proportion   0.35468  0.38534  0.41220  0.43750  0.46126
##                            PC11     PC12     PC13     PC14     PC15
## Standard deviation     12.68672 12.15769 11.83019 11.62554 11.43779
## Proportion of Variance  0.02357  0.02164  0.02049  0.01979  0.01915
## Cumulative Proportion   0.48482  0.50646  0.52695  0.54674  0.56590
##                            PC16     PC17     PC18     PC19    PC20
## Standard deviation     11.00051 10.65666 10.48880 10.43518 10.3219
## Proportion of Variance  0.01772  0.01663  0.01611  0.01594  0.0156
## Cumulative Proportion   0.58361  0.60024  0.61635  0.63229  0.6479
##                            PC21    PC22    PC23    PC24    PC25    PC26
## Standard deviation     10.14608 10.0544 9.90265 9.64766 9.50764 9.33253
## Proportion of Variance  0.01507  0.0148 0.01436 0.01363 0.01324 0.01275
## Cumulative Proportion   0.66296  0.6778 0.69212 0.70575 0.71899 0.73174
##                           PC27   PC28    PC29    PC30    PC31    PC32
## Standard deviation     9.27320 9.0900 8.98117 8.75003 8.59962 8.44738
## Proportion of Variance 0.01259 0.0121 0.01181 0.01121 0.01083 0.01045
## Cumulative Proportion  0.74433 0.7564 0.76824 0.77945 0.79027 0.80072
##                           PC33    PC34    PC35    PC36    PC37    PC38
## Standard deviation     8.37305 8.21579 8.15731 7.97465 7.90446 7.82127
## Proportion of Variance 0.01026 0.00988 0.00974 0.00931 0.00915 0.00896
## Cumulative Proportion  0.81099 0.82087 0.83061 0.83992 0.84907 0.85803
##                           PC39    PC40    PC41   PC42    PC43   PC44
## Standard deviation     7.72156 7.58603 7.45619 7.3444 7.10449 7.0131
## Proportion of Variance 0.00873 0.00843 0.00814 0.0079 0.00739 0.0072
## Cumulative Proportion  0.86676 0.87518 0.88332 0.8912 0.89861 0.9058
##                           PC45   PC46    PC47    PC48    PC49    PC50
## Standard deviation     6.95839 6.8663 6.80744 6.64763 6.61607 6.40793
## Proportion of Variance 0.00709 0.0069 0.00678 0.00647 0.00641 0.00601
## Cumulative Proportion  0.91290 0.9198 0.92659 0.93306 0.93947 0.94548
##                           PC51    PC52    PC53    PC54    PC55    PC56
## Standard deviation     6.21984 6.20326 6.06706 5.91805 5.91233 5.73539
## Proportion of Variance 0.00566 0.00563 0.00539 0.00513 0.00512 0.00482
## Cumulative Proportion  0.95114 0.95678 0.96216 0.96729 0.97241 0.97723
##                           PC57   PC58    PC59    PC60    PC61    PC62
## Standard deviation     5.47261 5.2921 5.02117 4.68398 4.17567 4.08212
## Proportion of Variance 0.00438 0.0041 0.00369 0.00321 0.00255 0.00244
## Cumulative Proportion  0.98161 0.9857 0.98940 0.99262 0.99517 0.99761
##                           PC63      PC64
## Standard deviation     4.04124 2.148e-14
## Proportion of Variance 0.00239 0.000e+00
## Cumulative Proportion  1.00000 1.000e+00
\end{verbatim}

Using the \texttt{plot()} function, we can also plot the variance explained by the
first few principal components:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(pca)}
\end{Highlighting}
\end{Shaded}

\includegraphics{07-unsupervised-learning_files/figure-latex/unnamed-chunk-22-1.pdf}

Note that the height of each bar in the bar plot is given by squaring the
corresponding element of \texttt{pr.out\textbackslash{}\$sdev}. However, it is generally more informative to
plot the PVE of each principal component (i.e.~a \textbf{scree plot}) and the cumulative
PVE of each principal component. This can be done with just a
little tweaking:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pve =}\StringTok{ }\DecValTok{100}\OperatorTok{*}\NormalTok{pca}\OperatorTok{$}\NormalTok{sdev}\OperatorTok{^}\DecValTok{2}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(pca}\OperatorTok{$}\NormalTok{sdev}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(pve,  }\DataTypeTok{type=}\StringTok{"o"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"PVE"}\NormalTok{, }\DataTypeTok{xlab=}\StringTok{"Principal Component"}\NormalTok{, }\DataTypeTok{col=}\StringTok{"blue"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{cumsum}\NormalTok{(pve), }\DataTypeTok{type=}\StringTok{"o"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"Cumulative PVE"}\NormalTok{, }\DataTypeTok{xlab=}\StringTok{"Principal Component"}\NormalTok{, }\DataTypeTok{col=}\StringTok{"brown3"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{07-unsupervised-learning_files/figure-latex/unnamed-chunk-23-1.pdf}

We see that together, the first seven principal components
explain around 40\% of the variance in the data. This is not a huge amount
of the variance. However, looking at the scree plot, we see that while each
of the first seven principal components explain a substantial amount of
variance, there is a marked decrease in the variance explained by further
principal components. That is, there is an \textbf{elbow} in the plot after approximately
the seventh principal component. This suggests that there may
be little benefit to examining more than seven or so principal components
(phew! even examining seven principal components may be difficult).

\hypertarget{clustering}{%
\section{Clustering}\label{clustering}}

Imagine that you are a large retailer interested in understanding the customer base. There may be several ``types'' of customers, such as those shopping for business with corporate accounts, those shopping for leisure, or debt-strapped grad students. Each of these customers would exhibit different behavior, and should be treated differently statistically. But how can a customer's ``type'' be defined? Especially for large customer data sets in the millions, one can imagine how this problem can be challenging.

Clustering algorithms look for groups of observations which are similar to one another. Because there is no target variable, measuring the quality of the ``fit'' is much more complicated. There are many clustering algorithms, but this exam only focuses on the two that are most common.

\hypertarget{k-means-clustering}{%
\subsection{K-Means Clustering}\label{k-means-clustering}}

Kmeans takes continuous data and assigns observations into k clusters, or groups. In the two-dimensional example, this is the same as drawing lines around points. This consists of the following steps:

\textbf{a)} Start with two variables (\(X_1\) on the X-axis, and \(X_2\) on the Y-axis.)
\textbf{b)} Randomly assign cluster centers.
\textbf{c)} Put each point into the cluster that is closest.
\textbf{d) - f)} Move the cluster center to the mean of the points assigned to it and continue until the centers stop moving.
\textbf{g)} Repeated steps a) - f) a given number of times (controlled by \texttt{n.starts}). This reduces the uncertainty from choosing the initial centers randomly.

\begin{center}\includegraphics[width=21.26in]{07-unsupervised-learning_files/figure-latex/unnamed-chunk-24-1} \end{center}

In \texttt{R}, the function \texttt{kmeans()} performs K-means clustering in R. We begin with
a simple simulated example in which there truly are two clusters in the
data: the first 25 observations have a mean shift relative to the next 25
observations.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\NormalTok{x =}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{50}\OperatorTok{*}\DecValTok{2}\NormalTok{), }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{)}
\NormalTok{x[}\DecValTok{1}\OperatorTok{:}\DecValTok{25}\NormalTok{,}\DecValTok{1}\NormalTok{] =}\StringTok{ }\NormalTok{x[}\DecValTok{1}\OperatorTok{:}\DecValTok{25}\NormalTok{,}\DecValTok{1}\NormalTok{]}\OperatorTok{+}\DecValTok{3}
\NormalTok{x[}\DecValTok{1}\OperatorTok{:}\DecValTok{25}\NormalTok{,}\DecValTok{2}\NormalTok{] =}\StringTok{ }\NormalTok{x[}\DecValTok{1}\OperatorTok{:}\DecValTok{25}\NormalTok{,}\DecValTok{2}\NormalTok{]}\OperatorTok{-}\DecValTok{4}
\end{Highlighting}
\end{Shaded}

We now perform K-means clustering with \texttt{K\ \ =\ \ 2}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{km_out =}\StringTok{ }\KeywordTok{kmeans}\NormalTok{(x,}\DecValTok{2}\NormalTok{,}\DataTypeTok{nstart =} \DecValTok{20}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The cluster assignments of the 50 observations are contained in
\texttt{km\_out\$cluster}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{km_out}\OperatorTok{$}\NormalTok{cluster}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2
## [36] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
\end{verbatim}

The K-means clustering perfectly separated the observations into two clusters
even though we did not supply any group information to \texttt{kmeans()}. We
can plot the data, with each observation colored according to its cluster
assignment:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(x, }\DataTypeTok{col =}\NormalTok{ (km_out}\OperatorTok{$}\NormalTok{cluster}\OperatorTok{+}\DecValTok{1}\NormalTok{), }\DataTypeTok{main =} \StringTok{"K-Means Clustering Results with K = 2"}\NormalTok{, }\DataTypeTok{xlab =} \StringTok{""}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{""}\NormalTok{, }\DataTypeTok{pch =} \DecValTok{20}\NormalTok{, }\DataTypeTok{cex =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{07-unsupervised-learning_files/figure-latex/unnamed-chunk-28-1.pdf}

Here the observations can be easily plotted because they are two-dimensional.
If there were more than two variables then we could instead perform PCA
and plot the first two principal components score vectors.

In this example, we knew that there really were two clusters because
we generated the data. However, for real data, in general we do not know
the true number of clusters. We could instead have performed K-means
clustering on this example with \texttt{K\ \ =\ \ 3}. If we do this, K-means clustering will split up the two ``real'' clusters, since it has no information about them:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{4}\NormalTok{)}
\NormalTok{km_out =}\StringTok{ }\KeywordTok{kmeans}\NormalTok{(x, }\DecValTok{3}\NormalTok{, }\DataTypeTok{nstart =} \DecValTok{20}\NormalTok{)}
\NormalTok{km_out}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## K-means clustering with 3 clusters of sizes 17, 23, 10
## 
## Cluster means:
##         [,1]        [,2]
## 1  3.7789567 -4.56200798
## 2 -0.3820397 -0.08740753
## 3  2.3001545 -2.69622023
## 
## Clustering vector:
##  [1] 1 3 1 3 1 1 1 3 1 3 1 3 1 3 1 3 1 1 1 1 1 3 1 1 1 2 2 2 2 2 2 2 2 2 2
## [36] 2 2 2 2 2 2 2 2 3 2 3 2 2 2 2
## 
## Within cluster sum of squares by cluster:
## [1] 25.74089 52.67700 19.56137
##  (between_SS / total_SS =  79.3 %)
## 
## Available components:
## 
## [1] "cluster"      "centers"      "totss"        "withinss"    
## [5] "tot.withinss" "betweenss"    "size"         "iter"        
## [9] "ifault"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(x, }\DataTypeTok{col =}\NormalTok{ (km_out}\OperatorTok{$}\NormalTok{cluster}\OperatorTok{+}\DecValTok{1}\NormalTok{), }\DataTypeTok{main =} \StringTok{"K-Means Clustering Results with K = 3"}\NormalTok{, }\DataTypeTok{xlab =} \StringTok{""}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{""}\NormalTok{, }\DataTypeTok{pch =} \DecValTok{20}\NormalTok{, }\DataTypeTok{cex =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{07-unsupervised-learning_files/figure-latex/unnamed-chunk-29-1.pdf}

To run the \texttt{kmeans()} function in R with multiple initial cluster assignments,
we use the \texttt{nstart} argument. If a value of \texttt{nstart} greater than one
is used, then K-means clustering will be performed using multiple random
assignments, and the \texttt{kmeans()} function will
report only the best results. Here we compare using \texttt{nstart\ =\ 1}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{3}\NormalTok{)}
\NormalTok{km_out =}\StringTok{ }\KeywordTok{kmeans}\NormalTok{(x, }\DecValTok{3}\NormalTok{, }\DataTypeTok{nstart =} \DecValTok{1}\NormalTok{)}
\NormalTok{km_out}\OperatorTok{$}\NormalTok{tot.withinss}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 97.97927
\end{verbatim}

to \texttt{nstart\ =\ 20}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{km_out =}\StringTok{ }\KeywordTok{kmeans}\NormalTok{(x,}\DecValTok{3}\NormalTok{,}\DataTypeTok{nstart =} \DecValTok{20}\NormalTok{)}
\NormalTok{km_out}\OperatorTok{$}\NormalTok{tot.withinss}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 97.97927
\end{verbatim}

Note that \texttt{km\_out\textbackslash{}\$tot.withinss} is the total within-cluster sum of squares,
which we seek to minimize by performing K-means clustering. The individual within-cluster sum-of-squares are contained in the
vector \texttt{km\_out\textbackslash{}\$withinss}.

It is generally recommended to always run K-means clustering with a large
value of \texttt{nstart}, such as 20 or 50 to avoid getting stuck in an undesirable local
optimum.

When performing K-means clustering, in addition to using multiple initial
cluster assignments, it is also important to set a random seed using the
\texttt{set.seed()} function. This way, the initial cluster assignments can
be replicated, and the K-means output will be fully reproducible.

\hypertarget{hierarchical-clustering}{%
\section{Hierarchical Clustering}\label{hierarchical-clustering}}

Kmeans required that we choose the number of clusters, k. Hierarchical clustering is an alternative that does not require that we choose only one value of k.

The most common type of hieararchical clustering uses a \emph{bottom-up} approach. This starts with a single \textbf{observation} and then looks for others which are close and puts them into a cluster. Then it looks for other \textbf{clusters} that are similar and groups these together into a \textbf{megacluster}. It continues to do this until all observations are in the same group.

This is analyzed with a graph called a dendrogram (dendro = tree, gram = graph). The height represents ``distance'', or how similar the clusters are to one another. The clusters on the bottom, which are vertically close to one another, have similar data values; the clusters that are further apart vertically are less similar.

Choosing the value of the cutoff height changes the number of clusters that result.

\begin{center}\includegraphics[width=28.86in]{07-unsupervised-learning_files/figure-latex/unnamed-chunk-32-1} \end{center}

Certain data have a natural hiearchical structure. For example, say that the variables are City, Town, State, Country, and Continent. If we used hiearchical clustering, this pattern could be established even if we did not have labels for Cities, Towns, and so forth.

The \texttt{hclust()} function implements hierarchical clustering in R. In the following example we use the data from the previous section to plot the hierarchical
clustering dendrogram using complete, single, and average linkage clustering,
with Euclidean distance as the dissimilarity measure. We begin by
clustering observations using complete linkage. The \texttt{dist()} function is used
to compute the 50 \(\times\) 50 inter-observation Euclidean distance matrix:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hc_complete =}\StringTok{ }\KeywordTok{hclust}\NormalTok{(}\KeywordTok{dist}\NormalTok{(x), }\DataTypeTok{method =} \StringTok{"complete"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We could just as easily perform hierarchical clustering with average or
single linkage instead:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hc_average =}\StringTok{ }\KeywordTok{hclust}\NormalTok{(}\KeywordTok{dist}\NormalTok{(x), }\DataTypeTok{method =} \StringTok{"average"}\NormalTok{)}
\NormalTok{hc_single =}\StringTok{ }\KeywordTok{hclust}\NormalTok{(}\KeywordTok{dist}\NormalTok{(x), }\DataTypeTok{method =} \StringTok{"single"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We can now plot the dendrograms obtained using the usual \texttt{plot()} function.
The numbers at the bottom of the plot identify each observation:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(hc_complete,}\DataTypeTok{main =} \StringTok{"Complete Linkage"}\NormalTok{, }\DataTypeTok{xlab =} \StringTok{""}\NormalTok{, }\DataTypeTok{sub =} \StringTok{""}\NormalTok{, }\DataTypeTok{cex =} \FloatTok{.9}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(hc_average, }\DataTypeTok{main =} \StringTok{"Average Linkage"}\NormalTok{, }\DataTypeTok{xlab =} \StringTok{""}\NormalTok{, }\DataTypeTok{sub =} \StringTok{""}\NormalTok{, }\DataTypeTok{cex =} \FloatTok{.9}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(hc_single, }\DataTypeTok{main =} \StringTok{"Single Linkage"}\NormalTok{, }\DataTypeTok{xlab =} \StringTok{""}\NormalTok{, }\DataTypeTok{sub =} \StringTok{""}\NormalTok{, }\DataTypeTok{cex =} \FloatTok{.9}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{07-unsupervised-learning_files/figure-latex/unnamed-chunk-35-1.pdf}

To determine the cluster labels for each observation associated with a
given cut of the dendrogram, we can use the \texttt{cutree()} function:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cutree}\NormalTok{(hc_complete, }\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2
## [36] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cutree}\NormalTok{(hc_average, }\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 1 2 2
## [36] 2 2 2 2 2 2 2 2 1 2 1 2 2 2 2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cutree}\NormalTok{(hc_single, }\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
## [36] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
\end{verbatim}

For this data, complete and average linkage generally separate the observations
into their correct groups. However, single linkage identifies one point
as belonging to its own cluster. A more sensible answer is obtained when
four clusters are selected, although there are still two singletons:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cutree}\NormalTok{(hc_single, }\DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3
## [36] 3 3 3 3 3 3 4 3 3 3 3 3 3 3 3
\end{verbatim}

To scale the variables before performing hierarchical clustering of the
observations, we can use the \texttt{scale()} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{xsc =}\StringTok{ }\KeywordTok{scale}\NormalTok{(x)}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{hclust}\NormalTok{(}\KeywordTok{dist}\NormalTok{(xsc), }\DataTypeTok{method =} \StringTok{"complete"}\NormalTok{), }\DataTypeTok{main =} \StringTok{"Hierarchical Clustering with Scaled Features"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{07-unsupervised-learning_files/figure-latex/unnamed-chunk-38-1.pdf}

Correlation-based distance can be computed using the \texttt{as.dist()} function, which converts an arbitrary square symmetric matrix into a form that
the \texttt{hclust()} function recognizes as a distance matrix. However, this only
makes sense for data with \textbf{at least three features} since the absolute correlation
between any two observations with measurements on two features is
always 1. Let's generate and cluster a three-dimensional data set:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x =}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{30}\OperatorTok{*}\DecValTok{3}\NormalTok{), }\DataTypeTok{ncol =} \DecValTok{3}\NormalTok{)}
\NormalTok{dd =}\StringTok{ }\KeywordTok{as.dist}\NormalTok{(}\DecValTok{1}\OperatorTok{-}\KeywordTok{cor}\NormalTok{(}\KeywordTok{t}\NormalTok{(x)))}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{hclust}\NormalTok{(dd, }\DataTypeTok{method =} \StringTok{"complete"}\NormalTok{), }\DataTypeTok{main =} \StringTok{"Complete Linkage with Correlation-Based Distance"}\NormalTok{, }\DataTypeTok{xlab =} \StringTok{""}\NormalTok{, }\DataTypeTok{sub =} \StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{07-unsupervised-learning_files/figure-latex/unnamed-chunk-39-1.pdf}

\hypertarget{example-clustering-cancel-cells}{%
\subsection{Example: Clustering Cancel Cells}\label{example-clustering-cancel-cells}}

Unsupervised techniques are often used in the analysis of genomic data. In this example, we'll see how hierarchical and K-means clustering compare on the \texttt{NCI60} cancer cell line microarray data, which
consists of 6,830 gene expression measurements on 64 cancer cell lines:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# The NCI60 data}
\KeywordTok{library}\NormalTok{(ISLR)}
\NormalTok{nci_labels =}\StringTok{ }\NormalTok{NCI60}\OperatorTok{$}\NormalTok{labs}
\NormalTok{nci_data =}\StringTok{ }\NormalTok{NCI60}\OperatorTok{$}\NormalTok{data}
\end{Highlighting}
\end{Shaded}

Each cell line is labeled with a cancer type. We'll ignore the
cancer types in performing clustering, as these are unsupervised
techniques. After performing clustering, we'll use this column to see the extent to which these cancer types agree with the results of these
unsupervised techniques.

The data has 64 rows and 6,830 columns.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dim}\NormalTok{(nci_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]   64 6830
\end{verbatim}

Let's take a look at the cancer types for the cell lines:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(nci_labels)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## nci_labels
##      BREAST         CNS       COLON K562A-repro K562B-repro    LEUKEMIA 
##           7           5           7           1           1           6 
## MCF7A-repro MCF7D-repro    MELANOMA       NSCLC     OVARIAN    PROSTATE 
##           1           1           8           9           6           2 
##       RENAL     UNKNOWN 
##           9           1
\end{verbatim}

We now proceed to hierarchically cluster the cell lines in the \texttt{NCI60} data,
with the goal of finding out whether or not the observations cluster into
distinct types of cancer. To begin, we standardize the variables to have
mean zero and standard deviation one. This step is
optional, and need only be performed if we want each gene to be on the
same scale:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sd_data =}\StringTok{ }\KeywordTok{scale}\NormalTok{(nci_data)}
\end{Highlighting}
\end{Shaded}

We now perform hierarchical clustering of the observations using complete,
single, and average linkage. We'll use standard Euclidean distance as the dissimilarity
measure:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{))}
\NormalTok{data_dist =}\StringTok{ }\KeywordTok{dist}\NormalTok{(sd_data)}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{hclust}\NormalTok{(data_dist), }\DataTypeTok{labels =}\NormalTok{ nci_labels, }\DataTypeTok{main =} \StringTok{"Complete Linkage"}\NormalTok{, }\DataTypeTok{xlab =} \StringTok{""}\NormalTok{, }\DataTypeTok{sub =} \StringTok{""}\NormalTok{,}\DataTypeTok{ylab =} \StringTok{""}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{hclust}\NormalTok{(data_dist, }\DataTypeTok{method =} \StringTok{"average"}\NormalTok{), }\DataTypeTok{labels =}\NormalTok{ nci_labels, }\DataTypeTok{main =} \StringTok{"Average Linkage"}\NormalTok{, }\DataTypeTok{xlab =} \StringTok{""}\NormalTok{, }\DataTypeTok{sub =} \StringTok{""}\NormalTok{,}\DataTypeTok{ylab =} \StringTok{""}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{hclust}\NormalTok{(data_dist, }\DataTypeTok{method =} \StringTok{"single"}\NormalTok{), }\DataTypeTok{labels =}\NormalTok{ nci_labels,  }\DataTypeTok{main =} \StringTok{"Single Linkage"}\NormalTok{, }\DataTypeTok{xlab =} \StringTok{""}\NormalTok{, }\DataTypeTok{sub =} \StringTok{""}\NormalTok{,}\DataTypeTok{ylab =} \StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{07-unsupervised-learning_files/figure-latex/unnamed-chunk-44-1.pdf}

We see that the choice of linkage
certainly does affect the results obtained. Typically, single linkage will tend
to yield trailing clusters: very large clusters onto which individual observations
attach one-by-one. On the other hand, complete and average linkage
tend to yield more balanced, attractive clusters. For this reason, complete
and average linkage are generally preferred to single linkage. Clearly cell
lines within a single cancer type do tend to cluster together, although the
clustering is not perfect.

Let's use our complete linkage hierarchical clustering
for the analysis. We can cut the dendrogram at the height that will yield a particular
number of clusters, say 4:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hc_out =}\StringTok{ }\KeywordTok{hclust}\NormalTok{(}\KeywordTok{dist}\NormalTok{(sd_data))}
\NormalTok{hc_clusters =}\StringTok{ }\KeywordTok{cutree}\NormalTok{(hc_out,}\DecValTok{4}\NormalTok{)}
\KeywordTok{table}\NormalTok{(hc_clusters,nci_labels)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##            nci_labels
## hc_clusters BREAST CNS COLON K562A-repro K562B-repro LEUKEMIA MCF7A-repro
##           1      2   3     2           0           0        0           0
##           2      3   2     0           0           0        0           0
##           3      0   0     0           1           1        6           0
##           4      2   0     5           0           0        0           1
##            nci_labels
## hc_clusters MCF7D-repro MELANOMA NSCLC OVARIAN PROSTATE RENAL UNKNOWN
##           1           0        8     8       6        2     8       1
##           2           0        0     1       0        0     1       0
##           3           0        0     0       0        0     0       0
##           4           1        0     0       0        0     0       0
\end{verbatim}

There are some clear patterns. All the leukemia cell lines fall in cluster 3,
while the breast cancer cell lines are spread out over three different clusters.
We can plot the cut on the dendrogram that produces these four clusters using the \texttt{abline()} function, which draws a straight line on top of any existing plot in R:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(hc_out, }\DataTypeTok{labels =}\NormalTok{ nci_labels)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{h =} \DecValTok{139}\NormalTok{, }\DataTypeTok{col =} \StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{07-unsupervised-learning_files/figure-latex/unnamed-chunk-46-1.pdf}

Printing the output of \texttt{hclust} gives a useful brief summary of the object:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hc_out}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## hclust(d = dist(sd_data))
## 
## Cluster method   : complete 
## Distance         : euclidean 
## Number of objects: 64
\end{verbatim}

We claimed earlier that K-means clustering and hierarchical
clustering with the dendrogram cut to obtain the same number
of clusters can yield \textbf{very} different results. How do these \texttt{NCI60} hierarchical
clustering results compare to what we get if we perform K-means clustering
with \texttt{K\ \ =\ \ 4}?

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\NormalTok{km_out =}\StringTok{ }\KeywordTok{kmeans}\NormalTok{(sd_data, }\DecValTok{4}\NormalTok{, }\DataTypeTok{nstart =} \DecValTok{20}\NormalTok{)}
\NormalTok{km_clusters =}\StringTok{ }\NormalTok{km_out}\OperatorTok{$}\NormalTok{cluster}
\end{Highlighting}
\end{Shaded}

We can use a confusion matrix to compare the differences in how the two methods assigned observations to clusters:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(km_clusters,hc_clusters)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##            hc_clusters
## km_clusters  1  2  3  4
##           1 11  0  0  9
##           2 20  7  0  0
##           3  9  0  0  0
##           4  0  0  8  0
\end{verbatim}

We see that the four clusters obtained using hierarchical clustering and Kmeans
clustering are somewhat different. Cluster 2 in K-means clustering is
identical to cluster 3 in hierarchical clustering. However, the other clusters
differ: for instance, cluster 4 in K-means clustering contains a portion of
the observations assigned to cluster 1 by hierarchical clustering, as well as
all of the observations assigned to cluster 2 by hierarchical clustering.

\hypertarget{references-1}{%
\subsection{References}\label{references-1}}

These examples are an adaptation of p.~404-407, 410-413 of ``Introduction to
Statistical Learning with Applications in R'' by Gareth James, Daniela Witten, Trevor Hastie and Robert
Tibshirani. Adapted by R. Jordan Crouser at Smith College for SDS293: Machine Learning (Spring 2016), and re-implemented in Fall 2016 in \texttt{tidyverse} format by Amelia McNamara and R. Jordan Crouser at Smith College.

Used with permission from Jordan Crouser at Smith College, and to the following contributors on github:

\begin{itemize}
\tightlist
\item
  github.com/jcrouser
\item
  github.com/AmeliaMN
\item
  github.com/mhusseinmidd
\item
  github.com/rudeboybert
\item
  github.com/ijlyttle
\end{itemize}

\hypertarget{practice-exams}{%
\chapter{Practice Exams}\label{practice-exams}}

Practice exams are available at \href{https://www.futuroinsight.com/pricing}{futuroinsight.com}.

\hypertarget{references-2}{%
\chapter{References}\label{references-2}}

Burkov, Andriy. 2019. \emph{The Hundred-Page Machine Learning Book.} \url{http://themlbook.com/}

Goldburd, Mark et al.~2016.\emph{Generalized Linear Models for Insurance Rating: CAS Monograph Series Number 5}. \url{https://contentpreview.s3.us-east-2.amazonaws.com/CAS+Monograph+5+-+Generalized+Linear+Models+for+Insurance+Ratemaking.pdf}

Hastie, Trevor, et al.~2002. \emph{The Elements of Statistical Learning}. Print.

James, Gareth, et al.~2017. \emph{An Introduction to Statistical Learning}.
\url{http://faculty.marshall.usc.edu/gareth-james/ISL/ISLR\%20Seventh\%20Printing.pdf}

Piech, Chris and Ng, Andrew. 2019. Stanford CS221. Course Notes. \url{https://stanford.edu/~cpiech/cs221/handouts/kmeans.html}

Rigollet, Philippe (2017). \emph{Lecture 21: Generalized Linear Models}. Video. \url{https://www.youtube.com/watch?v=X-ix97pw0xY\&t=899s}

Wickham, Hadley. 2019. \emph{R for Data Science}. \url{https://r4ds.had.co.nz/}

\bibliography{book.bib,packages.bib}


\end{document}
