<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title> 13 GLM variable selection | Exam PA Study Guide, Spring 2021</title>
  <meta name="description" content=" 13 GLM variable selection | Exam PA Study Guide, Spring 2021" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content=" 13 GLM variable selection | Exam PA Study Guide, Spring 2021" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content=" 13 GLM variable selection | Exam PA Study Guide, Spring 2021" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="images/artificial_actuary_logo_favicon.png" type="image/x-icon" />
<link rel="prev" href="additional-glm-topics.html"/>
<link rel="next" href="bias-variance-trade-off.html"/>
<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Exam PA Study Manual</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a>
<ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#faq-frequently-asked-questions"><i class="fa fa-check"></i><b>0.1</b> FAQ: Frequently Asked Questions</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="the-exam.html"><a href="the-exam.html"><i class="fa fa-check"></i><b>1</b> The exam</a></li>
<li class="chapter" data-level="2" data-path="prometric-demo.html"><a href="prometric-demo.html"><i class="fa fa-check"></i><b>2</b> Prometric Demo</a></li>
<li class="chapter" data-level="3" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>3</b> Introduction</a></li>
<li class="chapter" data-level="4" data-path="getting-started.html"><a href="getting-started.html"><i class="fa fa-check"></i><b>4</b> Getting started</a>
<ul>
<li class="chapter" data-level="4.1" data-path="getting-started.html"><a href="getting-started.html#installing-r"><i class="fa fa-check"></i><b>4.1</b> Installing R</a></li>
<li class="chapter" data-level="4.2" data-path="getting-started.html"><a href="getting-started.html#installing-rstudio"><i class="fa fa-check"></i><b>4.2</b> Installing RStudio</a></li>
<li class="chapter" data-level="4.3" data-path="getting-started.html"><a href="getting-started.html#download-the-data"><i class="fa fa-check"></i><b>4.3</b> Download the data</a></li>
<li class="chapter" data-level="4.4" data-path="getting-started.html"><a href="getting-started.html#download-islr"><i class="fa fa-check"></i><b>4.4</b> Download ISLR</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="how-much-r-do-i-need-to-know-to-pass.html"><a href="how-much-r-do-i-need-to-know-to-pass.html"><i class="fa fa-check"></i><b>5</b> How much R do I need to know to pass?</a>
<ul>
<li class="chapter" data-level="5.1" data-path="how-much-r-do-i-need-to-know-to-pass.html"><a href="how-much-r-do-i-need-to-know-to-pass.html#how-to-use-the-pa-r-cheat-sheets"><i class="fa fa-check"></i><b>5.1</b> How to use the PA R cheat sheets</a></li>
<li class="chapter" data-level="5.2" data-path="how-much-r-do-i-need-to-know-to-pass.html"><a href="how-much-r-do-i-need-to-know-to-pass.html#example-soa-pa-61620-task-8"><i class="fa fa-check"></i><b>5.2</b> Example: SOA PA 6/16/20, Task 8</a></li>
<li class="chapter" data-level="5.3" data-path="how-much-r-do-i-need-to-know-to-pass.html"><a href="how-much-r-do-i-need-to-know-to-pass.html#example-2---data-exploration"><i class="fa fa-check"></i><b>5.3</b> Example 2 - Data exploration</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="r-programming.html"><a href="r-programming.html"><i class="fa fa-check"></i><b>6</b> R programming</a>
<ul>
<li class="chapter" data-level="6.1" data-path="r-programming.html"><a href="r-programming.html#notebook-chunks"><i class="fa fa-check"></i><b>6.1</b> Notebook chunks</a></li>
<li class="chapter" data-level="6.2" data-path="r-programming.html"><a href="r-programming.html#basic-operations"><i class="fa fa-check"></i><b>6.2</b> Basic operations</a></li>
<li class="chapter" data-level="6.3" data-path="r-programming.html"><a href="r-programming.html#lists"><i class="fa fa-check"></i><b>6.3</b> Lists</a></li>
<li class="chapter" data-level="6.4" data-path="r-programming.html"><a href="r-programming.html#functions"><i class="fa fa-check"></i><b>6.4</b> Functions</a></li>
<li class="chapter" data-level="6.5" data-path="r-programming.html"><a href="r-programming.html#data-frames"><i class="fa fa-check"></i><b>6.5</b> Data frames</a></li>
<li class="chapter" data-level="6.6" data-path="r-programming.html"><a href="r-programming.html#pipes"><i class="fa fa-check"></i><b>6.6</b> Pipes</a></li>
<li class="chapter" data-level="6.7" data-path="r-programming.html"><a href="r-programming.html#the-soas-code-doesnt-use-pipes-or-dplyr-so-can-i-skip-learning-this"><i class="fa fa-check"></i><b>6.7</b> The SOA‚Äôs code doesn‚Äôt use pipes or dplyr, so can I skip learning this?</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="data-exploration.html"><a href="data-exploration.html"><i class="fa fa-check"></i><b>7</b> Data exploration</a>
<ul>
<li class="chapter" data-level="7.1" data-path="data-exploration.html"><a href="data-exploration.html#how-to-make-graphs-in-r"><i class="fa fa-check"></i><b>7.1</b> How to make graphs in R</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="data-exploration.html"><a href="data-exploration.html#add-a-plot"><i class="fa fa-check"></i><b>7.1.1</b> Add a plot</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="data-exploration.html"><a href="data-exploration.html#the-different-graph-types"><i class="fa fa-check"></i><b>7.2</b> The different graph types</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="data-exploration.html"><a href="data-exploration.html#histogram"><i class="fa fa-check"></i><b>7.2.1</b> Histogram</a></li>
<li class="chapter" data-level="7.2.2" data-path="data-exploration.html"><a href="data-exploration.html#box-plot"><i class="fa fa-check"></i><b>7.2.2</b> Box plot</a></li>
<li class="chapter" data-level="7.2.3" data-path="data-exploration.html"><a href="data-exploration.html#scatterplot"><i class="fa fa-check"></i><b>7.2.3</b> Scatterplot</a></li>
<li class="chapter" data-level="7.2.4" data-path="data-exploration.html"><a href="data-exploration.html#bar-charts"><i class="fa fa-check"></i><b>7.2.4</b> Bar charts</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="data-exploration.html"><a href="data-exploration.html#how-to-save-time-with-dplyr"><i class="fa fa-check"></i><b>7.3</b> How to save time with dplyr</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="data-exploration.html"><a href="data-exploration.html#data-manipulation-chaining"><i class="fa fa-check"></i><b>7.3.1</b> Data manipulation chaining</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="data-exploration.html"><a href="data-exploration.html#how-to-explore-the-data"><i class="fa fa-check"></i><b>7.4</b> How to explore the data</a></li>
<li class="chapter" data-level="7.5" data-path="data-exploration.html"><a href="data-exploration.html#how-to-transform-the-data"><i class="fa fa-check"></i><b>7.5</b> How to transform the data</a></li>
<li class="chapter" data-level="7.6" data-path="data-exploration.html"><a href="data-exploration.html#missing-values"><i class="fa fa-check"></i><b>7.6</b> Missing values</a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="data-exploration.html"><a href="data-exploration.html#types-of-missing-values"><i class="fa fa-check"></i><b>7.6.1</b> Types of Missing Values</a></li>
<li class="chapter" data-level="7.6.2" data-path="data-exploration.html"><a href="data-exploration.html#missing-value-resolutions"><i class="fa fa-check"></i><b>7.6.2</b> Missing Value Resolutions:</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="data-exploration.html"><a href="data-exploration.html#example-soa-pa-121219-task-1"><i class="fa fa-check"></i><b>7.7</b> Example: SOA PA 12/12/19, Task 1</a>
<ul>
<li class="chapter" data-level="7.7.1" data-path="data-exploration.html"><a href="data-exploration.html#garbage-in-garbage-out"><i class="fa fa-check"></i><b>7.7.1</b> Garbage in; garbage out üóë</a></li>
<li class="chapter" data-level="7.7.2" data-path="data-exploration.html"><a href="data-exploration.html#be-a-detective"><i class="fa fa-check"></i><b>7.7.2</b> Be a detective üîé</a></li>
<li class="chapter" data-level="7.7.3" data-path="data-exploration.html"><a href="data-exploration.html#a-picture-is-worth-a-thousand-words"><i class="fa fa-check"></i><b>7.7.3</b> A picture is worth a thousand words üì∑</a></li>
<li class="chapter" data-level="7.7.4" data-path="data-exploration.html"><a href="data-exploration.html#factor-or-numeric"><i class="fa fa-check"></i><b>7.7.4</b> Factor or numeric ‚ùì</a></li>
<li class="chapter" data-level="7.7.5" data-path="data-exploration.html"><a href="data-exploration.html#of-statistics-are-false"><i class="fa fa-check"></i><b>7.7.5</b> 73.6% of statistics are false üò≤</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="data-exploration.html"><a href="data-exploration.html#exercises"><i class="fa fa-check"></i><b>7.8</b> Exercises</a>
<ul>
<li class="chapter" data-level="7.8.1" data-path="data-exploration.html"><a href="data-exploration.html#data-exploration-practice"><i class="fa fa-check"></i><b>7.8.1</b> Data Exploration Practice</a></li>
<li class="chapter" data-level="7.8.2" data-path="data-exploration.html"><a href="data-exploration.html#dplyr-practice"><i class="fa fa-check"></i><b>7.8.2</b> Dplyr Practice</a></li>
</ul></li>
<li class="chapter" data-level="7.9" data-path="data-exploration.html"><a href="data-exploration.html#answers-to-exercises"><i class="fa fa-check"></i><b>7.9</b> Answers to exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html"><i class="fa fa-check"></i><b>8</b> Introduction to modeling</a>
<ul>
<li class="chapter" data-level="8.1" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#modeling-vocabulary"><i class="fa fa-check"></i><b>8.1</b> Modeling vocabulary</a></li>
<li class="chapter" data-level="8.2" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#modeling-notation"><i class="fa fa-check"></i><b>8.2</b> Modeling notation</a></li>
<li class="chapter" data-level="8.3" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#ordinary-least-squares-ols"><i class="fa fa-check"></i><b>8.3</b> Ordinary Least Squares (OLS)</a></li>
<li class="chapter" data-level="8.4" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#r2-statistic"><i class="fa fa-check"></i><b>8.4</b> R^2 Statistic</a></li>
<li class="chapter" data-level="8.5" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#correlation"><i class="fa fa-check"></i><b>8.5</b> Correlation</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#pearsons-correlation"><i class="fa fa-check"></i><b>8.5.1</b> Pearson‚Äôs correlation</a></li>
<li class="chapter" data-level="8.5.2" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#spearman-rank-correlation"><i class="fa fa-check"></i><b>8.5.2</b> Spearman (rank) correlation</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#regression-vs.-classification"><i class="fa fa-check"></i><b>8.6</b> Regression vs.¬†classification</a></li>
<li class="chapter" data-level="8.7" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#regression-metrics"><i class="fa fa-check"></i><b>8.7</b> Regression metrics</a>
<ul>
<li class="chapter" data-level="8.7.1" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#example-soa-pa-61820-task-4"><i class="fa fa-check"></i><b>8.7.1</b> Example: SOA PA 6/18/20, Task 4</a></li>
</ul></li>
<li class="chapter" data-level="8.8" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#example-health-costs"><i class="fa fa-check"></i><b>8.8</b> Example: Health Costs</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html"><i class="fa fa-check"></i><b>9</b> Generalized linear Models (GLMs)</a>
<ul>
<li class="chapter" data-level="9.0.1" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#assumptions-of-ols"><i class="fa fa-check"></i><b>9.0.1</b> Assumptions of OLS</a></li>
<li class="chapter" data-level="9.0.2" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#assumptions-of-glms"><i class="fa fa-check"></i><b>9.0.2</b> Assumptions of GLMs</a></li>
<li class="chapter" data-level="9.1" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#advantages-and-disadvantages"><i class="fa fa-check"></i><b>9.1</b> Advantages and disadvantages</a></li>
<li class="chapter" data-level="9.2" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#glms-for-regression"><i class="fa fa-check"></i><b>9.2</b> GLMs for regression</a></li>
<li class="chapter" data-level="9.3" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#interpretation-of-coefficients"><i class="fa fa-check"></i><b>9.3</b> Interpretation of coefficients</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#identity-link"><i class="fa fa-check"></i><b>9.3.1</b> Identity link</a></li>
<li class="chapter" data-level="9.3.2" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#log-link"><i class="fa fa-check"></i><b>9.3.2</b> Log link</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#other-links"><i class="fa fa-check"></i><b>9.4</b> Other links</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="glms-for-classification.html"><a href="glms-for-classification.html"><i class="fa fa-check"></i><b>10</b> GLMs for classification</a>
<ul>
<li class="chapter" data-level="10.1" data-path="glms-for-classification.html"><a href="glms-for-classification.html#binary-target"><i class="fa fa-check"></i><b>10.1</b> Binary target</a></li>
<li class="chapter" data-level="10.2" data-path="glms-for-classification.html"><a href="glms-for-classification.html#count-target"><i class="fa fa-check"></i><b>10.2</b> Count target</a></li>
<li class="chapter" data-level="10.3" data-path="glms-for-classification.html"><a href="glms-for-classification.html#link-functions"><i class="fa fa-check"></i><b>10.3</b> Link functions</a></li>
<li class="chapter" data-level="10.4" data-path="glms-for-classification.html"><a href="glms-for-classification.html#interpretation-of-coefficients-1"><i class="fa fa-check"></i><b>10.4</b> Interpretation of coefficients</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="glms-for-classification.html"><a href="glms-for-classification.html#logit"><i class="fa fa-check"></i><b>10.4.1</b> Logit</a></li>
<li class="chapter" data-level="10.4.2" data-path="glms-for-classification.html"><a href="glms-for-classification.html#probit-cauchit-cloglog"><i class="fa fa-check"></i><b>10.4.2</b> Probit, Cauchit, Cloglog</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="glms-for-classification.html"><a href="glms-for-classification.html#demo-the-model-for-interpretation"><i class="fa fa-check"></i><b>10.5</b> Demo the model for interpretation</a></li>
<li class="chapter" data-level="10.6" data-path="glms-for-classification.html"><a href="glms-for-classification.html#example---auto-claims"><i class="fa fa-check"></i><b>10.6</b> Example - Auto Claims</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="classification-metrics.html"><a href="classification-metrics.html"><i class="fa fa-check"></i><b>11</b> Classification metrics</a>
<ul>
<li class="chapter" data-level="11.1" data-path="classification-metrics.html"><a href="classification-metrics.html#area-under-the-roc-curve-auc"><i class="fa fa-check"></i><b>11.1</b> Area Under the ROC Curve (AUC)</a></li>
<li class="chapter" data-level="11.2" data-path="classification-metrics.html"><a href="classification-metrics.html#example---auto-claims-1"><i class="fa fa-check"></i><b>11.2</b> Example - Auto Claims</a></li>
<li class="chapter" data-level="11.3" data-path="classification-metrics.html"><a href="classification-metrics.html#example-soa-hr-task-5"><i class="fa fa-check"></i><b>11.3</b> Example: SOA HR, Task 5</a></li>
<li class="chapter" data-level="11.4" data-path="classification-metrics.html"><a href="classification-metrics.html#example-soa-pa-121219-task-11"><i class="fa fa-check"></i><b>11.4</b> Example: SOA PA 12/12/19, Task 11</a></li>
<li class="chapter" data-level="11.5" data-path="classification-metrics.html"><a href="classification-metrics.html#additional-reading"><i class="fa fa-check"></i><b>11.5</b> Additional reading</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html"><i class="fa fa-check"></i><b>12</b> Additional GLM topics</a>
<ul>
<li class="chapter" data-level="12.1" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#residuals"><i class="fa fa-check"></i><b>12.1</b> Residuals</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#raw-residuals"><i class="fa fa-check"></i><b>12.1.1</b> Raw residuals</a></li>
<li class="chapter" data-level="12.1.2" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#deviance-residuals"><i class="fa fa-check"></i><b>12.1.2</b> Deviance residuals</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#example"><i class="fa fa-check"></i><b>12.2</b> Example</a></li>
<li class="chapter" data-level="12.3" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#log-transforms-of-predictors"><i class="fa fa-check"></i><b>12.3</b> Log transforms of predictors</a></li>
<li class="chapter" data-level="12.4" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#reference-levels"><i class="fa fa-check"></i><b>12.4</b> Reference levels</a></li>
<li class="chapter" data-level="12.5" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#interactions"><i class="fa fa-check"></i><b>12.5</b> Interactions</a></li>
<li class="chapter" data-level="12.6" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#offsets"><i class="fa fa-check"></i><b>12.6</b> Offsets</a></li>
<li class="chapter" data-level="12.7" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#tweedie-regression"><i class="fa fa-check"></i><b>12.7</b> Tweedie regression</a></li>
<li class="chapter" data-level="12.8" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#combinations-of-link-functions-and-target-distributions"><i class="fa fa-check"></i><b>12.8</b> Combinations of Link Functions and Target Distributions</a>
<ul>
<li class="chapter" data-level="12.8.1" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#gaussian-response-with-log-link"><i class="fa fa-check"></i><b>12.8.1</b> Gaussian Response with Log Link</a></li>
<li class="chapter" data-level="12.8.2" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#gaussian-response-with-inverse-link"><i class="fa fa-check"></i><b>12.8.2</b> Gaussian Response with Inverse Link</a></li>
<li class="chapter" data-level="12.8.3" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#gaussian-response-with-identity-link"><i class="fa fa-check"></i><b>12.8.3</b> Gaussian Response with Identity Link</a></li>
<li class="chapter" data-level="12.8.4" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#gaussian-response-with-log-link-and-negative-values"><i class="fa fa-check"></i><b>12.8.4</b> Gaussian Response with Log Link and Negative Values</a></li>
<li class="chapter" data-level="12.8.5" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#gamma-response-with-log-link"><i class="fa fa-check"></i><b>12.8.5</b> Gamma Response with Log Link</a></li>
<li class="chapter" data-level="12.8.6" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#gamma-with-inverse-link"><i class="fa fa-check"></i><b>12.8.6</b> Gamma with Inverse Link</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="glm-variable-selection.html"><a href="glm-variable-selection.html"><i class="fa fa-check"></i><b>13</b> GLM variable selection</a>
<ul>
<li class="chapter" data-level="13.1" data-path="glm-variable-selection.html"><a href="glm-variable-selection.html#stepwise-subset-selection"><i class="fa fa-check"></i><b>13.1</b> Stepwise subset selection</a></li>
<li class="chapter" data-level="13.2" data-path="glm-variable-selection.html"><a href="glm-variable-selection.html#example-soa-pa-61219-task-6"><i class="fa fa-check"></i><b>13.2</b> Example: SOA PA 6/12/19, Task 6</a></li>
<li class="chapter" data-level="13.3" data-path="glm-variable-selection.html"><a href="glm-variable-selection.html#penalized-linear-models"><i class="fa fa-check"></i><b>13.3</b> Penalized Linear Models</a></li>
<li class="chapter" data-level="13.4" data-path="glm-variable-selection.html"><a href="glm-variable-selection.html#ridge-regression"><i class="fa fa-check"></i><b>13.4</b> Ridge Regression</a></li>
<li class="chapter" data-level="13.5" data-path="glm-variable-selection.html"><a href="glm-variable-selection.html#lasso"><i class="fa fa-check"></i><b>13.5</b> Lasso</a></li>
<li class="chapter" data-level="13.6" data-path="glm-variable-selection.html"><a href="glm-variable-selection.html#elastic-net"><i class="fa fa-check"></i><b>13.6</b> Elastic Net</a></li>
<li class="chapter" data-level="13.7" data-path="glm-variable-selection.html"><a href="glm-variable-selection.html#advantages-and-disadvantages-1"><i class="fa fa-check"></i><b>13.7</b> Advantages and disadvantages</a></li>
<li class="chapter" data-level="13.8" data-path="glm-variable-selection.html"><a href="glm-variable-selection.html#example-ridge-regression"><i class="fa fa-check"></i><b>13.8</b> Example: Ridge Regression</a></li>
<li class="chapter" data-level="13.9" data-path="glm-variable-selection.html"><a href="glm-variable-selection.html#example-the-lasso"><i class="fa fa-check"></i><b>13.9</b> Example: The Lasso</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="bias-variance-trade-off.html"><a href="bias-variance-trade-off.html"><i class="fa fa-check"></i><b>14</b> Bias-variance trade-off</a></li>
<li class="chapter" data-level="15" data-path="tree-based-models.html"><a href="tree-based-models.html"><i class="fa fa-check"></i><b>15</b> Tree-based models</a>
<ul>
<li class="chapter" data-level="15.1" data-path="tree-based-models.html"><a href="tree-based-models.html#decision-trees"><i class="fa fa-check"></i><b>15.1</b> Decision Trees</a>
<ul>
<li class="chapter" data-level="15.1.1" data-path="tree-based-models.html"><a href="tree-based-models.html#example-soa-pa-6182020-task-6"><i class="fa fa-check"></i><b>15.1.1</b> Example: SOA PA 6/18/2020, Task 6</a></li>
<li class="chapter" data-level="15.1.2" data-path="tree-based-models.html"><a href="tree-based-models.html#advantages-and-disadvantages-2"><i class="fa fa-check"></i><b>15.1.2</b> Advantages and disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="tree-based-models.html"><a href="tree-based-models.html#ensemble-learning"><i class="fa fa-check"></i><b>15.2</b> Ensemble learning</a>
<ul>
<li class="chapter" data-level="15.2.1" data-path="tree-based-models.html"><a href="tree-based-models.html#bagging"><i class="fa fa-check"></i><b>15.2.1</b> Bagging</a></li>
<li class="chapter" data-level="15.2.2" data-path="tree-based-models.html"><a href="tree-based-models.html#boosting"><i class="fa fa-check"></i><b>15.2.2</b> Boosting</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="tree-based-models.html"><a href="tree-based-models.html#random-forests"><i class="fa fa-check"></i><b>15.3</b> Random Forests</a>
<ul>
<li class="chapter" data-level="15.3.1" data-path="tree-based-models.html"><a href="tree-based-models.html#example-1"><i class="fa fa-check"></i><b>15.3.1</b> Example</a></li>
<li class="chapter" data-level="15.3.2" data-path="tree-based-models.html"><a href="tree-based-models.html#variable-importance"><i class="fa fa-check"></i><b>15.3.2</b> Variable Importance</a></li>
<li class="chapter" data-level="15.3.3" data-path="tree-based-models.html"><a href="tree-based-models.html#partial-dependence"><i class="fa fa-check"></i><b>15.3.3</b> Partial dependence</a></li>
<li class="chapter" data-level="15.3.4" data-path="tree-based-models.html"><a href="tree-based-models.html#advantages-and-disadvantages-3"><i class="fa fa-check"></i><b>15.3.4</b> Advantages and disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="tree-based-models.html"><a href="tree-based-models.html#gradient-boosted-trees"><i class="fa fa-check"></i><b>15.4</b> Gradient Boosted Trees</a>
<ul>
<li class="chapter" data-level="15.4.1" data-path="tree-based-models.html"><a href="tree-based-models.html#gradient-boosting"><i class="fa fa-check"></i><b>15.4.1</b> Gradient Boosting</a></li>
<li class="chapter" data-level="15.4.2" data-path="tree-based-models.html"><a href="tree-based-models.html#notation"><i class="fa fa-check"></i><b>15.4.2</b> Notation</a></li>
<li class="chapter" data-level="15.4.3" data-path="tree-based-models.html"><a href="tree-based-models.html#parameters"><i class="fa fa-check"></i><b>15.4.3</b> Parameters</a></li>
<li class="chapter" data-level="15.4.4" data-path="tree-based-models.html"><a href="tree-based-models.html#example-2"><i class="fa fa-check"></i><b>15.4.4</b> Example</a></li>
<li class="chapter" data-level="15.4.5" data-path="tree-based-models.html"><a href="tree-based-models.html#advantages-and-disadvantages-4"><i class="fa fa-check"></i><b>15.4.5</b> Advantages and disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="15.5" data-path="tree-based-models.html"><a href="tree-based-models.html#exercises-1"><i class="fa fa-check"></i><b>15.5</b> Exercises</a>
<ul>
<li class="chapter" data-level="15.5.1" data-path="tree-based-models.html"><a href="tree-based-models.html#rf-tuning-with-caret"><i class="fa fa-check"></i><b>15.5.1</b> 1. RF tuning with <code>caret</code></a></li>
<li class="chapter" data-level="15.5.2" data-path="tree-based-models.html"><a href="tree-based-models.html#tuning-a-gbm-with-caret"><i class="fa fa-check"></i><b>15.5.2</b> 2. Tuning a GBM with <code>caret</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html"><i class="fa fa-check"></i><b>16</b> Unsupervised Learning</a>
<ul>
<li class="chapter" data-level="16.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#types-of-learning"><i class="fa fa-check"></i><b>16.1</b> Types of Learning</a></li>
<li class="chapter" data-level="16.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#correlation-analysis"><i class="fa fa-check"></i><b>16.2</b> Correlation Analysis</a>
<ul>
<li class="chapter" data-level="16.2.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#correlation-does-not-equal-causation"><i class="fa fa-check"></i><b>16.2.1</b> Correlation does not equal causation</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#principal-component-analysis-pca"><i class="fa fa-check"></i><b>16.3</b> Principal Component Analysis (PCA)</a>
<ul>
<li class="chapter" data-level="16.3.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#example-us-arrests"><i class="fa fa-check"></i><b>16.3.1</b> Example: US Arrests</a></li>
<li class="chapter" data-level="16.3.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#example-soa-pa-61219-task-3"><i class="fa fa-check"></i><b>16.3.2</b> Example: SOA PA 6/12/19, Task 3</a></li>
<li class="chapter" data-level="16.3.3" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#example-pca-on-cancel-cells"><i class="fa fa-check"></i><b>16.3.3</b> Example: PCA on Cancel Cells</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#clustering"><i class="fa fa-check"></i><b>16.4</b> Clustering</a></li>
<li class="chapter" data-level="16.5" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#k-means-clustering"><i class="fa fa-check"></i><b>16.5</b> K-Means Clustering</a></li>
<li class="chapter" data-level="16.6" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#hierarchical-clustering"><i class="fa fa-check"></i><b>16.6</b> Hierarchical Clustering</a>
<ul>
<li class="chapter" data-level="16.6.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#example-clustering-cancel-cells"><i class="fa fa-check"></i><b>16.6.1</b> Example: Clustering Cancel Cells</a></li>
<li class="chapter" data-level="16.6.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#references"><i class="fa fa-check"></i><b>16.6.2</b> References</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="17" data-path="writing-and-communication.html"><a href="writing-and-communication.html"><i class="fa fa-check"></i><b>17</b> Writing and Communication</a>
<ul>
<li class="chapter" data-level="17.1" data-path="writing-and-communication.html"><a href="writing-and-communication.html#spelling-and-grammar"><i class="fa fa-check"></i><b>17.1</b> Spelling and Grammar</a></li>
<li class="chapter" data-level="17.2" data-path="writing-and-communication.html"><a href="writing-and-communication.html#how-to-write-for-pa"><i class="fa fa-check"></i><b>17.2</b> How to Write for PA</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="references-1.html"><a href="references-1.html"><i class="fa fa-check"></i><b>18</b> References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Exam PA Study Guide, Spring 2021</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="glm-variable-selection" class="section level1" number="13">
<h1><span class="header-section-number"> 13</span> GLM variable selection</h1>
<p>Predictive Analytics is about using results to solve business problems. Complex models are almost useless if they cannot be explained. In this chapter, we will learn how to make GLMs easier to explain by either removing variables entirely or lessening their impact.</p>
<div id="stepwise-subset-selection" class="section level2" number="13.1">
<h2><span class="header-section-number">13.1</span> Stepwise subset selection</h2>
<p>In theory, we could test all possible combinations of variables and interaction terms. This includes all <span class="math inline">\(p\)</span> models with one predictor, all p-choose-2 models with two predictors, all p-choose-3 models with three predictors, and so forth. Then we take whichever model has the best performance as the final model.</p>
<p>This ‚Äúbrute force‚Äù approach is statistically ineffective: the more variables which are searched, the higher the chance of finding models that over fit.</p>
<p>A subtler method, known as <em>stepwise selection</em>, reduces the chances of over-fitting by only looking at the most promising models.</p>
<p><strong>Forward Stepwise Selection:</strong></p>
<ol style="list-style-type: decimal">
<li>Start with no predictors in the model;</li>
<li>Evaluate all <span class="math inline">\(p\)</span> models which use only one predictor and choose the one with the best performance (highest <span class="math inline">\(R^2\)</span> or lowest <span class="math inline">\(\text{RSS}\)</span>);</li>
<li>Repeat the process when adding one additional predictor, and continue until there is a model with one predictor, a model with two predictors, a model with three predictors, and so forth until there are <span class="math inline">\(p\)</span> models;</li>
<li>Select the single best model which has the best <span class="math inline">\(\text{AIC}\)</span>,<span class="math inline">\(\text{BIC}\)</span>, or adjusted <span class="math inline">\(R^2\)</span>.</li>
</ol>
<p><strong>Backward Stepwise Selection:</strong></p>
<ol style="list-style-type: decimal">
<li>Start with a model that contains all predictors;</li>
<li>Create a model which removes all predictors;</li>
<li>Choose the best model which removes all-but-one predictor;</li>
<li>Choose the best model which removes all-but-two predictors;</li>
<li>Continue until there are <span class="math inline">\(p\)</span> models;</li>
<li>Select the single best model which has the best <span class="math inline">\(\text{AIC}\)</span>,<span class="math inline">\(\text{BIC}\)</span>, or adjusted <span class="math inline">\(R^2\)</span>.</li>
</ol>
<p><strong>Both Forward &amp; Backward Selection:</strong></p>
<p>A hybrid approach is to consider use both forward and backward selection. This is done by creating two lists of variables at each step, one from forward and one from backward selection. Then variables from <em>both</em> lists are tested to see if adding or subtracting from the current model would improve the fit or not. ISLR does not mention this directly, however, by default the <code>stepAIC</code> function uses a default of <code>both</code>.</p>
<blockquote>
<p><strong>Tip</strong>: Always load the <code>MASS</code> library before <code>dplyr</code> or <code>tidyverse</code>. Otherwise there will be conflicts as there are functions named <code>select()</code> and <code>filter()</code> in both. Alternatively, specify the library in the function call with <code>dplyr::select()</code>.</p>
</blockquote>
<table>
<thead>
<tr class="header">
<th>Readings</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><a href="https://contentpreview.s3.us-east-2.amazonaws.com/CAS+Monograph+5+-+Generalized+Linear+Models+for+Insurance+Ratemaking.pdf">CAS Monograph 5 Chapter 2</a></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<div id="example-soa-pa-61219-task-6" class="section level2" number="13.2">
<h2><span class="header-section-number">13.2</span> Example: SOA PA 6/12/19, Task 6</h2>
<iframe src="https://player.vimeo.com/video/467840434?title=0&amp;byline=0&amp;portrait=0" width="640" height="360" frameborder="0" allow="autoplay; fullscreen" allowfullscreen>
</iframe>
<p>Already enrolled? Watch the full video: <a target="_parent" href="https://course.exampa.net/mod/page/view.php?id=204
">Practice Exams</a> | <a target="_parent" href="https://course.exampa.net/mod/page/view.php?id=155
">Practice Exams + Lessons</a></p>
<blockquote>
<p>AIC and BIC are among the available techniques for feature selection. Briefly describe them and
outline the differences in the two criteria. Make a recommendation as to which one should be
used for this problem. Use only your recommended criterion when completing this task.</p>
</blockquote>
<blockquote>
<p>Some of the features may lack predictive power and lead to overfitting. Determine which
features should be retained. Use the stepAIC function (from the MASS package) to make this
determination. When using this function, there are two decisions to make. Make each decision
based on the business problem. Use ?stepAIC to learn more about these parameters (note that
the MASS package must be loaded before help on this function can be accessed).</p>
</blockquote>
<blockquote>
<p>Use direction = ‚Äúbackward‚Äù or direction = ‚Äúforward‚Äù
Use AIC (k = 2) or BIC (k=log(nrow(train)))</p>
</blockquote>
</div>
<div id="penalized-linear-models" class="section level2" number="13.3">
<h2><span class="header-section-number">13.3</span> Penalized Linear Models</h2>
<p>One of the main weaknesses of the GLM, including all linear models in this chapter, is that the features need to be selected by hand. Stepwise selection helps to improve this process, but fails when the inputs are correlated and often has a strong dependence on seemingly arbitrary choices of evaluation metrics such as using AIC or BIC and forward or backward directions.</p>
<p>The <strong>Bias Variance Trade-off</strong> is about finding the lowest error by changing the flexibility of the model. Penalization methods use a parameter to control for this flexibility directly.</p>
<p>Earlier on we said that the linear model minimizes the sum of square terms, known as the residual sum of squares (RSS)</p>
<p><span class="math display">\[
\text{RSS} = \sum_i(y_i - \hat{y})^2 = \sum_i(y_i - \beta_0 - \sum_{j = 1}^p\beta_j x_{ij})^2
\]</span></p>
<p>This loss function can be modified so that models which include more (and larger) coefficients are considered as worse. In other words, when there are more <span class="math inline">\(\beta\)</span>‚Äôs, or <span class="math inline">\(\beta\)</span>‚Äôs which are larger, the RSS is higher.</p>
</div>
<div id="ridge-regression" class="section level2" number="13.4">
<h2><span class="header-section-number">13.4</span> Ridge Regression</h2>
<p>Ridge regression adds a penalty term which is proportional to the square of the sum of the coefficients. This is known as the ‚ÄúL2‚Äù norm.</p>
<p><span class="math display">\[
\sum_i(y_i - \beta_0 - \sum_{j = 1}^p\beta_j x_{ij})^2 + \lambda \sum_{j = 1}^p\beta_j^2
\]</span></p>
<iframe width="563" height="383" src="https://www.youtube.com/embed/Q81RR3yKn30" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
<p>This <span class="math inline">\(\lambda\)</span> controls how much of a penalty is imposed on the size of the coefficients. When <span class="math inline">\(\lambda\)</span> is high, simpler models are treated more favorably because the <span class="math inline">\(\sum_{j = 1}^p\beta_j^2\)</span> carries more weight. Conversely, then <span class="math inline">\(\lambda\)</span> is low, complex models are more favored. When <span class="math inline">\(\lambda = 0\)</span>, we have an ordinary GLM.</p>
</div>
<div id="lasso" class="section level2" number="13.5">
<h2><span class="header-section-number">13.5</span> Lasso</h2>
<p>The official name is the Least Absolute Shrinkage and Selection Operator, but the common name is just ‚Äúthe lasso.‚Äù Just as with Ridge regression, we want to favor simpler models; however, we also want to <em>select</em> variables. This is the same as forcing some coefficients to be equal to 0.</p>
<p>Instead of taking the square of the coefficients (L2 norm), we take the absolute value (L1 norm).</p>
<p><span class="math display">\[
\sum_i(y_i - \beta_0 - \sum_{j = 1}^p\beta_j x_{ij})^2 + \lambda \sum_{j = 1}^p|\beta_j|
\]</span></p>
<iframe width="563" height="383" src="https://www.youtube.com/embed/NGf0voTMlcs" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
<p>In ISLR, Hastie et al show that this results in coefficients being forced to be exactly 0. This is extremely useful because it means that by changing <span class="math inline">\(\lambda\)</span>, we can select how many variables to use in the model.</p>
<p><strong>Note</strong>: While any response family is possible with penalized regression, in R, only the Gaussian family is possible in the library <code>glmnet</code>, and so this is the only type of question that the SOA can ask.</p>
</div>
<div id="elastic-net" class="section level2" number="13.6">
<h2><span class="header-section-number">13.6</span> Elastic Net</h2>
<p>The Elastic Net uses a penalty term which is between the L1 and L2 norms. The penalty term is a weighted average using the mixing parameter <span class="math inline">\(0 \leq \alpha \leq 1\)</span>. The loss function is then</p>
<p><span class="math display">\[\text{RSS} + (1 - \alpha) \sum_{j = 1}^{p}\beta_j^2 + \alpha \sum_{j = 1}^p |\beta_j|\]</span>
When <span class="math inline">\(\alpha = 1\)</span>, the model is known as the Lasso, and when <span class="math inline">\(\alpha = 0\)</span>, the model is known as Ridge Regression.</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/PaFPbb66DxQ?rel=0&amp;showinfo=1&amp;playlist=1dKRdX9bfIo,ctmNq7FgbvI" frameborder="0" allowfullscreen>
</iframe>
<p>Luckily, none of this needs to be memorized. On the exam, read the documentation in R to refresh your memory. For the Elastic Net, the function is <code>glmnet</code>, and so running <code>?glmnet</code> will give you this info.</p>
<blockquote>
<p><strong>Shortcut</strong>: When using complicated functions on the exam, use <code>?function_name</code> to get the documentation.</p>
</blockquote>
</div>
<div id="advantages-and-disadvantages-1" class="section level2" number="13.7">
<h2><span class="header-section-number">13.7</span> Advantages and disadvantages</h2>
<p><strong>Elastic Net/Lasso/Ridge Advantages</strong></p>
<ul>
<li>All benefits from GLMS</li>
<li>Automatic variable selection for Lasso; smaller coefficients for Ridge</li>
<li>Better predictive power than GLM</li>
</ul>
<p><strong>Elastic Net/Lasso/Ridge Disadvantages</strong></p>
<ul>
<li>All cons of GLMs</li>
</ul>
<table>
<thead>
<tr class="header">
<th>Readings</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ISLR 6.1 Subset Selection</td>
<td></td>
</tr>
<tr class="even">
<td>ISLR 6.2 Shrinkage Methods</td>
<td></td>
</tr>
</tbody>
</table>
</div>
<div id="example-ridge-regression" class="section level2" number="13.8">
<h2><span class="header-section-number">13.8</span> Example: Ridge Regression</h2>
<p>We will use the <code>glmnet</code> package in order to perform ridge regression and
the lasso. The main function in this package is <code>glmnet()</code>, which can be used
to fit ridge regression models, lasso models, and more. This function has
slightly different syntax from other model-fitting functions that we have
encountered thus far in this book. In particular, we must pass in an <span class="math inline">\(x\)</span>
matrix as well as a <span class="math inline">\(y\)</span> vector, and we do not use the <span class="math inline">\(y \sim x\)</span> syntax.</p>
<p>Before proceeding, let‚Äôs first ensure that the missing values have
been removed from the data, as described in the previous lab.</p>
<div class="sourceCode" id="cb313"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb313-1"><a href="glm-variable-selection.html#cb313-1" aria-hidden="true" tabindex="-1"></a>Hitters <span class="ot">=</span> <span class="fu">na.omit</span>(Hitters)</span></code></pre></div>
<p>We will now perform ridge regression and the lasso in order to predict <code>Salary</code> on
the <code>Hitters</code> data. Let‚Äôs set up our data:</p>
<div class="sourceCode" id="cb314"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb314-1"><a href="glm-variable-selection.html#cb314-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">model.matrix</span>(Salary<span class="sc">~</span>., Hitters)[,<span class="sc">-</span><span class="dv">1</span>] <span class="co"># trim off the first column</span></span>
<span id="cb314-2"><a href="glm-variable-selection.html#cb314-2" aria-hidden="true" tabindex="-1"></a>                                         <span class="co"># leaving only the predictors</span></span>
<span id="cb314-3"><a href="glm-variable-selection.html#cb314-3" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> Hitters <span class="sc">%&gt;%</span></span>
<span id="cb314-4"><a href="glm-variable-selection.html#cb314-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(Salary) <span class="sc">%&gt;%</span></span>
<span id="cb314-5"><a href="glm-variable-selection.html#cb314-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">unlist</span>() <span class="sc">%&gt;%</span></span>
<span id="cb314-6"><a href="glm-variable-selection.html#cb314-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.numeric</span>()</span></code></pre></div>
<p>The <code>model.matrix()</code> function is particularly useful for creating <span class="math inline">\(x\)</span>; not only
does it produce a matrix corresponding to the 19 predictors but it also
automatically transforms any qualitative variables into dummy variables.
The latter property is important because <code>glmnet()</code> can only take numerical,
quantitative inputs.</p>
<p>The <code>glmnet()</code> function has an alpha argument that determines what type
of model is fit. If <code>alpha = 0</code> then a ridge regression model is fit, and if <code>alpha = 1</code>
then a lasso model is fit. We first fit a ridge regression model:</p>
<div class="sourceCode" id="cb315"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb315-1"><a href="glm-variable-selection.html#cb315-1" aria-hidden="true" tabindex="-1"></a>grid <span class="ot">=</span> <span class="dv">10</span><span class="sc">^</span><span class="fu">seq</span>(<span class="dv">10</span>, <span class="sc">-</span><span class="dv">2</span>, <span class="at">length =</span> <span class="dv">100</span>)</span>
<span id="cb315-2"><a href="glm-variable-selection.html#cb315-2" aria-hidden="true" tabindex="-1"></a>ridge_mod <span class="ot">=</span> <span class="fu">glmnet</span>(x, y, <span class="at">alpha =</span> <span class="dv">0</span>, <span class="at">lambda =</span> grid)</span></code></pre></div>
<p>By default the <code>glmnet()</code> function performs ridge regression for an automatically
selected range of <span class="math inline">\(\lambda\)</span> values. However, here we have chosen to implement
the function over a grid of values ranging from <span class="math inline">\(\lambda = 10^10\)</span> to <span class="math inline">\(\lambda = 10^{-2}\)</span>, essentially covering the full range of scenarios from the null model containing
only the intercept, to the least squares fit.</p>
<p>As we will see, we can also compute
model fits for a particular value of <span class="math inline">\(\lambda\)</span> that is not one of the original
grid values. Note that by default, the <code>glmnet()</code> function standardizes the
variables so that they are on the same scale. To turn off this default setting,
use the argument <code>standardize = FALSE</code>.</p>
<p>Associated with each value of <span class="math inline">\(\lambda\)</span> is a vector of ridge regression coefficients,
stored in a matrix that can be accessed by <code>coef()</code>. In this case, it is a <span class="math inline">\(20 \times 100\)</span>
matrix, with 20 rows (one for each predictor, plus an intercept) and 100
columns (one for each value of <span class="math inline">\(\lambda\)</span>).</p>
<div class="sourceCode" id="cb316"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb316-1"><a href="glm-variable-selection.html#cb316-1" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(<span class="fu">coef</span>(ridge_mod))</span></code></pre></div>
<pre><code>## [1]  20 100</code></pre>
<p>We expect the coefficient estimates to be much smaller, in terms of <span class="math inline">\(l_2\)</span> norm,
when a large value of <span class="math inline">\(\lambda\)</span> is used, as compared to when a small value of <span class="math inline">\(\lambda\)</span> is
used. These are the coefficients when <span class="math inline">\(\lambda = 11498\)</span>, along with their <span class="math inline">\(l_2\)</span> norm:</p>
<div class="sourceCode" id="cb318"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb318-1"><a href="glm-variable-selection.html#cb318-1" aria-hidden="true" tabindex="-1"></a>ridge_mod<span class="sc">$</span>lambda[<span class="dv">50</span>] <span class="co">#Display 50th lambda value</span></span></code></pre></div>
<pre><code>## [1] 11497.57</code></pre>
<div class="sourceCode" id="cb320"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb320-1"><a href="glm-variable-selection.html#cb320-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(ridge_mod)[,<span class="dv">50</span>] <span class="co"># Display coefficients associated with 50th lambda value</span></span></code></pre></div>
<pre><code>##   (Intercept)         AtBat          Hits         HmRun          Runs           RBI         Walks 
## 407.356050200   0.036957182   0.138180344   0.524629976   0.230701523   0.239841459   0.289618741 
##         Years        CAtBat         CHits        CHmRun         CRuns          CRBI        CWalks 
##   1.107702929   0.003131815   0.011653637   0.087545670   0.023379882   0.024138320   0.025015421 
##       LeagueN     DivisionW       PutOuts       Assists        Errors    NewLeagueN 
##   0.085028114  -6.215440973   0.016482577   0.002612988  -0.020502690   0.301433531</code></pre>
<div class="sourceCode" id="cb322"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb322-1"><a href="glm-variable-selection.html#cb322-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sqrt</span>(<span class="fu">sum</span>(<span class="fu">coef</span>(ridge_mod)[<span class="sc">-</span><span class="dv">1</span>,<span class="dv">50</span>]<span class="sc">^</span><span class="dv">2</span>)) <span class="co"># Calculate l2 norm</span></span></code></pre></div>
<pre><code>## [1] 6.360612</code></pre>
<p>In contrast, here are the coefficients when <span class="math inline">\(\lambda = 705\)</span>, along with their <span class="math inline">\(l_2\)</span>
norm. Note the much larger <span class="math inline">\(l_2\)</span> norm of the coefficients associated with this
smaller value of <span class="math inline">\(\lambda\)</span>.</p>
<div class="sourceCode" id="cb324"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb324-1"><a href="glm-variable-selection.html#cb324-1" aria-hidden="true" tabindex="-1"></a>ridge_mod<span class="sc">$</span>lambda[<span class="dv">60</span>] <span class="co">#Display 60th lambda value</span></span></code></pre></div>
<pre><code>## [1] 705.4802</code></pre>
<div class="sourceCode" id="cb326"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb326-1"><a href="glm-variable-selection.html#cb326-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(ridge_mod)[,<span class="dv">60</span>] <span class="co"># Display coefficients associated with 60th lambda value</span></span></code></pre></div>
<pre><code>##  (Intercept)        AtBat         Hits        HmRun         Runs          RBI        Walks 
##  54.32519950   0.11211115   0.65622409   1.17980910   0.93769713   0.84718546   1.31987948 
##        Years       CAtBat        CHits       CHmRun        CRuns         CRBI       CWalks 
##   2.59640425   0.01083413   0.04674557   0.33777318   0.09355528   0.09780402   0.07189612 
##      LeagueN    DivisionW      PutOuts      Assists       Errors   NewLeagueN 
##  13.68370191 -54.65877750   0.11852289   0.01606037  -0.70358655   8.61181213</code></pre>
<div class="sourceCode" id="cb328"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb328-1"><a href="glm-variable-selection.html#cb328-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sqrt</span>(<span class="fu">sum</span>(<span class="fu">coef</span>(ridge_mod)[<span class="sc">-</span><span class="dv">1</span>,<span class="dv">60</span>]<span class="sc">^</span><span class="dv">2</span>)) <span class="co"># Calculate l2 norm</span></span></code></pre></div>
<pre><code>## [1] 57.11001</code></pre>
<p>We can use the <code>predict()</code> function for a number of purposes. For instance,
we can obtain the ridge regression coefficients for a new value of <span class="math inline">\(\lambda\)</span>, say 50:</p>
<div class="sourceCode" id="cb330"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb330-1"><a href="glm-variable-selection.html#cb330-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(ridge_mod, <span class="at">s=</span><span class="dv">50</span>, <span class="at">type=</span><span class="st">&quot;coefficients&quot;</span>)[<span class="dv">1</span><span class="sc">:</span><span class="dv">20</span>,]</span></code></pre></div>
<pre><code>##   (Intercept)         AtBat          Hits         HmRun          Runs           RBI         Walks 
##  4.876610e+01 -3.580999e-01  1.969359e+00 -1.278248e+00  1.145892e+00  8.038292e-01  2.716186e+00 
##         Years        CAtBat         CHits        CHmRun         CRuns          CRBI        CWalks 
## -6.218319e+00  5.447837e-03  1.064895e-01  6.244860e-01  2.214985e-01  2.186914e-01 -1.500245e-01 
##       LeagueN     DivisionW       PutOuts       Assists        Errors    NewLeagueN 
##  4.592589e+01 -1.182011e+02  2.502322e-01  1.215665e-01 -3.278600e+00 -9.496680e+00</code></pre>
<p>We now split the samples into a training set and a test set in order
to estimate the test error of ridge regression and the lasso.</p>
<div class="sourceCode" id="cb332"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb332-1"><a href="glm-variable-selection.html#cb332-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb332-2"><a href="glm-variable-selection.html#cb332-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb332-3"><a href="glm-variable-selection.html#cb332-3" aria-hidden="true" tabindex="-1"></a>train <span class="ot">=</span> Hitters <span class="sc">%&gt;%</span></span>
<span id="cb332-4"><a href="glm-variable-selection.html#cb332-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sample_frac</span>(<span class="fl">0.5</span>)</span>
<span id="cb332-5"><a href="glm-variable-selection.html#cb332-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb332-6"><a href="glm-variable-selection.html#cb332-6" aria-hidden="true" tabindex="-1"></a>test <span class="ot">=</span> Hitters <span class="sc">%&gt;%</span></span>
<span id="cb332-7"><a href="glm-variable-selection.html#cb332-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">setdiff</span>(train)</span>
<span id="cb332-8"><a href="glm-variable-selection.html#cb332-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb332-9"><a href="glm-variable-selection.html#cb332-9" aria-hidden="true" tabindex="-1"></a>x_train <span class="ot">=</span> <span class="fu">model.matrix</span>(Salary<span class="sc">~</span>., train)[,<span class="sc">-</span><span class="dv">1</span>]</span>
<span id="cb332-10"><a href="glm-variable-selection.html#cb332-10" aria-hidden="true" tabindex="-1"></a>x_test <span class="ot">=</span> <span class="fu">model.matrix</span>(Salary<span class="sc">~</span>., test)[,<span class="sc">-</span><span class="dv">1</span>]</span>
<span id="cb332-11"><a href="glm-variable-selection.html#cb332-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb332-12"><a href="glm-variable-selection.html#cb332-12" aria-hidden="true" tabindex="-1"></a>y_train <span class="ot">=</span> train <span class="sc">%&gt;%</span></span>
<span id="cb332-13"><a href="glm-variable-selection.html#cb332-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(Salary) <span class="sc">%&gt;%</span></span>
<span id="cb332-14"><a href="glm-variable-selection.html#cb332-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">unlist</span>() <span class="sc">%&gt;%</span></span>
<span id="cb332-15"><a href="glm-variable-selection.html#cb332-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.numeric</span>()</span>
<span id="cb332-16"><a href="glm-variable-selection.html#cb332-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb332-17"><a href="glm-variable-selection.html#cb332-17" aria-hidden="true" tabindex="-1"></a>y_test <span class="ot">=</span> test <span class="sc">%&gt;%</span></span>
<span id="cb332-18"><a href="glm-variable-selection.html#cb332-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(Salary) <span class="sc">%&gt;%</span></span>
<span id="cb332-19"><a href="glm-variable-selection.html#cb332-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">unlist</span>() <span class="sc">%&gt;%</span></span>
<span id="cb332-20"><a href="glm-variable-selection.html#cb332-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.numeric</span>()</span></code></pre></div>
<p>Next we fit a ridge regression model on the training set, and evaluate
its MSE on the test set, using <span class="math inline">\(\lambda = 4\)</span>. Note the use of the <code>predict()</code>
function again: this time we get predictions for a test set, by replacing
<code>type="coefficients"</code> with the <code>newx</code> argument.</p>
<div class="sourceCode" id="cb333"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb333-1"><a href="glm-variable-selection.html#cb333-1" aria-hidden="true" tabindex="-1"></a>ridge_mod <span class="ot">=</span> <span class="fu">glmnet</span>(x_train, y_train, <span class="at">alpha=</span><span class="dv">0</span>, <span class="at">lambda =</span> grid, <span class="at">thresh =</span> <span class="fl">1e-12</span>)</span>
<span id="cb333-2"><a href="glm-variable-selection.html#cb333-2" aria-hidden="true" tabindex="-1"></a>ridge_pred <span class="ot">=</span> <span class="fu">predict</span>(ridge_mod, <span class="at">s =</span> <span class="dv">4</span>, <span class="at">newx =</span> x_test)</span>
<span id="cb333-3"><a href="glm-variable-selection.html#cb333-3" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((ridge_pred <span class="sc">-</span> y_test)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 139858.6</code></pre>
<p>The test MSE is 101242.7. Note that if we had instead simply fit a model
with just an intercept, we would have predicted each test observation using
the mean of the training observations. In that case, we could compute the
test set MSE like this:</p>
<div class="sourceCode" id="cb335"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb335-1"><a href="glm-variable-selection.html#cb335-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((<span class="fu">mean</span>(y_train) <span class="sc">-</span> y_test)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 224692.1</code></pre>
<p>We could also get the same result by fitting a ridge regression model with
a very large value of <span class="math inline">\(\lambda\)</span>. Note that <code>1e10</code> means <span class="math inline">\(10^{10}\)</span>.</p>
<div class="sourceCode" id="cb337"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb337-1"><a href="glm-variable-selection.html#cb337-1" aria-hidden="true" tabindex="-1"></a>ridge_pred <span class="ot">=</span> <span class="fu">predict</span>(ridge_mod, <span class="at">s =</span> <span class="fl">1e10</span>, <span class="at">newx =</span> x_test)</span>
<span id="cb337-2"><a href="glm-variable-selection.html#cb337-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((ridge_pred <span class="sc">-</span> y_test)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 224692.1</code></pre>
<p>So fitting a ridge regression model with <span class="math inline">\(\lambda = 4\)</span> leads to a much lower test
MSE than fitting a model with just an intercept. We now check whether
there is any benefit to performing ridge regression with <span class="math inline">\(\lambda = 4\)</span> instead of
just performing least squares regression. Recall that least squares is simply
ridge regression with <span class="math inline">\(\lambda = 0\)</span>.</p>
<p>* Note: In order for <code>glmnet()</code> to yield the <strong>exact</strong> least squares coefficients when <span class="math inline">\(\lambda = 0\)</span>,
we use the argument <code>exact=T</code> when calling the <code>predict()</code> function. Otherwise, the
<code>predict()</code> function will interpolate over the grid of <span class="math inline">\(\lambda\)</span> values used in fitting the
<code>glmnet()</code> model, yielding approximate results. Even when we use <code>exact = T</code>, there remains
a slight discrepancy in the third decimal place between the output of <code>glmnet()</code> when
<span class="math inline">\(\lambda = 0\)</span> and the output of <code>lm()</code>; this is due to numerical approximation on the part of
<code>glmnet()</code>.</p>
<div class="sourceCode" id="cb339"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb339-1"><a href="glm-variable-selection.html#cb339-1" aria-hidden="true" tabindex="-1"></a>ridge_pred <span class="ot">=</span> <span class="fu">predict</span>(ridge_mod, <span class="at">s =</span> <span class="dv">0</span>, <span class="at">x =</span> x_train, <span class="at">y =</span> y_train, <span class="at">newx =</span> x_test, <span class="at">exact =</span> T)</span>
<span id="cb339-2"><a href="glm-variable-selection.html#cb339-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((ridge_pred <span class="sc">-</span> y_test)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 175051.7</code></pre>
<div class="sourceCode" id="cb341"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb341-1"><a href="glm-variable-selection.html#cb341-1" aria-hidden="true" tabindex="-1"></a><span class="fu">lm</span>(Salary<span class="sc">~</span>., <span class="at">data =</span> train)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Salary ~ ., data = train)
## 
## Coefficients:
## (Intercept)        AtBat         Hits        HmRun         Runs          RBI        Walks  
##   2.398e+02   -1.639e-03   -2.179e+00    6.337e+00    7.139e-01    8.735e-01    3.594e+00  
##       Years       CAtBat        CHits       CHmRun        CRuns         CRBI       CWalks  
##  -1.309e+01   -7.136e-01    3.316e+00    3.407e+00   -5.671e-01   -7.525e-01    2.347e-01  
##     LeagueN    DivisionW      PutOuts      Assists       Errors   NewLeagueN  
##   1.322e+02   -1.346e+02    2.099e-01    6.229e-01   -4.616e+00   -8.330e+01</code></pre>
<div class="sourceCode" id="cb343"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb343-1"><a href="glm-variable-selection.html#cb343-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(ridge_mod, <span class="at">s =</span> <span class="dv">0</span>, <span class="at">x =</span> x_train, <span class="at">y =</span> y_train, <span class="at">exact =</span> T, <span class="at">type=</span><span class="st">&quot;coefficients&quot;</span>)[<span class="dv">1</span><span class="sc">:</span><span class="dv">20</span>,]</span></code></pre></div>
<pre><code>##   (Intercept)         AtBat          Hits         HmRun          Runs           RBI         Walks 
##  239.83274953   -0.00175359   -2.17853087    6.33694957    0.71369687    0.87329878    3.59421378 
##         Years        CAtBat         CHits        CHmRun         CRuns          CRBI        CWalks 
##  -13.09231408   -0.71351092    3.31523605    3.40701392   -0.56709530   -0.75240961    0.23467433 
##       LeagueN     DivisionW       PutOuts       Assists        Errors    NewLeagueN 
##  132.15949536 -134.58503816    0.20992473    0.62288126   -4.61583857  -83.29432536</code></pre>
<p>It looks like we are indeed improving over regular least-squares! Side note: in general, if we want to fit a (unpenalized) least squares model, then
we should use the <code>lm()</code> function, since that function provides more useful
outputs, such as standard errors and <span class="math inline">\(p\)</span>-values for the coefficients.</p>
<p>Instead of arbitrarily choosing <span class="math inline">\(\lambda = 4\)</span>, it would be better to
use cross-validation to choose the tuning parameter <span class="math inline">\(\lambda\)</span>. We can do this using
the built-in cross-validation function, <code>cv.glmnet()</code>. By default, the function
performs 10-fold cross-validation, though this can be changed using the
argument <code>folds</code>. Note that we set a random seed first so our results will be
reproducible, since the choice of the cross-validation folds is random.</p>
<div class="sourceCode" id="cb345"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb345-1"><a href="glm-variable-selection.html#cb345-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb345-2"><a href="glm-variable-selection.html#cb345-2" aria-hidden="true" tabindex="-1"></a>cv.out <span class="ot">=</span> <span class="fu">cv.glmnet</span>(x_train, y_train, <span class="at">alpha =</span> <span class="dv">0</span>) <span class="co"># Fit ridge regression model on training data</span></span>
<span id="cb345-3"><a href="glm-variable-selection.html#cb345-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(cv.out) <span class="co"># Draw plot of training MSE as a function of lambda</span></span></code></pre></div>
<p><img src="Exam-PA-Study-Manual_files/figure-html/unnamed-chunk-196-1.png" width="576" /></p>
<div class="sourceCode" id="cb346"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb346-1"><a href="glm-variable-selection.html#cb346-1" aria-hidden="true" tabindex="-1"></a>bestlam <span class="ot">=</span> cv.out<span class="sc">$</span>lambda.min  <span class="co"># Select lamda that minimizes training MSE</span></span>
<span id="cb346-2"><a href="glm-variable-selection.html#cb346-2" aria-hidden="true" tabindex="-1"></a>bestlam</span></code></pre></div>
<pre><code>## [1] 326.1406</code></pre>
<p>Therefore, we see that the value of <span class="math inline">\(\lambda\)</span> that results in the smallest cross-validation
error is 339.1845 What is the test MSE associated with this value of
<span class="math inline">\(\lambda\)</span>?</p>
<div class="sourceCode" id="cb348"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb348-1"><a href="glm-variable-selection.html#cb348-1" aria-hidden="true" tabindex="-1"></a>ridge_pred <span class="ot">=</span> <span class="fu">predict</span>(ridge_mod, <span class="at">s =</span> bestlam, <span class="at">newx =</span> x_test) <span class="co"># Use best lambda to predict test data</span></span>
<span id="cb348-2"><a href="glm-variable-selection.html#cb348-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((ridge_pred <span class="sc">-</span> y_test)<span class="sc">^</span><span class="dv">2</span>) <span class="co"># Calculate test MSE</span></span></code></pre></div>
<pre><code>## [1] 140056.2</code></pre>
<p>This represents a further improvement over the test MSE that we got using
<span class="math inline">\(\lambda = 4\)</span>. Finally, we refit our ridge regression model on the full data set,
using the value of <span class="math inline">\(\lambda\)</span> chosen by cross-validation, and examine the coefficient
estimates.</p>
<div class="sourceCode" id="cb350"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb350-1"><a href="glm-variable-selection.html#cb350-1" aria-hidden="true" tabindex="-1"></a>out <span class="ot">=</span> <span class="fu">glmnet</span>(x, y, <span class="at">alpha =</span> <span class="dv">0</span>) <span class="co"># Fit ridge regression model on full dataset</span></span>
<span id="cb350-2"><a href="glm-variable-selection.html#cb350-2" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(out, <span class="at">type =</span> <span class="st">&quot;coefficients&quot;</span>, <span class="at">s =</span> bestlam)[<span class="dv">1</span><span class="sc">:</span><span class="dv">20</span>,] <span class="co"># Display coefficients using lambda chosen by CV</span></span></code></pre></div>
<pre><code>##  (Intercept)        AtBat         Hits        HmRun         Runs          RBI        Walks 
##  15.44835008   0.07716945   0.85906253   0.60120339   1.06366687   0.87936073   1.62437580 
##        Years       CAtBat        CHits       CHmRun        CRuns         CRBI       CWalks 
##   1.35296287   0.01134998   0.05746377   0.40678422   0.11455696   0.12115916   0.05299953 
##      LeagueN    DivisionW      PutOuts      Assists       Errors   NewLeagueN 
##  22.08942749 -79.03490973   0.16618830   0.02941513  -1.36075644   9.12528398</code></pre>
<p>As expected, none of the coefficients are exactly zero - ridge regression does not
perform variable selection!</p>
</div>
<div id="example-the-lasso" class="section level2" number="13.9">
<h2><span class="header-section-number">13.9</span> Example: The Lasso</h2>
<p>We saw that ridge regression with a wise choice of <span class="math inline">\(\lambda\)</span> can outperform least
squares as well as the null model on the Hitters data set. We now ask
whether the lasso can yield either a more accurate or a more interpretable
model than ridge regression. In order to fit a lasso model, we once again
use the <code>glmnet()</code> function; however, this time we use the argument <code>alpha=1</code>.
Other than that change, we proceed just as we did in fitting a ridge model:</p>
<div class="sourceCode" id="cb352"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb352-1"><a href="glm-variable-selection.html#cb352-1" aria-hidden="true" tabindex="-1"></a>lasso_mod <span class="ot">=</span> <span class="fu">glmnet</span>(x_train, y_train, <span class="at">alpha =</span> <span class="dv">1</span>, <span class="at">lambda =</span> grid) <span class="co"># Fit lasso model on training data</span></span>
<span id="cb352-2"><a href="glm-variable-selection.html#cb352-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lasso_mod)                                          <span class="co"># Draw plot of coefficients</span></span></code></pre></div>
<p><img src="Exam-PA-Study-Manual_files/figure-html/unnamed-chunk-199-1.png" width="576" /></p>
<p>Notice that in the coefficient plot that depending on the choice of tuning
parameter, some of the coefficients are exactly equal to zero. We now
perform cross-validation and compute the associated test error:</p>
<div class="sourceCode" id="cb353"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb353-1"><a href="glm-variable-selection.html#cb353-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb353-2"><a href="glm-variable-selection.html#cb353-2" aria-hidden="true" tabindex="-1"></a>cv.out <span class="ot">=</span> <span class="fu">cv.glmnet</span>(x_train, y_train, <span class="at">alpha =</span> <span class="dv">1</span>) <span class="co"># Fit lasso model on training data</span></span>
<span id="cb353-3"><a href="glm-variable-selection.html#cb353-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(cv.out) <span class="co"># Draw plot of training MSE as a function of lambda</span></span></code></pre></div>
<p><img src="Exam-PA-Study-Manual_files/figure-html/unnamed-chunk-200-1.png" width="576" /></p>
<div class="sourceCode" id="cb354"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb354-1"><a href="glm-variable-selection.html#cb354-1" aria-hidden="true" tabindex="-1"></a>bestlam <span class="ot">=</span> cv.out<span class="sc">$</span>lambda.min <span class="co"># Select lamda that minimizes training MSE</span></span>
<span id="cb354-2"><a href="glm-variable-selection.html#cb354-2" aria-hidden="true" tabindex="-1"></a>lasso_pred <span class="ot">=</span> <span class="fu">predict</span>(lasso_mod, <span class="at">s =</span> bestlam, <span class="at">newx =</span> x_test) <span class="co"># Use best lambda to predict test data</span></span>
<span id="cb354-3"><a href="glm-variable-selection.html#cb354-3" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((lasso_pred <span class="sc">-</span> y_test)<span class="sc">^</span><span class="dv">2</span>) <span class="co"># Calculate test MSE</span></span></code></pre></div>
<pre><code>## [1] 143273</code></pre>
<p>This is substantially lower than the test set MSE of the null model and of
least squares, and very similar to the test MSE of ridge regression with <span class="math inline">\(\lambda\)</span>
chosen by cross-validation.</p>
<p>However, the lasso has a substantial advantage over ridge regression in
that the resulting coefficient estimates are sparse. Here we see that 12 of
the 19 coefficient estimates are exactly zero:</p>
<div class="sourceCode" id="cb356"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb356-1"><a href="glm-variable-selection.html#cb356-1" aria-hidden="true" tabindex="-1"></a>out <span class="ot">=</span> <span class="fu">glmnet</span>(x, y, <span class="at">alpha =</span> <span class="dv">1</span>, <span class="at">lambda =</span> grid) <span class="co"># Fit lasso model on full dataset</span></span>
<span id="cb356-2"><a href="glm-variable-selection.html#cb356-2" aria-hidden="true" tabindex="-1"></a>lasso_coef <span class="ot">=</span> <span class="fu">predict</span>(out, <span class="at">type =</span> <span class="st">&quot;coefficients&quot;</span>, <span class="at">s =</span> bestlam)[<span class="dv">1</span><span class="sc">:</span><span class="dv">20</span>,] <span class="co"># Display coefficients using lambda chosen by CV</span></span>
<span id="cb356-3"><a href="glm-variable-selection.html#cb356-3" aria-hidden="true" tabindex="-1"></a>lasso_coef</span></code></pre></div>
<pre><code>##   (Intercept)         AtBat          Hits         HmRun          Runs           RBI         Walks 
##    1.27429897   -0.05490834    2.18012455    0.00000000    0.00000000    0.00000000    2.29189433 
##         Years        CAtBat         CHits        CHmRun         CRuns          CRBI        CWalks 
##   -0.33767315    0.00000000    0.00000000    0.02822467    0.21627609    0.41713051    0.00000000 
##       LeagueN     DivisionW       PutOuts       Assists        Errors    NewLeagueN 
##   20.28190194 -116.16524424    0.23751978    0.00000000   -0.85604181    0.00000000</code></pre>
<p>Selecting only the predictors with non-zero coefficients, we see that the lasso model with <span class="math inline">\(\lambda\)</span>
chosen by cross-validation contains only seven variables:</p>
<div class="sourceCode" id="cb358"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb358-1"><a href="glm-variable-selection.html#cb358-1" aria-hidden="true" tabindex="-1"></a>lasso_coef[lasso_coef<span class="sc">!=</span><span class="dv">0</span>] <span class="co"># Display only non-zero coefficients</span></span></code></pre></div>
<pre><code>##   (Intercept)         AtBat          Hits         Walks         Years        CHmRun         CRuns 
##    1.27429897   -0.05490834    2.18012455    2.29189433   -0.33767315    0.02822467    0.21627609 
##          CRBI       LeagueN     DivisionW       PutOuts        Errors 
##    0.41713051   20.28190194 -116.16524424    0.23751978   -0.85604181</code></pre>
<p>Practice questions:</p>
<ul>
<li>How do ridge regression and the lasso improve on simple least squares?</li>
<li>In what cases would you expect ridge regression outperform the lasso, and vice versa?</li>
</ul>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="additional-glm-topics.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="bias-variance-trade-off.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/sdcastillo/PA-R-Study-Manual/edit/master/04-linear-models.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Exam-PA-Study-Manual.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
